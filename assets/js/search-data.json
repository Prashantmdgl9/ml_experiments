{
  
    
        "post0": {
            "title": "Gans And Ganas",
            "content": "The Story of GANAS . If you have watched stand and deliver then you would know Jaime Escalante’s words that all we need is Ganas, which is roughly translated to desire to do something, having zeal and passion to do something despite having the odds against us. . Now, the actual transation has been disputed but it sits very well with the movie. One of the best when it comes to education and student teacher relationships. . GAN and Ganas don’t have anything in common except similar sounding words but it definitely makes me think about this course I took on deep learning and the instructor Jeremy Howard. No, I won’t elevate him to status of Jaime Escalante but definitely he has done the work of bringing a topic to the world for which universities are charging an exorbitant amount of money and the subject is more or less commoditised in today’s world. . I have discussed it with many friends of mine who are in deep learning and they would say that he has ulterior motives, he wants to be famous as someone who is locking horns with big corps in the field of deep learning, some say his API is half baked, a few have good words to say about him, one of them said that he has become a millionaire through affiliations and what not. . I don’t know whether there is merit in them or not but I do apprecaite the fact that he has broguht this course that people can benefit greatly from. People talk about democracy but in real sense, provision of knowledge for all is the real democratisation of the AI. . It bridged many gaps in my knowledge, the things that I had just taken for granted for I couldn’t understand or come up with an explanation for their existence, I understood the reason behind them. I don’t want to make a comparison with any other courses out there but this one was just right up my alley of top down approach than the bottom up which I detest for many reasons. . Overall, I found his course quite helpful and it doesn’t really matter what others say, at least his’ is open source. . The story of GANs . I don’t have to educate the masses on what GANs are, these small posts are for my perusual as they help in crystallise my understanding and also act as a quick reference for future use. If anyone else finds them useful, then that’s an added cherry on the top. . Ok, about GANs. . If you have a low resolution photograph and you want to make it high resolution, then a simple neural network that uses the MSE/RMSE/MAE as the loss functions won’t be able to perform well as the output image would be quite close as far as the pixels are concerned and thus the loss would not be really high. To mitigate this problem, we need to form a different question - Is this a good quality image of the object under consideration? e.g. Is this a good quality image of a cat? . Earlier, the question was how far off we are from the original image which is supposed to be good quality but because pixels don’t really mismatch a lot, the loss is not very high despite having a low resolution image at hand. . These issues are solved by something called GAN - Generative Adversarial Network. A GAN is something that has two parts. A usual loss function that will check how far off you are from the real object and the other one is called a critic or a discriminator that will ascertain whether the geenrated/cleaned image is a good/high resolution image of the thing. . The loss function calls the critic/discriminator. . The discriminator will take all the generated/cleaned images and take the high resluton images and will try to classify them in binary bucket of high res vs generated/cleaned. The idea is to fool the discriminator to a degree that it will start classifying the generated/cleaned images as the high res images. . . It is more of a ping pong process. You train the predictor and then ask the discriminator how am I doing and then train the predictor more and then go back to discriminator and so on. . You train the generator for a while and then ask the discriminator how did you do and once you got the results then you train the generator again, after a while your generator will be quite good and will start fooling the discriminator. So, you stop training the generator and train the discriminator with the newly geenrated/cleaned image, now your discriminator is strong. You go back again and train the generator. This keeps going on till you are satisfied with the results. . They take a long time to train as generator and discriminator are dependent on each other. If neither is trained then it is the case of the blind leading the blind. .",
            "url": "https://prashantmdgl9.github.io/ml_experiments/2022/09/26/GANs-and-GANAS.html",
            "relUrl": "/2022/09/26/GANs-and-GANAS.html",
            "date": " • Sep 26, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "How To Finish Fastai Course",
            "content": "How to finish FastAI course . Start from Lesson 1, watch the video, finish the exercise to get a taste of the APIs. | Continue with the rest of the lessons. Make sure you finish the questionnaire, you can leave the further research questions for the time being. | If you are stuck, then try to find solutions but don’t stay there too long. | Lesson which talks about Neural Net foundations is an important one, and the one where you build SGD and a few other models from scratch. Spend extra time on them, a week isn’t a bad idea. | Run all the notebooks yourself to see the outputs and understand what it happening, if you don’t understand something then refer the various resources. | If there are additional resources used in the lesson such as excel sheets, they will be available on the repo, always replicate them, excel is a great way to learn things from basics on small examples. | Once you are in the middle of the course, take a gander of the things and write a couple of blog pieces to crystallise your understanding. They can be on any topics of your choice. | It’s always a good idea to re run the notebooks and then run them again on a dataset of your choice and see if you can get the good levels of accuracy and if not then what are the hurdles. | Finish all the lessons at least once. Finishing something entirely gives you a psychological advantage. | Before wrapping up, go through all the questionnaires from all the lessons and find all the tech papers that were discussed in the lessons. You are done for 2 weeks. | After 2 weeks of break, go back to the lessons, from 1 o 2, whatever you prefer and rewatch at 1.5x speed or whatever you prefer. This time take a stab at all the further research questions. . The things to focus in this iteration: . Further research questions | In every lesson there is a central idea, such as SGD from scratch, neural net from scratch, design your own grad etc, try to implement them without taking external help. | Fill any gaps in understanding of the subject matter through primary and secondary research. | After or during the first iteration, if there is an idea forming on a particular problem statement that you want to solve then start working in that direction. Whatever you have learned in these 2 iterations, apply them on your problem statement. Try to finish it fromend to end i.e after finishing it, deploy it or productify it and write an extensive blogpost about it. .",
            "url": "https://prashantmdgl9.github.io/ml_experiments/2022/09/20/How-to-Finish-FastAI-Course.html",
            "relUrl": "/2022/09/20/How-to-Finish-FastAI-Course.html",
            "date": " • Sep 20, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "How Resnets Work?",
            "content": "How Resnets work? . One can think that if one would keep on increasing the depth of a neural network, one can keep on achieving higher accuracy but that’s not the case as we have seen in the paper by Kaiming He et al. . Link to the paper - https://arxiv.org/pdf/1512.03385.pdf . As we see in the first image itself, the training error for the deep network with 56 layers is worse than the training error with 20 layers. . . The reason behind this stalling of the training of the neural network as we go deeper is the problem of the vanishng gradient. During the backpropagation, the model has to adjust the weights and those adjustments are calculated accordin to the chain rule of derivatives. For many of the weights, these values come out as less than 1. . Also, these values are multiplied by a earning rate whose value is 0.1 &gt; learning_rate &gt; 0.0001, essentially a much smaller number. When we multiply more and more numbers that are less than 1, we get more small numbers and thus the weights of the earlier layers don’t get adjusted at all during the backpropagation. . For earlier layers, the chain rule will contain many more terms than the later layers and thus the earlier layers’ weight adjustement will consist of more multiplications which will result in a tiny number and according to: . new_weight = old_weight - learning_rate * result of chain rule . How Resnets help? . Resents are short for residual network and they contain a special block called the skip connection block after each convolution. . This skip connected block helps in avoiding the issue of vanishing gradient and the model is able to train not only faster but also with much greater accuracy. . We can see the loss surface of the two networks, one with the skip connection and the other without. . Goldstein, Xu, Li et all did some really great work for this paper of theirs in which they visualise the loss landscape of the neural networks. . Link to the paper - https://arxiv.org/pdf/1712.09913.pdf . In a normal convolution network, the layers are stacked on top of one another, while in Resnet, apart from the stacking, there is a skip connection that goes from the input to the output. . . We have added the original input to the output of the convolution block via the skip connection, this will help during the backpropagation calculus and the partial derivative won’t reach a very small value and thus avoiding the issue of the vanishing gradient. . . X is the input, f(X) is the output of the small network block and we have added the skip connection, so, at the adder, the value is f(X) + X. . If we can make f(X) = 0, then the output will at the adder will be Y = X, which is what we want = No error from the system and input gets recognised as the output(if not 0 then make f(X) as small as possible) . Note The first layer of the Resnet arch has stride 2 convolution. The vernacular 1x1 conv, 64 means filter size of 1x1 and 64 such filters; 3x3, 128 means 3x3 sized filter and 128 such filters. . . As we can see, in the residual model, after 3 conv layers, the number of filters have changed from 64 to 128 which means we must have decreased the size of the image. The size shift at various times can be seen through the dotted connections. . Size of the image after each block is given by: . (n + 2P - f)/2 + 1 . So, a 300 by 300 image after first layer of convolution with stride, s= 2, padding, f = 3, filter size, p = 7 becomes 150 by 150 . As we move through the layers, the size keeps on decreasing. . One more thing about the dotted and solid skip conenctions, the dotted ones are called the convolution blocks while the solid ones are called the identity blocks. . The identity block is used when the input size = the output size i.e. the image doesn’t go through any size transformation. If the image has gone throguh the convolutions and also size transformations, in that case the input can’t simply be added via a skip connection because of disparity in the input and output sizes. . In this case, before feeding the value of x, it goes through a convolution block of its own to make the sizes consistent . . The convolution in the skip connection block generally is 1x1 filter size convolution. . That’s the architecture of the Resnet, it works while striving to make the f(X) = 0 and thus making, Y = X. .",
            "url": "https://prashantmdgl9.github.io/ml_experiments/2022/09/17/How-Resnets-Work.html",
            "relUrl": "/2022/09/17/How-Resnets-Work.html",
            "date": " • Sep 17, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Entity Embeddings",
            "content": "The purpose of the notebook is to implement entity embeddings for categorical variables. The paper for the same can be found here https://arxiv.org/pdf/1604.06737.pdf . Problem Statement:Use the attributes data to predict individual product failures of new codes with their individual lab test results. . Imports and file reads . import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) from fastai.tabular.all import * from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor import matplotlib.pyplot as plt import seaborn as sns . df_train = pd.read_csv(&#39;../input/tabular-playground-series-aug-2022/train.csv&#39;, low_memory=False) . df_train.head(10) . id product_code loading attribute_0 attribute_1 attribute_2 attribute_3 measurement_0 measurement_1 measurement_2 ... measurement_9 measurement_10 measurement_11 measurement_12 measurement_13 measurement_14 measurement_15 measurement_16 measurement_17 failure . 0 0 | A | 80.10 | material_7 | material_8 | 9 | 5 | 7 | 8 | 4 | ... | 10.672 | 15.859 | 17.594 | 15.193 | 15.029 | NaN | 13.034 | 14.684 | 764.100 | 0 | . 1 1 | A | 84.89 | material_7 | material_8 | 9 | 5 | 14 | 3 | 3 | ... | 12.448 | 17.947 | 17.915 | 11.755 | 14.732 | 15.425 | 14.395 | 15.631 | 682.057 | 0 | . 2 2 | A | 82.43 | material_7 | material_8 | 9 | 5 | 12 | 1 | 5 | ... | 12.715 | 15.607 | NaN | 13.798 | 16.711 | 18.631 | 14.094 | 17.946 | 663.376 | 0 | . 3 3 | A | 101.07 | material_7 | material_8 | 9 | 5 | 13 | 2 | 6 | ... | 12.471 | 16.346 | 18.377 | 10.020 | 15.250 | 15.562 | 16.154 | 17.172 | 826.282 | 0 | . 4 4 | A | 188.06 | material_7 | material_8 | 9 | 5 | 9 | 2 | 8 | ... | 10.337 | 17.082 | 19.932 | 12.428 | 16.182 | 12.760 | 13.153 | 16.412 | 579.885 | 0 | . 5 5 | A | 75.35 | material_7 | material_8 | 9 | 5 | 11 | 4 | 0 | ... | 10.622 | 14.904 | 19.107 | 13.327 | 15.354 | 19.251 | NaN | 17.625 | 832.902 | 0 | . 6 6 | A | 161.71 | material_7 | material_8 | 9 | 5 | 12 | 2 | 4 | ... | 11.370 | 17.714 | 19.924 | 11.560 | 16.653 | 17.734 | NaN | 16.637 | 684.438 | 1 | . 7 7 | A | 177.92 | material_7 | material_8 | 9 | 5 | 4 | 8 | 8 | ... | 10.254 | 16.449 | 20.478 | 12.207 | 15.624 | 16.968 | 15.176 | 17.231 | 684.000 | 1 | . 8 8 | A | 109.50 | material_7 | material_8 | 9 | 5 | 9 | 6 | 5 | ... | 11.557 | 15.965 | 19.604 | 14.091 | 15.674 | 13.327 | 13.535 | 15.408 | NaN | 0 | . 9 9 | A | 98.72 | material_7 | material_8 | 9 | 5 | 10 | 4 | 7 | ... | 10.384 | 15.237 | 18.427 | 12.635 | 14.318 | 14.327 | 12.867 | NaN | NaN | 0 | . 10 rows × 26 columns . df_train.columns . Index([&#39;id&#39;, &#39;product_code&#39;, &#39;loading&#39;, &#39;attribute_0&#39;, &#39;attribute_1&#39;, &#39;attribute_2&#39;, &#39;attribute_3&#39;, &#39;measurement_0&#39;, &#39;measurement_1&#39;, &#39;measurement_2&#39;, &#39;measurement_3&#39;, &#39;measurement_4&#39;, &#39;measurement_5&#39;, &#39;measurement_6&#39;, &#39;measurement_7&#39;, &#39;measurement_8&#39;, &#39;measurement_9&#39;, &#39;measurement_10&#39;, &#39;measurement_11&#39;, &#39;measurement_12&#39;, &#39;measurement_13&#39;, &#39;measurement_14&#39;, &#39;measurement_15&#39;, &#39;measurement_16&#39;, &#39;measurement_17&#39;, &#39;failure&#39;], dtype=&#39;object&#39;) . A lot of nulls, FastAI Tabular API should handle it . df_train.isna().sum() . id 0 product_code 0 loading 250 attribute_0 0 attribute_1 0 attribute_2 0 attribute_3 0 measurement_0 0 measurement_1 0 measurement_2 0 measurement_3 381 measurement_4 538 measurement_5 676 measurement_6 796 measurement_7 937 measurement_8 1048 measurement_9 1227 measurement_10 1300 measurement_11 1468 measurement_12 1601 measurement_13 1774 measurement_14 1874 measurement_15 2009 measurement_16 2110 measurement_17 2284 failure 0 dtype: int64 . dep_var = &#39;failure&#39; . df_train = df_train.drop(&#39;id&#39;, axis=1) . Divide the columns into continuous and categorical . cont,cat = cont_cat_split(df_train, max_card=9000, dep_var=dep_var) . cont, cat . ([&#39;loading&#39;, &#39;measurement_3&#39;, &#39;measurement_4&#39;, &#39;measurement_5&#39;, &#39;measurement_6&#39;, &#39;measurement_7&#39;, &#39;measurement_8&#39;, &#39;measurement_9&#39;, &#39;measurement_10&#39;, &#39;measurement_11&#39;, &#39;measurement_12&#39;, &#39;measurement_13&#39;, &#39;measurement_14&#39;, &#39;measurement_15&#39;, &#39;measurement_16&#39;, &#39;measurement_17&#39;], [&#39;product_code&#39;, &#39;attribute_0&#39;, &#39;attribute_1&#39;, &#39;attribute_2&#39;, &#39;attribute_3&#39;, &#39;measurement_0&#39;, &#39;measurement_1&#39;, &#39;measurement_2&#39;]) . splits = RandomSplitter(seed=42)(df_train) . The preprocessing on the data can be specified here . procs_nn = [Categorify, FillMissing, Normalize] . Some weird step, not sure why this was required but many people faced this issue and they suggested this. . df_train = df_train.astype({dep_var: np.float16}) . to_nn = TabularPandas(df_train, procs_nn, cat, cont, splits=splits, y_names=dep_var) . dls = to_nn.dataloaders(1024) . len(to_nn.train), len(to_nn.valid) . (21256, 5314) . to_nn.train.y.min(), to_nn.train.y.max() . (0.0, 1.0) . A Neural Model . A prelim neural network that will have embedding layers and we can extract them later. . learn = tabular_learner(dls, y_range=(0,1),layers=[500,250], n_out=1, loss_func=F.mse_loss) . learn.lr_find() . SuggestedLRs(valley=0.0063095735386013985) . learn.fit_one_cycle(10, 1e-3) . epoch train_loss valid_loss time . 0 | 0.253959 | 0.238980 | 00:00 | . 1 | 0.245832 | 0.231623 | 00:00 | . 2 | 0.233190 | 0.220957 | 00:00 | . 3 | 0.217750 | 0.200752 | 00:00 | . 4 | 0.201300 | 0.184770 | 00:00 | . 5 | 0.185941 | 0.179196 | 00:00 | . 6 | 0.173416 | 0.173974 | 00:00 | . 7 | 0.163278 | 0.172251 | 00:00 | . 8 | 0.155363 | 0.171942 | 00:00 | . 9 | 0.149819 | 0.171659 | 00:00 | . Not very good score! . preds,targs = learn.get_preds() roc_auc_score(targs, preds) . 0.5395034995325967 . def r_mse(pred,y): return round(math.sqrt(((pred-y)**2).mean()), 6) def m_rmse(m, xs, y): return r_mse(m.predict(xs), y) . Just checking! . to_nn.train.xs . product_code attribute_0 attribute_1 attribute_2 attribute_3 measurement_0 measurement_1 measurement_2 loading_na measurement_3_na ... measurement_8 measurement_9 measurement_10 measurement_11 measurement_12 measurement_13 measurement_14 measurement_15 measurement_16 measurement_17 . 16256 4 | 2 | 1 | 2 | 2 | 8 | 15 | 10 | 1 | 1 | ... | 1.127348 | 0.569510 | -1.186327 | 1.079770 | -0.337911 | 0.979852 | -1.306813 | 0.020901 | -0.105335 | -0.770814 | . 14071 3 | 2 | 3 | 1 | 3 | 7 | 3 | 6 | 1 | 1 | ... | -1.955778 | -1.023527 | 0.813038 | 0.984690 | -3.103619 | -0.443726 | -1.313075 | 1.534082 | 0.366453 | -0.586731 | . 12745 3 | 2 | 3 | 1 | 3 | 6 | 12 | 11 | 1 | 1 | ... | -1.394840 | -0.192154 | 0.215929 | -0.067298 | -0.745408 | 0.503835 | 1.560496 | -0.012620 | -0.013413 | -1.880876 | . 8530 2 | 1 | 1 | 3 | 3 | 5 | 12 | 7 | 1 | 1 | ... | -1.803898 | -0.985597 | 1.174368 | -0.444221 | -0.831490 | 1.034433 | 0.518217 | 1.306133 | -0.916811 | -0.003998 | . 135 1 | 2 | 3 | 4 | 1 | 17 | 11 | 17 | 1 | 1 | ... | 0.177604 | 1.084122 | 0.005700 | -1.791635 | 0.057091 | 1.000432 | -1.006236 | 1.133160 | 0.097990 | 0.250980 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 1679 1 | 2 | 3 | 4 | 1 | 5 | 2 | 5 | 1 | 1 | ... | -0.516984 | -0.087592 | 1.108672 | -0.067976 | -1.191087 | -0.019605 | -0.932483 | 0.098672 | 0.779799 | -0.864274 | . 24951 5 | 2 | 2 | 2 | 4 | 1 | 16 | 3 | 1 | 1 | ... | 0.547173 | 1.932921 | 0.311554 | -2.871468 | -2.605181 | -0.780158 | -0.564415 | 0.703408 | -2.964675 | 1.450600 | . 6149 2 | 1 | 1 | 3 | 3 | 12 | 20 | 7 | 1 | 1 | ... | -0.007686 | 0.092830 | -1.298741 | -0.473423 | -0.644055 | -1.366233 | 0.867499 | -0.217104 | -1.265630 | -0.110186 | . 13786 3 | 2 | 3 | 1 | 3 | 4 | 15 | 3 | 1 | 1 | ... | -0.198042 | 0.139985 | -0.143942 | -1.905731 | 0.139701 | 0.078820 | 0.774264 | -0.514779 | -0.464503 | -0.214580 | . 18294 4 | 2 | 1 | 2 | 2 | 8 | 8 | 7 | 1 | 1 | ... | 0.324420 | 0.430094 | -2.067391 | 0.027782 | -1.592337 | 0.842058 | 1.011526 | -2.007177 | 0.245918 | -0.311797 | . 21256 rows × 40 columns . Initial estimate on a random forest and see variable importance . xs,y = to_nn.train.xs,to_nn.train.y valid_xs,valid_y = to_nn.valid.xs,to_nn.valid.y . def rf(xs, y, n_estimators=100, max_samples=21_000, max_features=0.5, min_samples_leaf=5, **kwargs): return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators, max_samples=max_samples, max_features=max_features, min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y) . m = rf(xs, y); . Even 27% error on train &#129300;&#129488; . m_rmse(m, xs, y) . 0.273393 . m_rmse(m, valid_xs, valid_y) . 0.407537 . Use Tabular API to form a tabular datastructure, it has its own advantages than using pandas directly. . procs = [Categorify, FillMissing] to = TabularPandas(df_train, procs, cat, cont, dep_var, splits = splits) . cat_x = cat . learn.embeds[0].weight . Parameter containing: tensor([[ 0.0042, -0.0003, 0.0061, -0.0018], [ 0.0285, 0.0216, -0.0394, -0.0025], [-0.0105, -0.0114, -0.0086, -0.0202], [-0.0231, -0.0214, 0.0227, -0.0065], [-0.0481, -0.0364, 0.0286, 0.0316], [ 0.0049, -0.0071, -0.0379, -0.0254]], requires_grad=True) . Embeddings Transfer . Here the transfer of embeddings from the learner object to the tabular datastructure will take place . def add_embeds(learn, x): x = x.copy() for i, cat in enumerate(cat_x): emb = learn.embeds[i] vec = tensor(x[cat], dtype=torch.int64) # this is on cpu emb_data = emb(vec) emb_names = [f&#39;{cat}_{j}&#39; for j in range(emb_data.shape[1])] emb_df = pd.DataFrame(emb_data, index=x.index, columns=emb_names) x = x.drop(columns=cat) x = x.join(emb_df) return x . emb_xs = add_embeds(learn, to.train.xs) emb_valid_xs = add_embeds(learn, to.valid.xs) . On entire data as well, this will be fed to K fold mapper function . emb_xs_full = add_embeds(learn, to.xs) . emb_xs_full . loading measurement_3 measurement_4 measurement_5 measurement_6 measurement_7 measurement_8 measurement_9 measurement_10 measurement_11 ... measurement_14_na_2 measurement_15_na_0 measurement_15_na_1 measurement_15_na_2 measurement_16_na_0 measurement_16_na_1 measurement_16_na_2 measurement_17_na_0 measurement_17_na_1 measurement_17_na_2 . 16256 226.889999 | 16.542999 | 11.318 | 16.931999 | 16.516001 | 11.342 | 20.136999 | 11.985 | 14.499000 | 20.764999 | ... | 0.016842 | -0.001957 | -0.004819 | -0.012645 | 0.013927 | 0.040781 | -0.007095 | -0.004075 | -0.028883 | -0.039124 | . 14071 223.570007 | 17.822001 | 13.633 | 19.893999 | 17.169001 | 11.744 | 17.091999 | 10.431 | 17.238001 | 20.625000 | ... | 0.016842 | -0.001957 | -0.004819 | -0.012645 | 0.013927 | 0.040781 | -0.007095 | -0.004075 | -0.028883 | -0.039124 | . 12745 126.699997 | 17.507000 | 11.430 | 16.539000 | 16.652000 | 10.445 | 17.646000 | 11.242 | 16.420000 | 19.076000 | ... | 0.016842 | -0.026518 | -0.005157 | 0.017559 | -0.002328 | 0.000422 | -0.019381 | -0.004075 | -0.028883 | -0.039124 | . 8530 154.910004 | 17.986000 | 13.312 | 16.287001 | 17.518000 | 11.609 | 17.242001 | 10.468 | 17.733000 | 18.521000 | ... | 0.016842 | -0.001957 | -0.004819 | -0.012645 | 0.013927 | 0.040781 | -0.007095 | 0.011637 | -0.011171 | 0.027875 | . 135 110.620003 | 17.813999 | 12.651 | 17.520000 | 18.902000 | 9.456 | 19.198999 | 12.487 | 16.132000 | 16.537001 | ... | 0.016842 | -0.001957 | -0.004819 | -0.012645 | 0.013927 | 0.040781 | -0.007095 | -0.004075 | -0.028883 | -0.039124 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 3492 107.650002 | 17.816999 | 10.420 | 16.044001 | 18.687000 | 12.284 | 18.768000 | 12.545 | 16.643999 | 20.921000 | ... | 0.016842 | -0.001957 | -0.004819 | -0.012645 | 0.013927 | 0.040781 | -0.007095 | -0.004075 | -0.028883 | -0.039124 | . 19303 81.480003 | 18.282000 | 11.988 | 18.143000 | 18.448999 | 12.473 | 19.016001 | 12.176 | 13.979000 | 19.351999 | ... | 0.016842 | -0.001957 | -0.004819 | -0.012645 | 0.013927 | 0.040781 | -0.007095 | -0.004075 | -0.028883 | -0.039124 | . 23849 129.259995 | 16.525999 | 11.458 | 18.861000 | 18.282000 | 12.291 | 19.820999 | 11.776 | 17.207001 | 17.054001 | ... | 0.016842 | -0.001957 | -0.004819 | -0.012645 | 0.013927 | 0.040781 | -0.007095 | -0.004075 | -0.028883 | -0.039124 | . 10882 94.599998 | 17.041000 | 12.081 | 16.714001 | 18.591999 | 12.381 | 19.448000 | 10.987 | 16.327000 | 21.305000 | ... | 0.016842 | -0.001957 | -0.004819 | -0.012645 | 0.013927 | 0.040781 | -0.007095 | -0.004075 | -0.028883 | -0.039124 | . 20129 167.639999 | 18.910999 | 11.120 | 17.813999 | 16.090000 | 12.270 | 19.201000 | 11.157 | 20.118999 | 18.712000 | ... | 0.016842 | -0.001957 | -0.004819 | -0.012645 | 0.013927 | 0.040781 | -0.007095 | -0.004075 | -0.028883 | -0.039124 | . 26570 rows × 114 columns . to.train.xs.head(2), to.train.xs.shape . ( product_code attribute_0 attribute_1 attribute_2 attribute_3 16256 4 2 1 2 2 14071 3 2 3 1 3 measurement_0 measurement_1 measurement_2 loading_na 16256 8 15 10 1 14071 7 3 6 1 measurement_3_na ... measurement_8 measurement_9 measurement_10 16256 1 ... 20.136999 11.985 14.499000 14071 1 ... 17.091999 10.431 17.238001 measurement_11 measurement_12 measurement_13 measurement_14 16256 20.764999 11.210 16.743999 14.170 14071 20.625000 7.226 15.153000 14.161 measurement_15 measurement_16 measurement_17 16256 15.025 16.290001 609.656006 14071 17.282 17.065001 631.458008 [2 rows x 40 columns], (21256, 40)) . emb_xs.head(2), emb_xs.shape . ( loading measurement_3 measurement_4 measurement_5 measurement_6 16256 226.889999 16.542999 11.318 16.931999 16.516001 14071 223.570007 17.822001 13.633 19.893999 17.169001 measurement_7 measurement_8 measurement_9 measurement_10 16256 11.342 20.136999 11.985 14.499000 14071 11.744 17.091999 10.431 17.238001 measurement_11 ... measurement_14_na_2 measurement_15_na_0 16256 20.764999 ... 0.016842 -0.001957 14071 20.625000 ... 0.016842 -0.001957 measurement_15_na_1 measurement_15_na_2 measurement_16_na_0 16256 -0.004819 -0.012645 0.013927 14071 -0.004819 -0.012645 0.013927 measurement_16_na_1 measurement_16_na_2 measurement_17_na_0 16256 0.040781 -0.007095 -0.004075 14071 0.040781 -0.007095 -0.004075 measurement_17_na_1 measurement_17_na_2 16256 -0.028883 -0.039124 14071 -0.028883 -0.039124 [2 rows x 114 columns], (21256, 114)) . Modeling . Random Forest Models . from sklearn.metrics import mean_squared_error from sklearn.model_selection import KFold . Wrapper functions . def RF_wrapper(Xt, yt, Xv, yv, fold=-1): model = RandomForestRegressor(n_jobs=-1, n_estimators=100, max_samples=15000, max_features=0.5, min_samples_leaf=5, oob_score=False).fit(Xt, yt) print(f&#39;Training fold {fold}...&#39;) score_train = np.sqrt(mean_squared_error(model.predict(Xt), yt)) oof = model.predict(Xv) score = np.sqrt(mean_squared_error(oof, yv)) print(f&#39;Fold {fold}: training RMSLE: {score_train}, validation RMSLE: {score} n&#39;) return model, oof, score . def perform_CV(model_wrap, xs, ys, n_splits=3): kf = KFold(n_splits=n_splits, shuffle=True) models = [] scores = [] oof_total = np.zeros(xs.shape[0]) for fold, (train_idx, val_idx) in enumerate(kf.split(xs), start=1): Xt = xs.iloc[train_idx] yt = ys[train_idx] Xv, yv = xs.iloc[val_idx], ys[val_idx] model, oof, score = model_wrap(Xt, yt, Xv, yv, fold) models.append(model) scores.append(score) oof_total[val_idx] = oof print(&#39;Training completed.&#39;) print(f&#39;&gt; Mean RMSLE across folds: {np.mean(scores)}, std: {np.std(scores)}&#39;) print(f&#39;&gt; OOF RMSLE: {np.sqrt(mean_squared_error(ys, oof_total))}&#39;) return models, scores, oof_total . y_x = to[&quot;failure&quot;] . p = [] for c in to.cat_names: p.append(c) for c in to.cont_names: p.append(c) . to_x = to[p] . RF on the simple data, no embeddings - 0.4126 RMSE on validation . %%time n_splits = 3 models, _, _ = perform_CV(RF_wrapper, to_x, y_x, n_splits=n_splits) . Training fold 1... Fold 1: training RMSLE: 0.2863504507218632, validation RMSLE: 0.4125701353060731 Training fold 2... Fold 2: training RMSLE: 0.2857120392244807, validation RMSLE: 0.41492954084590494 Training fold 3... Fold 3: training RMSLE: 0.286567715511933, validation RMSLE: 0.41035574993160084 Training completed. &gt; Mean RMSLE across folds: 0.41261847536119295, std: 0.0018675551580679705 &gt; OOF RMSLE: 0.41392034556146445 CPU times: user 1min 14s, sys: 227 ms, total: 1min 14s Wall time: 24.1 s . importance = pd.DataFrame([model.feature_importances_ for model in models], columns=p, index=[f&#39;Fold {i}&#39; for i in range(1, n_splits + 1)]) importance = importance.T importance[&#39;Average importance&#39;] = importance.mean(axis=1) importance = importance.sort_values(by=&#39;Average importance&#39;, ascending=False) plt.figure(figsize=(10,7)) sns.barplot(x=&#39;Average importance&#39;, y=importance.index, data=importance); . RF with embeddings data - 0.4115 RMSE on validation data, a marginal improvement . %%time models_emb, _, _ = perform_CV(RF_wrapper, emb_xs_full, y_x, n_splits=n_splits) . Training fold 1... Fold 1: training RMSLE: 0.282660857344881, validation RMSLE: 0.40459037018198307 Training fold 2... Fold 2: training RMSLE: 0.2799249009775313, validation RMSLE: 0.4146841244291653 Training fold 3... Fold 3: training RMSLE: 0.2790328017014002, validation RMSLE: 0.4154808860434382 Training completed. &gt; Mean RMSLE across folds: 0.4115851268848622, std: 0.004956724272262138 &gt; OOF RMSLE: 0.4128117012765796 CPU times: user 1min 57s, sys: 239 ms, total: 1min 58s Wall time: 36.1 s . emb_xs.columns . Index([&#39;loading&#39;, &#39;measurement_3&#39;, &#39;measurement_4&#39;, &#39;measurement_5&#39;, &#39;measurement_6&#39;, &#39;measurement_7&#39;, &#39;measurement_8&#39;, &#39;measurement_9&#39;, &#39;measurement_10&#39;, &#39;measurement_11&#39;, ... &#39;measurement_14_na_2&#39;, &#39;measurement_15_na_0&#39;, &#39;measurement_15_na_1&#39;, &#39;measurement_15_na_2&#39;, &#39;measurement_16_na_0&#39;, &#39;measurement_16_na_1&#39;, &#39;measurement_16_na_2&#39;, &#39;measurement_17_na_0&#39;, &#39;measurement_17_na_1&#39;, &#39;measurement_17_na_2&#39;], dtype=&#39;object&#39;, length=114) . importance = pd.DataFrame([model.feature_importances_ for model in models_emb], columns=emb_xs.columns, index=[f&#39;Fold {i}&#39; for i in range(1, n_splits + 1)]) importance = importance.T importance[&#39;Average importance&#39;] = importance.mean(axis=1) importance = importance.sort_values(by=&#39;Average importance&#39;, ascending=False)[:20] plt.figure(figsize=(10,7)) sns.barplot(x=&#39;Average importance&#39;, y=importance.index, data=importance); . Remarks:Entity embeddings work, maybe require more feature engineering and more preprocessing to get some decent results on this dataset. . End of Notebook .",
            "url": "https://prashantmdgl9.github.io/ml_experiments/2022/09/12/entity-embeddings-aug-2022-tabular-playground.html",
            "relUrl": "/2022/09/12/entity-embeddings-aug-2022-tabular-playground.html",
            "date": " • Sep 12, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "How To Explain Gradient Boosting?",
            "content": "How to Explain Gradient Boosting? . Adaboost . The trees in Adaboost/Normal boosting are built sequentially and each tree boosts the attributes that lead to the misclassifications/errors in the previous tree. . How? . Each data point is assigned a weight in the beginning, usually same weight to say that all data points are equally important to the model(all the weights add upto 1). | After assigning the weight, a small tree is created, these small trees are called stumps. | After the first weak learner/model, one finds which data points were misclassified and the weights of the misclassified data points are increased/changed to tell the next model that these data points are more important than the other ones.(Also, because the sum of weights need to be 1, and we have increased weight of a few data points, to make the things consistent, we need to decrease the weights of the other data points - Noramlise). | A second weak learner will try to improve the misclassifications of the previous model and in turn it will have its own misclassifications, we go to step 3 again and adjust the weights for the third weak learner. | This goes on until a stopping criteria such as time limit, number of iterations, number of trees etc has reached. | Essentially, these sequential trees help in decreasing the error of the previous misclassifications and the learnings are carried forward as we move though the iterations. The results of the final model encompass the learnings of all the weak learners. . Gradient Boosting . Gradient boost has a little difference. In GBM, learning happens by optimising the loss function(just like how a regression model tries to optimise the sum of squared errors). Too much jargon! . How? . Maybe an example might help in this case: . Distance in miles, time in hours . Distance Time Speed . 5 m | 1 h | 5 m/h | . 6 m | 2 h | 3 m/h | . 3 m | 0.5 h | 6 m/h | . Let’s say we don’t know how to calculate speed given the time and distance of a vehicle, so we have to take help of the gradient boosting algo(yeah! because calculators aren’t available, can any scenario be more hypothetical than this!!) As a first step, let’s estimate the speed randomly. . Distance Time Speed Estimated Speed . 5 m | 1 h | 5 m/h | 4 m/h | . 6 m | 2 h | 3 m/h | 4 m/h | . 3 m | 0.5 h | 6 m/h | 4 m/h | . From the estimate, we’ll calculate the first set of residual(actual-predicted): . Distance Time Speed Estimated Speed First Residual . 5 m | 1 h | 5 m/h | 4 m/h | 1 | . 6 m | 2 h | 3 m/h | 4 m/h | -1 | . 3 m | 0.5 h | 6 m/h | 4 m/h | 2 | . Now, as the next step, the GBM will use the first residual as the target i.e. it will use Distance and time columns to predict the first residual. This will be the first residual model. Predicted residual is the column which is the prediction of this residual column. . Distance Time Speed Estimated Speed First Residual Predicted Residual 1 . 5 m | 1 h | 5 m/h | 4 m/h | 1 | 1.5 | . 6 m | 2 h | 3 m/h | 4 m/h | -1 | -1.2 | . 3 m | 0.5 h | 6 m/h | 4 m/h | 2 | 2.4 | . After this first model, the new estimated speed will be: Estimated speed + learning rate * predicted residual 1 If learning rate is 0.1, then new estimate for the first row will be: 4 + 0.1 * 1.5 = 4.15 which is a little closer to 5 than 4 is. . Distance Time Speed Estimated Speed First Residual Predicted Residual 1 New Estimate . 5 m | 1 h | 5 m/h | 4 m/h | 1 | 1.5 | 4.15 | . 6 m | 2 h | 3 m/h | 4 m/h | -1 | -1.2 | 3.88 | . 3 m | 0.5 h | 6 m/h | 4 m/h | 2 | 2.4 | 4.24 | . Use the New Estimate values to calculate second residuals(Speed - New estimate). . Distance Time Speed Estimated Speed First Residual Predicted Residual 1 New Estimate Second Residual . 5 m | 1 h | 5 m/h | 4 m/h | 1 | 1.5 | 4.15 | 0.75 | . 6 m | 2 h | 3 m/h | 4 m/h | -1 | -1.2 | 3.88 | -0.88 | . 3 m | 0.5 h | 6 m/h | 4 m/h | 2 | 2.4 | 4.24 | 1.76 | . As you see, that the second residuals are smaller than the first residual. . Now, for the second residual model, use the second residual as the target and predict a value. . Distance Time Speed Estimated Speed First Residual Predicted Residual 1 New Estimate Second Residual Predicted Residual 2 . 5 m | 1 h | 5 m/h | 4 m/h | 1 | 1.5 | 4.15 | 0.75 | 0.8 | . 6 m | 2 h | 3 m/h | 4 m/h | -1 | -1.2 | 3.88 | -0.88 | -0.6 | . 3 m | 0.5 h | 6 m/h | 4 m/h | 2 | 2.4 | 4.24 | 1.76 | 2 | . Again, use the New Estimate + learning rate * predicted residual 2 to update the estimated Speed, let’s call it new estiamte 2. . Distance Time Speed Estimated Speed First Residual Predicted Residual 1 New Estimate Second Residual Predicted Residual 2 New Estimate 2 . 5 m | 1 h | 5 m/h | 4 m/h | 1 | 1.5 | 4.15 | 0.75 | 0.8 | 4.23 | . 6 m | 2 h | 3 m/h | 4 m/h | -1 | -1.2 | 3.88 | -0.88 | -0.6 | 3.82 | . 3 m | 0.5 h | 6 m/h | 4 m/h | 2 | 2.4 | 4.24 | 1.76 | 2 | 4.44 | . Again, find the Third Residual by subtracting, New estimate 2 from speed and use the third residual as the target that your third residual model will predict. . This will keep going on until a stopping criteria is reached such as number of iterations, depth of a tree, time limit etc . Essentially, in this algorithm, we are boosting the gradient(though the learning param and new residuals calculated at each step) and it is helping in decreasing the overall error. . Also, as we can see that it is an additive model. . Estmated Speed + learning rate * predicted residual 1 + learning rate * predicted residual 2 + ... . That’s how Gradient Boosting works!! Quite simple :P .",
            "url": "https://prashantmdgl9.github.io/ml_experiments/2022/09/10/How-to-Explain-Gradient-Boosting.html",
            "relUrl": "/2022/09/10/How-to-Explain-Gradient-Boosting.html",
            "date": " • Sep 10, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Embeddings - A Game of Thrones Approach - What are they and how to create your own?",
            "content": "The motivation to write this piece came from the FastAI course, chapter 7 on collaborative filtering. I have used embeddings in the past and understood their context in the NLP world and the relevance but in the lecture Jeremy takes you under the hood and explains what an embedding layer is. It was an aha! moment for me and motivated me to go back to some old codes that I had saved and run them again. . In image processing tasks, we have images and images have RGB channels which quantify the redness, greenness,and the blueness of the image and that can be fed to a neural net or any other machine learning algorithm to learn about the image. . But there are problems that deal with the categorical data such as: . written text in which the relationship and context among the words isn&#39;t provided to us, or | recommendation systems in which the relationship between the users and products isn&#39;t available to us in any usable format. | . The relationship could be user from a particular age cohort, geography, or sex liking a particular product; or it could be words that are said in a particualr context such as &quot;Coffee is as good as water&quot; would have a differnt connotation in an airline review space vs a health magazine space. . How can we determine these relationships and contexts, so the ML models can use them? The answer is, we don&#39;t. We let the ML models learn them. . The method is straight forward, we assign random values for let&#39;s say users and products(it can be either a single float value or a vector of values for each of the user and the product), we call these random values as latent factors. | With some mathematical calculation, most likely a dot product, we find the approximate interaction between the user and product. | We compare this approximate interacion with the &quot;interaction values&quot; such as ratings, time spent, money spent, or any other relevant metric, and find how far off our approximations are. | We use Stochastic Gradient Descent to find the gradients and update the values that we had randomly assigned in the step 1 and repeat steps 2, 3, and 4 | These upadted values or vectors of certain length are the embeddings. They encode in them the intrinsic realtionship between various factors and the context for your particular problem at hand . In the beginning, those random values didn&#39;t mean anything as they were chosen randomly, but by the end of the traning, they do as they learn on the existing data about the hidden relationships. . This YouTube video has more information, especially the part from 1:18:30 to 1:20:10 . To calculate the result/interaction for a particular product and user combination, we have to look up the index of the product in our product latent factor matrix and the index of the user in our user latent factor matrix; then we can do our dot product between the two latent factor vectors. But look up in an index is not an operation deep learning models know how to do. They know how to do matrix products, and activation functions but not look ups. . We can represent look up in an index as a matrix product. The trick is to replace our indices with one-hot-encoded vectors e.g. for index 2 in a length 5 vector, the encoded vector would be [0, 1, 0, 0, 0] . Embedding is a computational shortcut of multiplying something with a one hot encoded vector . This multiplication with one-hot-encoded vectors is fine if it is done for only a few indices but for practical purposes creating too many enncoded vectors will cause memory management issues. . Most deep learning libraries avoid this problem by including a special layer that does this task of look up - they index into a vector using an intger. Also, the gradient calculated in such a manner that it is same as matrix multiplication with a one-hot-encoded vector. . This layer is called Embedding. . Now that we have some understanding of what an embedding is, let&#39;s try to create our own. . . Problem Statement :Create the word embeddings for the Game of Thrones . Library to use :Word2Vec by Gensim . Data is present on Kaggle here, it&#39;s a text format of the five books of famous Game of Thrones . Downloads . !pip install gensim !pip install nltk . Requirement already satisfied: gensim in /opt/conda/lib/python3.7/site-packages (4.0.1) Requirement already satisfied: numpy&gt;=1.11.3 in /opt/conda/lib/python3.7/site-packages (from gensim) (1.21.6) Requirement already satisfied: scipy&gt;=0.18.1 in /opt/conda/lib/python3.7/site-packages (from gensim) (1.7.3) Requirement already satisfied: smart-open&gt;=1.8.1 in /opt/conda/lib/python3.7/site-packages (from gensim) (5.2.1) WARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv Requirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (3.7) Requirement already satisfied: regex&gt;=2021.8.3 in /opt/conda/lib/python3.7/site-packages (from nltk) (2021.11.10) Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk) (8.0.4) Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk) (4.64.0) Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk) (1.0.1) Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click-&gt;nltk) (4.12.0) Requirement already satisfied: typing-extensions&gt;=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata-&gt;click-&gt;nltk) (4.3.0) Requirement already satisfied: zipp&gt;=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata-&gt;click-&gt;nltk) (3.8.0) WARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv . Imports and File reads . import glob import codecs import nltk from nltk.corpus import stopwords import re import multiprocessing import gensim.models.word2vec as w2v import sklearn.manifold import pandas as pd . path = &quot;../input/game-of-thrones-book-files/&quot; book_filenames = sorted(glob.glob(path + &quot;*.txt&quot;)) . print(&quot;Found books:&quot;) for file in book_filenames: print(file[36:]) . Found books: got1.txt got2.txt got3.txt got4.txt got5.txt . Combine the books into one long string . corpus_raw = u&quot;&quot; for book_filename in book_filenames: print(&quot;Reading &#39;{0}&#39;...&quot;.format(book_filename)) with codecs.open(book_filename, &quot;r&quot;, &quot;utf-8&quot;) as book_file: corpus_raw += book_file.read() print(&quot;Corpus is now {0} characters long&quot;.format(len(corpus_raw))) print() . Reading &#39;../input/game-of-thrones-book-files/got1.txt&#39;... Corpus is now 1770659 characters long Reading &#39;../input/game-of-thrones-book-files/got2.txt&#39;... Corpus is now 4071041 characters long Reading &#39;../input/game-of-thrones-book-files/got3.txt&#39;... Corpus is now 6391405 characters long Reading &#39;../input/game-of-thrones-book-files/got4.txt&#39;... Corpus is now 8107945 characters long Reading &#39;../input/game-of-thrones-book-files/got5.txt&#39;... Corpus is now 9719485 characters long . Data Preprocessing and Cleaning . Preprocess the data . Split the corpus into sentences . tokenizer = nltk.data.load(&#39;tokenizers/punkt/english.pickle&#39;) . nltk.download(&quot;punkt&quot;) nltk.download(&quot;stopwords&quot;) . [nltk_data] Downloading package punkt to /usr/share/nltk_data... [nltk_data] Package punkt is already up-to-date! [nltk_data] Downloading package stopwords to /usr/share/nltk_data... [nltk_data] Package stopwords is already up-to-date! . True . This is somehow taking too long, so I have created this raw_sentences list on local machine and uploaded for use here. . raw_sentences = tokenizer.tokenize(corpus_raw) . Reading the uploaded file . raw_sentences = [] with open(r&#39;../input/sentences-corpus/sentences.txt&#39;, &#39;r&#39;) as fp: for line in fp: x = line[:-1] raw_sentences.append(x) . len(raw_sentences) . 188855 . From sentences to List of words and remove any unnecessary special characters using re . def sentence_to_wordlist(raw): clean = re.sub(&quot;[^a-zA-Z]&quot;,&quot; &quot;, raw) words = clean.split() return words . stop_words = set(stopwords.words(&#39;english&#39;)) . Remove the stop words such as prepositions, conjunctions etc . filtered_sentences = [] for raw_sentence in raw_sentences: if len(raw_sentence) &gt; 0: a = sentence_to_wordlist(raw_sentence) filtered_sentences.append([w for w in a if not w.lower() in stop_words]) . len(filtered_sentences) . 157925 . c = 0 for x in filtered_sentences: c = c + len(x) print(&quot;The filtered corpus contains {0:,} tokens&quot;.format(c)) . The filtered corpus contains 917,562 tokens . As we can see above that the book corpus has 1,818,103 tokens while the filtered one that is sanitised version of the book has 917,562 tokens. . Word2Vec Model Training . Define the parameters for the Word2vec model . num_workers = multiprocessing.cpu_count() # Fror multi threading downsampling = 1e-3 # For frequently occurring words, downsample rate, so they don&#39;t overbeer the vocab seed = 42 # For replication of the results . wordVectors = w2v.Word2Vec( sg=1, # 1 for Skip gram, 0 for CBOW seed=seed, workers=num_workers, vector_size=300, # How many embeddings we want per word( can be any number) min_count=5, # The minimum number of occurences for the word to be considered in building the word vector window=7, # How many neighbours of the word need to be taken into account, for the context sample=downsampling ) . Build the vocab . wordVectors.build_vocab(filtered_sentences) . len(wordVectors.wv) . 16967 . _This is where we train the model, we can also pass len(wordVectors.corpus_length) for the total_words parameter_ . wordVectors.train(filtered_sentences, total_words= len(wordVectors.wv), epochs= 15) . (13178142, 13763430) . wordVectors.wv.vectors.shape . (16967, 300) . The model is trained, let&#39;s see what we have created. The shape tells us that there are 16,967 word vectors each with 300 components . Dense Matrix - T-SNE . The embeddings have 300 dimensions, so unless you&#39;re an extra-terrestrial being, you won&#39;t be able to visualise them. We&#39;ll use T-SNE to compress the 300 dimensions into 2 dimensions . tsne = sklearn.manifold.TSNE(n_components=2, random_state=0) . all_word_vectors_matrix = wordVectors.wv.vectors . all_word_vectors_matrix . array([[-0.15183511, -0.10445416, -0.15541989, ..., 0.14572449, 0.04010338, -0.00964977], [ 0.09000773, 0.00260416, -0.21182461, ..., 0.19929099, 0.07410478, 0.26346382], [ 0.00926181, -0.13957332, -0.12223979, ..., 0.12079053, -0.02541121, 0.05712756], ..., [ 0.00106871, -0.02786902, -0.03142978, ..., 0.0080044 , 0.01172699, 0.01934569], [ 0.00177492, -0.01036997, -0.0088923 , ..., 0.00428336, 0.007435 , 0.00769483], [ 0.00204526, -0.00720066, -0.00651641, ..., -0.00069571, -0.0011584 , 0.00415785]], dtype=float32) . Here we compress the 300d to 2d. What TSNE does is find the nearest neighbours and and preserves the neighbourhood structure while compressing, while PCA loses a lot of information while compressing. . all_word_vectors_matrix_2d = tsne.fit_transform(all_word_vectors_matrix) . /opt/conda/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from &#39;random&#39; to &#39;pca&#39; in 1.2. FutureWarning, /opt/conda/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to &#39;auto&#39; in 1.2. FutureWarning, . all_word_vectors_matrix_2d . array([[-22.1214 , -40.20547 ], [-22.737371 , -51.337734 ], [ -1.2034475, -43.987854 ], ..., [-39.63466 , 45.280827 ], [-68.378525 , -17.104765 ], [-23.636723 , 10.487 ]], dtype=float32) . Create a dataframe for plotting . df = pd.DataFrame( [ (word, coords[0], coords[1]) for word, coords in [ (word, all_word_vectors_matrix_2d[wordVectors.wv.key_to_index[word]]) for word in wordVectors.wv.index_to_key ] ], columns=[&quot;word&quot;, &quot;x&quot;, &quot;y&quot;] ) . The dataframe has the word and the x, y coordindates that can be plotted as a scatter plot . df.head(5) . word x y . 0 said | -22.121401 | -40.205471 | . 1 would | -22.737371 | -51.337734 | . 2 one | -1.203447 | -43.987854 | . 3 Lord | -21.235813 | -23.119661 | . 4 could | -22.683886 | -51.672825 | . Plotting . Bokeh and Altair provide much better visualisations . from bokeh.plotting import figure, output_file, show, output_notebook from bokeh.palettes import Category20 from bokeh.models import ColumnDataSource, Range1d, LabelSet, Label, CustomJS, Div, Button from bokeh.layouts import column, row, widgetbox from bokeh.models.widgets import Toggle from bokeh import events from bokeh.palettes import Spectral6 output_notebook() from bokeh.transform import linear_cmap from bokeh.transform import log_cmap # create a colour map #colourmap = {} #for idx, cat in enumerate(categories): colourmap[cat] = Category20[len(categories)][idx] #colors = [colourmap[x] for x in points[&#39;word&#39;]] # create data source source = ColumnDataSource(data=dict( x=df[&#39;x&#39;], y=df[&#39;y&#39;], name=df[&#39;word&#39;] )) TOOLTIPS = [ (&quot;word&quot;, &quot;@name&quot;), ] output_notebook() #output_file(filename=&quot;TSNE.html&quot;, title=&quot;Word Embeddings Game of Thrones&quot;) # create a new plot p = figure( tools=&quot;pan,box_zoom,reset,save&quot;, title=&quot;Word Embeddings Visualisation Using TSNE&quot;, tooltips=TOOLTIPS, plot_width=1000, plot_height=1000 ) mapper = linear_cmap(field_name=&#39;x&#39;, palette=&quot;Viridis256&quot; ,low=-80 ,high=60) # add some renderers p.circle(&#39;x&#39;, &#39;y&#39;, source=source, line_color=mapper, color=mapper) labels = LabelSet(x=&#39;x&#39;, y=&#39;y&#39;, text=&#39;name&#39;, level=&#39;underlay&#39;, x_offset=5, y_offset=5, source=source, render_mode=&#39;canvas&#39;, text_font_size=&quot;8pt&quot;) #p.add_layout(labels) # show the results show(p) . Loading BokehJS ... Loading BokehJS ... . For an interactive Bokeh plot, please click here. . Let&#39;s look at some results . This is done to find the coordinates of a word we are interested in the plot . df[df[&quot;word&quot;] == &quot;Jon&quot;] . word x y . 9 Jon | -8.131994 | -53.089184 | . wordVectors.wv.most_similar(&quot;Stark&quot;) . [(&#39;Ned&#39;, 0.871069610118866), (&#39;Eddard&#39;, 0.8589808344841003), (&#39;ward&#39;, 0.8371943831443787), (&#39;Greyjoy&#39;, 0.8356505036354065), (&#39;Arryn&#39;, 0.8288456201553345), (&#39;Edmure&#39;, 0.7985324859619141), (&#39;direwolf&#39;, 0.7942730784416199), (&#39;Winterfell&#39;, 0.7936611175537109), (&#39;Robb&#39;, 0.7925195693969727), (&#39;Tully&#39;, 0.7905474305152893)] . wordVectors.wv.most_similar(&quot;Jon&quot;) . [(&#39;Sam&#39;, 0.7944141030311584), (&#39;Catelyn&#39;, 0.7163089513778687), (&#39;Snow&#39;, 0.7153167724609375), (&#39;Qhorin&#39;, 0.7027862668037415), (&#39;Theon&#39;, 0.6976580619812012), (&#39;Jojen&#39;, 0.6847004890441895), (&#39;Bear&#39;, 0.683414101600647), (&#39;cry&#39;, 0.6755016446113586), (&#39;Ned&#39;, 0.673988938331604), (&#39;Mormont&#39;, 0.6729429364204407)] . wordVectors.wv.most_similar(&quot;Tyrion&quot;) . [(&#39;Varys&#39;, 0.7423267960548401), (&#39;Cersei&#39;, 0.7259597778320312), (&#39;Jaime&#39;, 0.7211229205131531), (&#39;Bronn&#39;, 0.7203950881958008), (&#39;Dany&#39;, 0.7153236865997314), (&#39;Littlefinger&#39;, 0.6874520778656006), (&#39;Catelyn&#39;, 0.6540576815605164), (&#39;Davos&#39;, 0.6521568298339844), (&#39;queen&#39;, 0.6493181586265564), (&#39;moment&#39;, 0.6428911685943604)] . Of course, the results are as they should be . . wordVectors.wv.most_similar(&quot;Cersei&quot;) . [(&#39;Jaime&#39;, 0.9132179021835327), (&#39;dwarf&#39;, 0.8904335498809814), (&#39;Imp&#39;, 0.8688753247261047), (&#39;Lancel&#39;, 0.8603962659835815), (&#39;Joffrey&#39;, 0.8597180843353271), (&#39;Joff&#39;, 0.8585003614425659), (&#39;queen&#39;, 0.8486377000808716), (&#39;sister&#39;, 0.8455819487571716), (&#39;Littlefinger&#39;, 0.8399965763092041), (&#39;Kingslayer&#39;, 0.8309287428855896)] . . Altair Visuals . alt.data_transformers.disable_max_rows() . DataTransformerRegistry.enable(&#39;default&#39;) . import altair as alt source = df alt.Chart(source).mark_circle(size=60).encode( x=&#39;x&#39;, y=&#39;y&#39;, tooltip=[&#39;word&#39;] ).properties( width=900, height=800 ).interactive() . Change the parameters for creating the models and see how the results are affected. . It&#39;s interesting that how embeddings bridge the gap between words and numbers. In one of the models at work, I even passed them as a parameter to a regression model and the beta value was significant! . End of Notebook .",
            "url": "https://prashantmdgl9.github.io/ml_experiments/2022/09/04/a-detailed-explanation-and-creation-of-embeddings.html",
            "relUrl": "/2022/09/04/a-detailed-explanation-and-creation-of-embeddings.html",
            "date": " • Sep 4, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "What do you see when you look up?",
            "content": "This work is done in order to understand the APIs of FastAI in a better fashion and how the model tuning works in an incremental fashion . Problem Statement - Classify the Clouds in one of the ten cohorts . The categories are Altocumulus, Altostratus, Cirroculumulus, Cirrostratus, Cirrus, Cumulonimbus, Cumulus, Nimbostratus, Stratocumulus, Stratus . Data - The data analysed in the present work is present here . Some information about the clouds . Clouds are classified according to their height above and appearance (texture) from the ground. . The following cloud roots and translations summarize the components of this classification system . Cirro-:curl of hair, high; &gt; - Alto-:mid; &gt; - Strato-:layer;&gt; - Nimbo-:rain, precipitation;&gt; - Cumulo-:heap. | . Imports and Downloads . import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import os input_dir = &quot;/kaggle/input/&quot; dirs = [&quot;howard-cloudx&quot;, &quot;clouds&quot;, &quot;cloudtest&quot;] for d in dirs: for dirname, _, filenames in os.walk(input_dir + d): counter = 0 for filename in filenames: counter = counter + 1 print(f&quot;Number of files loaded:{counter}&quot;) . Number of files loaded:0 Number of files loaded:0 Number of files loaded:0 Number of files loaded:25 Number of files loaded:25 Number of files loaded:25 Number of files loaded:25 Number of files loaded:25 Number of files loaded:25 Number of files loaded:25 Number of files loaded:25 Number of files loaded:25 Number of files loaded:25 Number of files loaded:0 Number of files loaded:106 Number of files loaded:112 Number of files loaded:107 Number of files loaded:100 Number of files loaded:162 Number of files loaded:104 Number of files loaded:106 Number of files loaded:163 Number of files loaded:100 Number of files loaded:111 Number of files loaded:0 Number of files loaded:0 Number of files loaded:0 Number of files loaded:25 Number of files loaded:25 Number of files loaded:25 Number of files loaded:25 Number of files loaded:25 Number of files loaded:25 Number of files loaded:25 Number of files loaded:25 Number of files loaded:0 Number of files loaded:106 Number of files loaded:112 Number of files loaded:107 Number of files loaded:100 Number of files loaded:162 Number of files loaded:106 Number of files loaded:163 Number of files loaded:100 Number of files loaded:3 . Download Timm for the pyTorch models . ! pip install timm . Collecting timm Downloading timm-0.6.7-py3-none-any.whl (509 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 510.0/510.0 kB 2.1 MB/s eta 0:00:00a 0:00:01 Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm) (0.12.0) Requirement already satisfied: torch&gt;=1.4 in /opt/conda/lib/python3.7/site-packages (from timm) (1.11.0) Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch&gt;=1.4-&gt;timm) (4.3.0) Requirement already satisfied: pillow!=8.3.*,&gt;=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision-&gt;timm) (9.1.1) Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision-&gt;timm) (1.21.6) Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision-&gt;timm) (2.28.1) Requirement already satisfied: charset-normalizer&lt;3,&gt;=2 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;torchvision-&gt;timm) (2.1.0) Requirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;torchvision-&gt;timm) (3.3) Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;torchvision-&gt;timm) (1.26.12) Requirement already satisfied: certifi&gt;=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;torchvision-&gt;timm) (2022.6.15) Installing collected packages: timm Successfully installed timm-0.6.7 WARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv . from fastai.vision.all import * from fastai import * from fastai.vision import * from fastai.vision.widgets import * import timm . Set the relevant paths . path = Path(&#39;../input/howard-cloudx/Howard-Cloud-X/&#39;) path.ls() . (#2) [Path(&#39;../input/howard-cloudx/Howard-Cloud-X/test&#39;),Path(&#39;../input/howard-cloudx/Howard-Cloud-X/train&#39;)] . train = (path/&quot;train&quot;) valid = (path/&quot;test&quot;) . len(train.ls()) . 10 . fnames = get_image_files(path) . fnames . (#1420) [Path(&#39;../input/howard-cloudx/Howard-Cloud-X/test/Cirroculumulus/7bc6ae9d-9f32-4e42-8927-5561fa5d6cf5.jpg&#39;),Path(&#39;../input/howard-cloudx/Howard-Cloud-X/test/Cirroculumulus/7aa35bb4-4779-4d28-8bc7-f6ebde621dc1.jpg&#39;),Path(&#39;../input/howard-cloudx/Howard-Cloud-X/test/Cirroculumulus/0b7f52be-982c-4d98-b8f8-2630dbd77472.jpg&#39;),Path(&#39;../input/howard-cloudx/Howard-Cloud-X/test/Cirroculumulus/8bd1cfbb-663e-4760-ae12-dc4a35f84648.jpg&#39;),Path(&#39;../input/howard-cloudx/Howard-Cloud-X/test/Cirroculumulus/584d01a2-a9c8-4160-8186-434b617d7c3f.jpg&#39;),Path(&#39;../input/howard-cloudx/Howard-Cloud-X/test/Cirroculumulus/5d144b98-9413-49e5-8cd6-ff866b83eec5.jpg&#39;),Path(&#39;../input/howard-cloudx/Howard-Cloud-X/test/Cirroculumulus/5bde98f2-49c0-476f-be6c-52172ecb120c.jpg&#39;),Path(&#39;../input/howard-cloudx/Howard-Cloud-X/test/Cirroculumulus/3eb68ded-346c-4003-b02f-615e37e100e9.jpg&#39;),Path(&#39;../input/howard-cloudx/Howard-Cloud-X/test/Cirroculumulus/00a82e59-a751-4668-81fe-1b1ad638749f.jpg&#39;),Path(&#39;../input/howard-cloudx/Howard-Cloud-X/test/Cirroculumulus/1b7312b1-d774-4a03-8fa3-25773ac6f358.jpg&#39;)...] . dblock = DataBlock() dsets = dblock.datasets(fnames) dsets.train[0] len(dsets.train), len(dsets.valid) . (1136, 284) . Attempt 1 . Use a vanilla datablock, imitate the various tutorials and see how is the result . Model used - Resnet 34 . Error - 45% . Item transformation is needed because all the images are of different sizes, the deep models generally stack all the images together and flatten them, that&#39;s why it is imperative to resize them to same dimensions. . dblock = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items = get_image_files, get_y = parent_label, splitter = RandomSplitter(), item_tfms = Resize(300, method = ResizeMethod.Squish), batch_tfms=aug_transforms(size= 224)) . We also have used batch transformations which is Data augmentation here. Each batch of the images will be augmented( orientation change, crop randomly, darken, lighten etc) and then the size will be changed to the size we supplied. . dsets = dblock.datasets(path) . dsets.vocab . [&#39;Altocumulus&#39;, &#39;Altostratus&#39;, &#39;Cirroculumulus&#39;, &#39;Cirrostratus&#39;, &#39;Cirrus&#39;, &#39;Cumulonimbus&#39;, &#39;Cumulus&#39;, &#39;Nimbostratus&#39;, &#39;Stratocumulus&#39;, &#39;Stratus&#39;] . dls = dblock.dataloaders(path) dls.show_batch() . learn = vision_learner(dls, resnet34, metrics=error_rate) learn.lr_find() . Downloading: &#34;https://download.pytorch.org/models/resnet34-b627a593.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth . SuggestedLRs(valley=0.0005754399462603033) . learn.fine_tune(5, 3e-3) . epoch train_loss valid_loss error_rate time . 0 | 3.113337 | 2.028698 | 0.588028 | 00:50 | . epoch train_loss valid_loss error_rate time . 0 | 2.064874 | 1.747166 | 0.609155 | 00:48 | . 1 | 1.752833 | 1.704080 | 0.510563 | 00:48 | . 2 | 1.523126 | 1.648633 | 0.482394 | 00:47 | . 3 | 1.290462 | 1.527772 | 0.461268 | 00:46 | . 4 | 1.103239 | 1.472800 | 0.450704 | 00:48 | . 45% error rate is high . Keep calling the garbage collector as these models are large . import gc _ = gc.collect() . Attempt 2 . Use Image data loaders, provide train and test data separately. . Model used - Convnext_small_in22k . Accuracy - 59-60% . dls_2 = ImageDataLoaders.from_folder(path, train = &quot;train&quot;, valid = &quot;test&quot;,vocab = dsets.vocab, bs = 64, item_tfms = Resize(300, method = ResizeMethod.Squish), batch_tfms=aug_transforms(size= 224)) #batch_tfms=[*aug_transforms(size=224),Normalize.from_stats(*imagenet_stats)]) . dls_2.show_batch() . learn_2 = vision_learner(dls_2, &quot;convnext_small_in22k&quot;, model_dir=&quot;/tmp/model/&quot;, metrics=accuracy) . Downloading: &#34;https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_224.pth&#34; to /root/.cache/torch/hub/checkpoints/convnext_small_22k_224.pth . learn_2.fine_tune(10, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 2.892889 | 2.082639 | 0.436000 | 01:08 | . epoch train_loss valid_loss accuracy time . 0 | 1.785524 | 1.654420 | 0.496000 | 01:16 | . 1 | 1.653368 | 1.497555 | 0.532000 | 01:15 | . 2 | 1.462549 | 1.604444 | 0.532000 | 01:12 | . 3 | 1.318390 | 1.526556 | 0.508000 | 00:56 | . 4 | 1.201218 | 1.520712 | 0.536000 | 00:58 | . 5 | 1.087901 | 1.508507 | 0.560000 | 00:57 | . 6 | 0.971522 | 1.487124 | 0.544000 | 00:57 | . 7 | 0.869471 | 1.502575 | 0.544000 | 01:00 | . 8 | 0.774482 | 1.483035 | 0.556000 | 01:00 | . 9 | 0.720841 | 1.477164 | 0.556000 | 00:59 | . learn_2.fine_tune(10, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 0.651584 | 1.675648 | 0.560000 | 00:48 | . epoch train_loss valid_loss accuracy time . 0 | 0.611424 | 1.463446 | 0.584000 | 00:57 | . 1 | 0.557592 | 1.490726 | 0.584000 | 00:57 | . 2 | 0.535933 | 1.540918 | 0.580000 | 00:56 | . 3 | 0.532068 | 1.563381 | 0.552000 | 00:56 | . 4 | 0.505497 | 1.657638 | 0.536000 | 00:57 | . 5 | 0.488271 | 1.666002 | 0.564000 | 00:56 | . 6 | 0.449273 | 1.650724 | 0.560000 | 00:56 | . 7 | 0.415201 | 1.592716 | 0.556000 | 00:57 | . 8 | 0.380137 | 1.578643 | 0.556000 | 00:57 | . 9 | 0.358816 | 1.575698 | 0.552000 | 00:56 | . learn_2.fine_tune(5, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 0.392105 | 1.610668 | 0.540000 | 01:08 | . epoch train_loss valid_loss accuracy time . 0 | 0.370500 | 1.589147 | 0.556000 | 01:14 | . 1 | 0.364840 | 1.605959 | 0.568000 | 01:14 | . 2 | 0.338760 | 1.648296 | 0.580000 | 01:16 | . 3 | 0.315394 | 1.643008 | 0.600000 | 01:16 | . 4 | 0.281450 | 1.620948 | 0.592000 | 01:16 | . Validation loss is very bumpy . learn_2.show_results() . Attempt 3 . Similar to attempt 2, change in bacth size and model . Model used - Convnext_tiny_hnf . Accuracy - 59% . dls_3 = ImageDataLoaders.from_folder(path, train = &quot;train&quot;, valid = &quot;test&quot;,vocab = dsets.vocab, bs = 32, item_tfms = Resize(400, method = ResizeMethod.Squish), batch_tfms=aug_transforms(size= 300)) #batch_tfms=[*aug_transforms(size=224),Normalize.from_stats(*imagenet_stats)]) . learn_3 = vision_learner(dls_3, &quot;convnext_tiny_hnf&quot;, model_dir=&quot;/tmp/model/&quot;, metrics=accuracy) . Downloading: &#34;https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/convnext_tiny_hnf_a2h-ab7e9df2.pth&#34; to /root/.cache/torch/hub/checkpoints/convnext_tiny_hnf_a2h-ab7e9df2.pth . learn_3.fine_tune(10, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 2.775865 | 2.170268 | 0.392000 | 00:50 | . epoch train_loss valid_loss accuracy time . 0 | 1.705282 | 1.659915 | 0.464000 | 01:03 | . 1 | 1.510822 | 1.677615 | 0.500000 | 01:02 | . 2 | 1.389804 | 1.828296 | 0.484000 | 01:05 | . 3 | 1.243197 | 1.696750 | 0.548000 | 01:02 | . 4 | 1.028381 | 1.620666 | 0.536000 | 01:04 | . 5 | 0.865795 | 1.609255 | 0.596000 | 01:04 | . 6 | 0.722874 | 1.573360 | 0.572000 | 01:05 | . 7 | 0.598872 | 1.562924 | 0.576000 | 01:05 | . 8 | 0.526616 | 1.588943 | 0.596000 | 01:05 | . 9 | 0.483461 | 1.561903 | 0.572000 | 01:02 | . learn_3.fine_tune(5, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 0.480600 | 1.774047 | 0.548000 | 00:51 | . epoch train_loss valid_loss accuracy time . 0 | 0.511615 | 1.587777 | 0.580000 | 01:03 | . 1 | 0.579787 | 1.852812 | 0.564000 | 01:05 | . 2 | 0.533436 | 1.827137 | 0.576000 | 01:03 | . 3 | 0.445922 | 1.816562 | 0.576000 | 01:03 | . 4 | 0.378245 | 1.790648 | 0.596000 | 01:05 | . Not much improvement here . learn_3.show_results() . A lot of confused classes . Use interp = ClassificationInterpretation.from_learner(learn_3) . interp.most_confused(min_val =3) . [(&#39;Altocumulus&#39;, &#39;Cirroculumulus&#39;, 7), (&#39;Stratus&#39;, &#39;Stratocumulus&#39;, 7), (&#39;Altocumulus&#39;, &#39;Altostratus&#39;, 6), (&#39;Stratocumulus&#39;, &#39;Stratus&#39;, 6), (&#39;Cirrostratus&#39;, &#39;Altostratus&#39;, 5), (&#39;Cirrostratus&#39;, &#39;Cirrus&#39;, 5), (&#39;Cirrus&#39;, &#39;Cirrostratus&#39;, 5), (&#39;Cumulus&#39;, &#39;Cumulonimbus&#39;, 4), (&#39;Cirrostratus&#39;, &#39;Altocumulus&#39;, 3), (&#39;Cirrostratus&#39;, &#39;Cirroculumulus&#39;, 3), (&#39;Cumulonimbus&#39;, &#39;Cumulus&#39;, 3)] . Attempt 4 . Train more, use default data augmentation and change model to convnext base . Model used - Convnext_base . Accuracy - 61% . dls_4 = ImageDataLoaders.from_folder(path, train = &quot;train&quot;, valid = &quot;test&quot;,vocab = dsets.vocab, bs = 32, item_tfms = Resize(128), batch_tfms=aug_transforms()) #batch_tfms=[*aug_transforms(size=224),Normalize.from_stats(*imagenet_stats)]) . learn_4 = vision_learner(dls_4, &quot;convnext_base&quot;, model_dir=&quot;/tmp/model/&quot;, metrics=accuracy) . Downloading: &#34;https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth&#34; to /root/.cache/torch/hub/checkpoints/convnext_base_1k_224_ema.pth . Trained once and then train again . learn_4.fine_tune(10, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 3.028600 | 2.383297 | 0.348000 | 00:40 | . epoch train_loss valid_loss accuracy time . 0 | 2.147752 | 1.833840 | 0.404000 | 00:42 | . 1 | 1.912063 | 1.745342 | 0.456000 | 00:42 | . 2 | 1.813161 | 1.781601 | 0.488000 | 00:43 | . 3 | 1.630206 | 1.714715 | 0.476000 | 00:41 | . 4 | 1.467049 | 1.503493 | 0.528000 | 00:42 | . 5 | 1.309382 | 1.521237 | 0.560000 | 00:42 | . 6 | 1.134238 | 1.565486 | 0.556000 | 00:42 | . 7 | 0.990034 | 1.484479 | 0.544000 | 00:42 | . 8 | 0.902086 | 1.504959 | 0.536000 | 00:42 | . 9 | 0.880721 | 1.513757 | 0.540000 | 00:41 | . turns = 2 for i in range(turns): learn_4.fine_tune(10, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 0.995900 | 1.701401 | 0.560000 | 00:41 | . epoch train_loss valid_loss accuracy time . 0 | 0.912602 | 1.510895 | 0.536000 | 00:42 | . 1 | 0.865395 | 1.549080 | 0.576000 | 00:42 | . 2 | 0.875826 | 1.717728 | 0.512000 | 00:42 | . 3 | 0.862483 | 1.573677 | 0.548000 | 00:41 | . 4 | 0.786304 | 1.639315 | 0.556000 | 00:42 | . 5 | 0.735020 | 1.523650 | 0.596000 | 00:42 | . 6 | 0.648714 | 1.536606 | 0.560000 | 00:42 | . 7 | 0.574546 | 1.554603 | 0.568000 | 00:43 | . 8 | 0.524813 | 1.570686 | 0.576000 | 00:44 | . 9 | 0.495079 | 1.571282 | 0.576000 | 00:43 | . epoch train_loss valid_loss accuracy time . 0 | 0.551824 | 1.720386 | 0.560000 | 00:39 | . epoch train_loss valid_loss accuracy time . 0 | 0.547203 | 1.604538 | 0.572000 | 00:43 | . 1 | 0.494175 | 1.651787 | 0.580000 | 00:42 | . 2 | 0.553949 | 1.665809 | 0.580000 | 00:42 | . 3 | 0.550795 | 1.639198 | 0.588000 | 00:44 | . 4 | 0.509917 | 1.724905 | 0.564000 | 00:42 | . 5 | 0.497022 | 1.643369 | 0.580000 | 00:42 | . 6 | 0.448560 | 1.632840 | 0.596000 | 00:43 | . 7 | 0.384618 | 1.619914 | 0.604000 | 00:42 | . 8 | 0.346280 | 1.624194 | 0.600000 | 00:43 | . 9 | 0.329392 | 1.632581 | 0.612000 | 00:42 | . learn_4.show_results() . interp = ClassificationInterpretation.from_learner(learn_4) interp.plot_confusion_matrix() . The confusion matrix shows that certain classes are prone to getting misclassified. Let&#39;s have one more attempt and then see what&#39;s happening . Altocumulus, Stratus, Stratocumulus, and Cirrorstratus don&#39;t produce good results . Attempt 5 . Data augmentation according to Imagenet statistics . Model used - Convnext_small_in22k . Accuracy - 59% . Empty the torch cache to avoid the CUDA out of memory error . torch.cuda.empty_cache() . dls_5 = ImageDataLoaders.from_folder(path, train = &quot;train&quot;, valid = &quot;test&quot;,vocab = dsets.vocab, bs = 64, item_tfms = Resize(300, method = ResizeMethod.Squish), #batch_tfms=aug_transforms(size= 224)) batch_tfms=[*aug_transforms(size=224),Normalize.from_stats(*imagenet_stats)]) . learn_5 = vision_learner(dls_5, &quot;convnext_small_in22k&quot;, model_dir=&quot;/tmp/model/&quot;, metrics=[accuracy]) . turns = 2 for i in range(turns): learn_5.fine_tune(10, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 2.932875 | 1.975686 | 0.472000 | 00:50 | . epoch train_loss valid_loss accuracy time . 0 | 1.765770 | 1.708579 | 0.516000 | 00:59 | . 1 | 1.627048 | 1.535210 | 0.588000 | 01:00 | . 2 | 1.466308 | 1.609761 | 0.532000 | 01:01 | . 3 | 1.335094 | 1.660312 | 0.560000 | 01:01 | . 4 | 1.179072 | 1.543942 | 0.544000 | 00:59 | . 5 | 1.062035 | 1.544576 | 0.568000 | 00:59 | . 6 | 0.941348 | 1.508574 | 0.556000 | 00:59 | . 7 | 0.847385 | 1.469271 | 0.576000 | 00:59 | . 8 | 0.760762 | 1.449720 | 0.584000 | 01:00 | . 9 | 0.686221 | 1.446685 | 0.584000 | 00:59 | . epoch train_loss valid_loss accuracy time . 0 | 0.697822 | 1.632761 | 0.548000 | 00:49 | . epoch train_loss valid_loss accuracy time . 0 | 0.647399 | 1.456817 | 0.584000 | 01:00 | . 1 | 0.600847 | 1.467131 | 0.580000 | 00:58 | . 2 | 0.592070 | 1.505024 | 0.588000 | 00:59 | . 3 | 0.560953 | 1.572787 | 0.608000 | 00:59 | . 4 | 0.530090 | 1.550739 | 0.588000 | 00:59 | . 5 | 0.501039 | 1.511042 | 0.612000 | 01:00 | . 6 | 0.453741 | 1.523643 | 0.580000 | 01:01 | . 7 | 0.410653 | 1.509357 | 0.596000 | 00:59 | . 8 | 0.377629 | 1.516859 | 0.584000 | 01:00 | . 9 | 0.350783 | 1.506683 | 0.588000 | 00:59 | . learn_5.show_results() . interp = ClassificationInterpretation.from_learner(learn_5) interp.plot_confusion_matrix() . interp.most_confused(min_val =4) . [(&#39;Cirrostratus&#39;, &#39;Altostratus&#39;, 7), (&#39;Altocumulus&#39;, &#39;Cirroculumulus&#39;, 6), (&#39;Stratocumulus&#39;, &#39;Stratus&#39;, 6), (&#39;Stratus&#39;, &#39;Stratocumulus&#39;, 6), (&#39;Altocumulus&#39;, &#39;Altostratus&#39;, 5), (&#39;Cumulonimbus&#39;, &#39;Cumulus&#39;, 5), (&#39;Stratocumulus&#39;, &#39;Nimbostratus&#39;, 5), (&#39;Cumulus&#39;, &#39;Cumulonimbus&#39;, 4), (&#39;Nimbostratus&#39;, &#39;Cumulus&#39;, 4)] . As it turns out that even in this iteration Altocumulus, Cirrostratus, and Stratocumulus are getting misclassified a lot . I am no cloud expert, so googling to see the difference between these clouds. . This primer provides a quick overview. . Cirrostratus has a tendency to get misclassified in Cirro category clouds viz. Cirrus and Cirrocumulus. Also, it gets classified as Altostratus . Similarly, Altocumulus looks a lot like Altostratus and Cirrocumulus. Thus, a lot of misclassification. . There are other misclassifications too but for now, let&#39;s see how the model behaves when we remove two noisy classes out of the three. I choose Altocumulus and Cirrostratus randomly and would see the result. . There should be better methods to improve the efficiency:&gt; - Remove noisy class&gt; - Balance the classes . Get more samples per class | Data preprocessing | . For now, let&#39;s remove the noisy class and see how the model performs . Changed Data . The clouds directory has the updated data . Attempt 6 . Build the model on updated dataset and use the parameters from the test that performed the best . Model used - Convnext_small_in22k . Accuracy - 70% . for dirname, _, filenames in os.walk(&#39;/kaggle/input/clouds/Clouds&#39;): counter = 0 for filename in filenames: counter = counter + 1 #print(os.path.join(dirname, filename)) print(f&quot;Number of files loaded:{counter}&quot;) . Number of files loaded:0 Number of files loaded:0 Number of files loaded:25 Number of files loaded:25 Number of files loaded:25 Number of files loaded:25 Number of files loaded:25 Number of files loaded:25 Number of files loaded:25 Number of files loaded:25 Number of files loaded:0 Number of files loaded:106 Number of files loaded:112 Number of files loaded:107 Number of files loaded:100 Number of files loaded:162 Number of files loaded:106 Number of files loaded:163 Number of files loaded:100 . path_n = Path(&#39;../input/clouds/Clouds&#39;) path_n.ls() . (#2) [Path(&#39;../input/clouds/Clouds/test&#39;),Path(&#39;../input/clouds/Clouds/train&#39;)] . _This part isn&#39;t really needed, one can create a list of classes for dsetsn . dblock_n = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items = get_image_files, get_y = parent_label, splitter = RandomSplitter(), item_tfms = Resize(300, method = ResizeMethod.Squish), batch_tfms=aug_transforms(size= 224)) dsets_n = dblock_n.datasets(path_n) dsets_n.vocab . [&#39;Altostratus&#39;, &#39;Cirroculumulus&#39;, &#39;Cirrus&#39;, &#39;Cumulonimbus&#39;, &#39;Cumulus&#39;, &#39;Nimbostratus&#39;, &#39;Stratocumulus&#39;, &#39;Stratus&#39;] . Use the same configuration of batch size 32, image resize, and batch transformations . dls_6 = ImageDataLoaders.from_folder(path_n, train = &quot;train&quot;, valid = &quot;test&quot;,vocab = dsets_n.vocab, bs = 32, item_tfms = Resize(128), batch_tfms=aug_transforms()) . learn_6 = vision_learner(dls_6, &quot;convnext_small_in22k&quot;, model_dir=&quot;/tmp/model/&quot;, metrics=accuracy) . Try to retrain the entire model from scratch instead of using pretrained weights from the transfer learning model . learn_6.fit_one_cycle(3, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 2.518435 | 1.781701 | 0.555000 | 00:30 | . 1 | 1.950543 | 1.266906 | 0.595000 | 00:28 | . 2 | 1.560208 | 1.127361 | 0.620000 | 00:26 | . learn_6.unfreeze() . learn_6.fit_one_cycle(10) . epoch train_loss valid_loss accuracy time . 0 | 1.218273 | 1.534749 | 0.550000 | 00:29 | . 1 | 1.637923 | 3.227510 | 0.345000 | 00:29 | . 2 | 1.940466 | 4.200366 | 0.140000 | 00:29 | . 3 | 1.838173 | 11.386209 | 0.215000 | 00:29 | . 4 | 1.644612 | 5.846445 | 0.240000 | 00:29 | . 5 | 1.447882 | 2.120700 | 0.415000 | 00:29 | . 6 | 1.259362 | 2.177001 | 0.495000 | 00:28 | . 7 | 1.076406 | 1.193845 | 0.620000 | 00:29 | . 8 | 0.922181 | 1.021270 | 0.680000 | 00:29 | . 9 | 0.804969 | 1.049691 | 0.670000 | 00:30 | . Not much improvement, fine tune to see if there is any change. . learn_6.fine_tune(10, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 0.777473 | 1.511383 | 0.655000 | 00:28 | . epoch train_loss valid_loss accuracy time . 0 | 0.701466 | 1.232059 | 0.645000 | 00:29 | . 1 | 0.694632 | 1.230741 | 0.665000 | 00:29 | . 2 | 0.689335 | 1.308101 | 0.650000 | 00:29 | . 3 | 0.656844 | 1.264420 | 0.720000 | 00:30 | . 4 | 0.618628 | 1.193656 | 0.715000 | 00:29 | . 5 | 0.592718 | 1.226670 | 0.680000 | 00:29 | . 6 | 0.567708 | 1.216985 | 0.695000 | 00:29 | . 7 | 0.532835 | 1.179013 | 0.700000 | 00:29 | . 8 | 0.509911 | 1.218369 | 0.670000 | 00:29 | . 9 | 0.479909 | 1.211526 | 0.680000 | 00:29 | . learn_6.fine_tune(10, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 0.562977 | 1.306425 | 0.695000 | 00:28 | . epoch train_loss valid_loss accuracy time . 0 | 0.552285 | 1.275757 | 0.685000 | 00:28 | . 1 | 0.485817 | 1.299778 | 0.675000 | 00:29 | . 2 | 0.491668 | 1.238084 | 0.690000 | 00:29 | . 3 | 0.511293 | 1.329848 | 0.680000 | 00:30 | . 4 | 0.494083 | 1.386478 | 0.700000 | 00:29 | . 5 | 0.464426 | 1.351498 | 0.685000 | 00:29 | . 6 | 0.455210 | 1.327508 | 0.685000 | 00:29 | . 7 | 0.435786 | 1.300146 | 0.665000 | 00:29 | . 8 | 0.406114 | 1.265782 | 0.695000 | 00:29 | . 9 | 0.387949 | 1.260656 | 0.700000 | 00:29 | . Validation loss is very bumpy . It can mean that the learning rate isn&#39;t set correctly. . learn_6.recorder.plot_loss() . learn_6.show_results() . The Confusion Matrix looks a little better than before . interp = ClassificationInterpretation.from_learner(learn_6) interp.plot_confusion_matrix() . Attempt 7 . Use a smaller model and use different learning rates . Model used - Convnext_small . Accuracy - 72.5% . dls_7 = ImageDataLoaders.from_folder(path_n, train = &quot;train&quot;, valid = &quot;test&quot;,vocab = dsets_n.vocab, bs = 32, item_tfms = Resize(224), batch_tfms=aug_transforms()) . learn_7 = vision_learner(dls_7, &quot;convnext_small&quot;, model_dir=&quot;/tmp/model/&quot;, metrics=accuracy) . Downloading: &#34;https://dl.fbaipublicfiles.com/convnext/convnext_small_1k_224_ema.pth&#34; to /root/.cache/torch/hub/checkpoints/convnext_small_1k_224_ema.pth . learn_7.lr_find() . SuggestedLRs(valley=0.0014454397605732083) . This has been a good lesson. Use high larger learning rate when you start the training and then decrease it progressively. . FastAI provides discriminative learning rate using splice but we are doing it manually here . lr = [2e-2, 3e-3] for i in range(len(lr)): learn_7.fine_tune(10, lr[i]) . epoch train_loss valid_loss accuracy time . 0 | 2.758011 | 2.937358 | 0.410000 | 00:32 | . epoch train_loss valid_loss accuracy time . 0 | 1.701880 | 1.546097 | 0.565000 | 00:37 | . 1 | 1.466775 | 1.456164 | 0.595000 | 00:38 | . 2 | 1.434179 | 1.275465 | 0.620000 | 00:37 | . 3 | 1.335906 | 1.562193 | 0.580000 | 00:39 | . 4 | 1.186197 | 1.245194 | 0.655000 | 00:38 | . 5 | 1.023701 | 1.149056 | 0.660000 | 00:38 | . 6 | 0.841730 | 1.275522 | 0.680000 | 00:37 | . 7 | 0.666139 | 1.119972 | 0.700000 | 00:38 | . 8 | 0.524536 | 1.110425 | 0.710000 | 00:38 | . 9 | 0.438384 | 1.070021 | 0.715000 | 00:38 | . epoch train_loss valid_loss accuracy time . 0 | 0.343739 | 1.162611 | 0.715000 | 00:32 | . epoch train_loss valid_loss accuracy time . 0 | 0.339435 | 1.150641 | 0.715000 | 00:38 | . 1 | 0.332937 | 1.089203 | 0.715000 | 00:38 | . 2 | 0.351498 | 1.137119 | 0.690000 | 00:38 | . 3 | 0.324245 | 1.143893 | 0.720000 | 00:38 | . 4 | 0.302303 | 1.170959 | 0.705000 | 00:38 | . 5 | 0.279240 | 1.209391 | 0.700000 | 00:38 | . 6 | 0.272465 | 1.129670 | 0.720000 | 00:38 | . 7 | 0.260656 | 1.139702 | 0.720000 | 00:39 | . 8 | 0.245559 | 1.133754 | 0.720000 | 00:39 | . 9 | 0.235748 | 1.133464 | 0.725000 | 00:38 | . Still a little bumpy validation loss but accuracy seems stable . learn_7.recorder.plot_loss() . learn_7.show_results() . interp = ClassificationInterpretation.from_learner(learn_7) interp.plot_confusion_matrix() . learn_7.loss_func . FlattenedLoss of CrossEntropyLoss() . learn_7.opt . &lt;fastai.optimizer.Optimizer at 0x7f3511212d10&gt; . Attempt 8 - Final attempt . Change the loss function; Decrease the size of the model . Model used - Convnext_tiny . Accuracy - 71.5% . dls_8 = ImageDataLoaders.from_folder(path_n, train = &quot;train&quot;, valid = &quot;test&quot;,vocab = dsets_n.vocab, bs = 32, item_tfms = Resize(224), batch_tfms=aug_transforms(size= 128)) . The Binary cross entropy loss would penalise those preductions that are confident but wrong. . Focal loss disccussed in this paper https://arxiv.org/pdf/1708.02002.pdf provides more control over the penalisation with the help of a gamma parameter. Gamma = 0 behaves similar to Cross Entropy Loss but higher values of gamma (0 to 2) change this behaviour. . Higher values of gamma down-weight easy examples&#8217; contribution to loss . learn_8 = vision_learner(dls_8, &quot;convnext_tiny&quot;, model_dir=&quot;/tmp/model/&quot;, loss_func= FocalLossFlat(gamma = 1.5),metrics=accuracy) . Just checking whether loss function has updated or not!!! (Trust issues) :P . learn_8.loss_func . FlattenedLoss of FocalLoss() . learn_8.lr_find() . SuggestedLRs(valley=0.0004786300996784121) . Use not only the discrimative learning rates but also decrease the amount of training for lower training rate . lr = [2e-2, 3e-3, 4e-4] epoch_lst = [10, 8, 7] for i in range(len(lr)): learn_8.fine_tune(epoch_lst[i], lr[i]) . epoch train_loss valid_loss accuracy time . 0 | 2.366662 | 2.098476 | 0.450000 | 00:42 | . epoch train_loss valid_loss accuracy time . 0 | 1.365729 | 1.222900 | 0.505000 | 00:43 | . 1 | 1.218585 | 0.952413 | 0.570000 | 00:44 | . 2 | 1.123894 | 1.245164 | 0.545000 | 00:44 | . 3 | 1.093431 | 1.219178 | 0.585000 | 00:42 | . 4 | 0.954352 | 1.016823 | 0.605000 | 00:44 | . 5 | 0.795425 | 0.980452 | 0.615000 | 00:43 | . 6 | 0.675656 | 0.715877 | 0.660000 | 00:43 | . 7 | 0.543923 | 0.732847 | 0.685000 | 00:43 | . 8 | 0.431559 | 0.700989 | 0.705000 | 00:41 | . 9 | 0.366880 | 0.680698 | 0.700000 | 00:42 | . epoch train_loss valid_loss accuracy time . 0 | 0.261441 | 0.790363 | 0.660000 | 00:44 | . epoch train_loss valid_loss accuracy time . 0 | 0.262735 | 0.715863 | 0.700000 | 00:42 | . 1 | 0.291509 | 0.663961 | 0.695000 | 00:42 | . 2 | 0.267504 | 0.725574 | 0.710000 | 00:44 | . 3 | 0.229500 | 0.753732 | 0.710000 | 00:42 | . 4 | 0.229024 | 0.719795 | 0.710000 | 00:44 | . 5 | 0.211879 | 0.703274 | 0.710000 | 00:43 | . 6 | 0.201298 | 0.708483 | 0.710000 | 00:42 | . 7 | 0.192585 | 0.680750 | 0.700000 | 00:43 | . epoch train_loss valid_loss accuracy time . 0 | 0.174810 | 0.688584 | 0.715000 | 00:42 | . epoch train_loss valid_loss accuracy time . 0 | 0.183451 | 0.683985 | 0.715000 | 00:42 | . 1 | 0.160688 | 0.700210 | 0.715000 | 00:43 | . 2 | 0.168368 | 0.690924 | 0.710000 | 00:43 | . 3 | 0.161951 | 0.702923 | 0.710000 | 00:42 | . 4 | 0.163574 | 0.694562 | 0.710000 | 00:43 | . 5 | 0.162362 | 0.700117 | 0.705000 | 00:42 | . 6 | 0.164124 | 0.698838 | 0.715000 | 00:44 | . learn_8.show_results() . Hmmm...okayish results here, still many misclassifications in Stratus and Stratocumulus. . interp = ClassificationInterpretation.from_learner(learn_8) interp.plot_confusion_matrix() . Out of all, Nimbostratus should not have any misclassifications. Nimbostartus are the dark clouds during the rainy days!!! . interp.most_confused(min_val =4) . [(&#39;Cumulonimbus&#39;, &#39;Cumulus&#39;, 5), (&#39;Stratocumulus&#39;, &#39;Stratus&#39;, 5), (&#39;Cumulus&#39;, &#39;Cumulonimbus&#39;, 4), (&#39;Stratus&#39;, &#39;Cumulonimbus&#39;, 4), (&#39;Stratus&#39;, &#39;Stratocumulus&#39;, 4)] . Testing Phase . Let&#39;s test on unseen data. All these images have been taken from the internet. . Please upload more images in the cloudtest directory for testing purpose . A Cirrus Cloud . img_2 = PILImage.create(&quot;../input/cloudtest/Screenshot 2022-09-02 at 10.44.33 PM.png&quot;) img_2 . learn_8.predict(img_2) . (&#39;Cirrus&#39;, TensorBase(2), TensorBase([1.7253e-07, 3.9383e-04, 9.9961e-01, 1.5218e-07, 5.4315e-07, 2.1076e-08, 1.6901e-07, 2.4720e-07])) . Another Cirrus . img_3 = PILImage.create(&quot;../input/cloudtest/Screenshot 2022-09-02 at 11.35.30 PM.png&quot;) img_3 . learn_8.predict(img_3) . (&#39;Cirrus&#39;, TensorBase(2), TensorBase([6.0119e-04, 3.1065e-01, 6.7279e-01, 1.9115e-04, 1.5691e-04, 5.5428e-03, 4.2583e-03, 5.8062e-03])) . A Cumulus Cloud . img_4 = PILImage.create(&quot;../input/cloudtest/Screenshot 2022-09-02 at 11.36.15 PM.png&quot;) img_4 . learn_8.predict(img_4) . (&#39;Cumulus&#39;, TensorBase(4), TensorBase([1.1306e-05, 1.1890e-06, 2.5026e-06, 1.8337e-03, 9.9480e-01, 3.3508e-03, 1.7057e-06, 2.3660e-07])) . The model correctly classified all the three unseen images. . It&#39;s only 72.5% correct, so it is bound to fail on 30% unless we make improvements. . Let&#39;s check how the validation loss is behaving in the last model . learn_8.recorder.plot_loss() . It&#39;s empty because the recorder object is somehow empty. . Strange!! . learn_8.recorder.losses . [] . Dirty work, create a dataframe from the epochs and train, valid_loss from the iterations above manually and plot . data = {&#39;Epochs&#39;: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], &#39;train_loss&#39;: [1.365729, 1.218585, 1.123894, 1.093431, 0.954352, 0.795425, 0.675656, 0.543923, 0.431559, 0.366880, 0.168368, 0.161951, 0.163574, 0.162362, 0.164124 ], &#39;valid_loss&#39;: [1.222900, 0.952413, 1.245164, 1.219178, 1.016823, 0.980452, 0.715877, 0.732847, 0.700989, 0.680698, 0.690924, 0.702923, 0.694562, 0.700117, 0.698838], &#39;accuracy&#39; : [0.505000, 0.570000, 0.545000, 0.585000, 0.605000, 0.615000, 0.660000, 0.685000, 0.705000, 0.700000, 0.710000, 0.710000, 0.710000, 0.705000, 0.715000] } df = pd.DataFrame(data) df . Epochs train_loss valid_loss accuracy . 0 0 | 1.365729 | 1.222900 | 0.505 | . 1 1 | 1.218585 | 0.952413 | 0.570 | . 2 2 | 1.123894 | 1.245164 | 0.545 | . 3 3 | 1.093431 | 1.219178 | 0.585 | . 4 4 | 0.954352 | 1.016823 | 0.605 | . 5 5 | 0.795425 | 0.980452 | 0.615 | . 6 6 | 0.675656 | 0.715877 | 0.660 | . 7 7 | 0.543923 | 0.732847 | 0.685 | . 8 8 | 0.431559 | 0.700989 | 0.705 | . 9 9 | 0.366880 | 0.680698 | 0.700 | . 10 10 | 0.168368 | 0.690924 | 0.710 | . 11 11 | 0.161951 | 0.702923 | 0.710 | . 12 12 | 0.163574 | 0.694562 | 0.710 | . 13 13 | 0.162362 | 0.700117 | 0.705 | . 14 14 | 0.164124 | 0.698838 | 0.715 | . Shows a decent picture that how validation loss decreased and with that the accuracy increased. . I prefer this version that attempt 7 as the validation loss seems much lower and accuracy doesn&#39;t suffer too greatly. . plt.figure(figsize=(10, 10)) plt.xlabel(&quot;Epochs&quot;) plt.ylabel(&quot;Loss and Accuracy&quot;) plt.plot(df[&#39;train_loss&#39;], label = &quot;train_loss&quot;) plt.plot(df[&#39;valid_loss&#39;], label = &quot;valid_loss&quot;) plt.plot(df[&#39;accuracy&#39;], label = &quot;accuracy&quot;) plt.legend() plt.show() . learn_8.recorder.metric_names . (#5) [&#39;epoch&#39;,&#39;train_loss&#39;,&#39;valid_loss&#39;,&#39;accuracy&#39;,&#39;time&#39;] . cm = interp.confusion_matrix() interp.plot_confusion_matrix() cm . array([[22, 0, 2, 0, 0, 0, 0, 1], [ 2, 20, 2, 0, 1, 0, 0, 0], [ 1, 2, 22, 0, 0, 0, 0, 0], [ 0, 0, 0, 17, 5, 1, 2, 0], [ 0, 0, 0, 4, 20, 1, 0, 0], [ 0, 0, 1, 1, 2, 17, 2, 2], [ 1, 0, 1, 0, 3, 2, 13, 5], [ 0, 1, 0, 4, 2, 2, 4, 12]]) . As this is a classification problem, let&#39;s check the precision, recall, and F1 score . tp = cm.diagonal() fn = cm.sum(1) - tp fp = cm.sum(0) - tp precision = tp / (tp + fp) recall = tp / (tp + fn) . recall . array([0.88, 0.8 , 0.88, 0.68, 0.8 , 0.68, 0.52, 0.48]) . Precision : It is the quantity of the right predictions that the model made. It doesn&#39;t consider the wrong predictions made by the model. . precision . array([0.84615385, 0.86956522, 0.78571429, 0.65384615, 0.60606061, 0.73913043, 0.61904762, 0.6 ]) . An ideal system with high precision and high recall will return many results, with all results labeled correctly. . np.sum(precision), np.sum(recall) . (5.719518162996424, 5.720000000000001) . print(&quot;Precision for the Model : &quot;, 0.715) print(&quot;Recall for the Model :&quot;, 0.715) . Precision for the Model : 0.715 Recall for the Model : 0.715 . F1 score is the harmonic mean of precision and recall and balances precision and recall, a value close to 1 is desirable . f1 = (2* precision * recall)/(precision+recall) f1 . array([0.8627451 , 0.83333333, 0.83018868, 0.66666667, 0.68965517, 0.70833333, 0.56521739, 0.53333333]) . np.mean(f1) . 0.7111841259586632 . 0.711 as F1 score. It&#39;s not great but ok! . There are many things that can be done to improve the model but would require a long time and most of it would be in data prep. For now, I am moving on and might come back to check on it again. . End of Notebook .",
            "url": "https://prashantmdgl9.github.io/ml_experiments/2022/09/03/which-cloud-do-you-see-in-the-sky.html",
            "relUrl": "/2022/09/03/which-cloud-do-you-see-in-the-sky.html",
            "date": " • Sep 3, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Torch.where",
            "content": "I had a lot of issues in understanding torch.where, so tried to deconstruct its working . from fastai.vision.all import * matplotlib.rc(&#39;image&#39;, cmap=&#39;Greys&#39;) . Element-wise operation . 0, 1, 0 will be compated with row one of prds and then row 2 and so on... . trgts = tensor([0, 1, 0]) prds = tensor([0.8, 0.4, 0.2], [0.6, 0.2, 0.5]) trgts, prds . (tensor([0, 1, 0]), tensor([[0.8000, 0.4000, 0.2000], [0.6000, 0.2000, 0.5000]])) . torch.where(trgts == 1, 1-prds, prds).mean() . tensor(0.5833) . Single tensor . 0 will be compared with row 1 of prds and then row 2 and so on... . trgts = tensor([0]) prds = tensor([0.4, 0.3],[0.1, 0.2]) trgts, prds . (tensor([0]), tensor([[0.4000, 0.3000], [0.1000, 0.2000]])) . torch.where(trgts == 1, 1-prds, prds).mean() . tensor(0.2500) . trgts = tensor([1]) prds = tensor([0.4, 0.3],[0.1, 0.2]) trgts, prds . (tensor([1]), tensor([[0.4000, 0.3000], [0.1000, 0.2000]])) . torch.where(trgts == 1, 1-prds, prds).mean() . tensor(0.7500) . Create a function . def ret_where(a, b): c = torch.where(a == 1, 1-b, b) return c, c.mean() . This is an interesting case . trgts = tensor([0], [1]) prds = tensor([0.4, 0.3],[0.1, 0.2]) trgts, prds . (tensor([[0], [1]]), tensor([[0.4000, 0.3000], [0.1000, 0.2000]])) . First target tensor with first prediction tensor row, second target tensor row with second prediction row . ret_where(trgts, prds) . (tensor([[0.4000, 0.3000], [0.9000, 0.8000]]), tensor(0.6000)) . trgts = tensor([0], [1], [0]) prds = tensor([0.4, 0.3],[0.1, 0.2], [0.1, 0.6] ) trgts, prds . (tensor([[0], [1], [0]]), tensor([[0.4000, 0.3000], [0.1000, 0.2000], [0.1000, 0.6000]])) . a = torch.where(trgts == 1, 1-prds, prds) a, a.mean() . (tensor([[0.4000, 0.3000], [0.9000, 0.8000], [0.1000, 0.6000]]), tensor(0.5167)) . Entire first target is broadcasted to first prediction, second target to second row and so on... . ret_where(trgts, prds) . (tensor([[0.4000, 0.3000], [0.9000, 0.8000], [0.1000, 0.6000]]), tensor(0.5167)) .",
            "url": "https://prashantmdgl9.github.io/ml_experiments/2022/08/30/torch-where-fastai.html",
            "relUrl": "/2022/08/30/torch-where-fastai.html",
            "date": " • Aug 30, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Hindi MNIST Computer Vision Classifier with FastAI",
            "content": "The notebok is based on FastAI course that ran in April to May 2022; it is written to explore the &quot;further research&quot; questions of lesson 3 of the course, which deals with Neural Net foundations. . Problem statement - Classify the hand written digits with the help of FastAI&#39;s libraries . Data - Hindi MNIST data present at https://www.kaggle.com/datasets/imbikramsaha/hindi-mnist . The Methodology will remain the same as discussed in the chapter. . Define the baseline model first. | Define dataloaders and other parameters that are required for implementing the Stochastic Gradient Descent, | Define the loss function and the accuracy metric | Fit your model using FastAI&#39;s libraries and check whether it beats the baseline model. | Make Improvements. | . Resources: &gt; - Chapter Link:https://course.fast.ai/Lessons/lesson3.html&gt; - Video based on 2020 course where the part of the problem is discussed:https://www.youtube.com/watch?v=p50s63nPq9I&amp;t=6605s . Imports and Downloads . from fastai.vision.all import * from fastbook import * matplotlib.rc(&#39;image&#39;, cmap=&#39;Greys&#39;) . Read the Data . # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python # For example, here&#39;s several helpful packages to load import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # Input data files are available in the read-only &quot;../input/&quot; directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): cnt = 0 for filename in filenames: cnt = cnt+1 #print(os.path.join(dirname, filename)) print(f&quot;Read {cnt} files from the directory- {dirname}&quot;) # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using &quot;Save &amp; Run All&quot; # You can also write temporary files to /kaggle/temp/, but they won&#39;t be saved outside of the current session . Read 0 files from the directory- /kaggle/input Read 0 files from the directory- /kaggle/input/hindi-mnist Read 0 files from the directory- /kaggle/input/hindi-mnist/Hindi-MNIST Read 0 files from the directory- /kaggle/input/hindi-mnist/Hindi-MNIST/test Read 300 files from the directory- /kaggle/input/hindi-mnist/Hindi-MNIST/test/7 Read 300 files from the directory- /kaggle/input/hindi-mnist/Hindi-MNIST/test/2 Read 300 files from the directory- /kaggle/input/hindi-mnist/Hindi-MNIST/test/5 Read 300 files from the directory- /kaggle/input/hindi-mnist/Hindi-MNIST/test/8 Read 300 files from the directory- /kaggle/input/hindi-mnist/Hindi-MNIST/test/0 Read 300 files from the directory- /kaggle/input/hindi-mnist/Hindi-MNIST/test/3 Read 300 files from the directory- /kaggle/input/hindi-mnist/Hindi-MNIST/test/1 Read 300 files from the directory- /kaggle/input/hindi-mnist/Hindi-MNIST/test/4 Read 300 files from the directory- /kaggle/input/hindi-mnist/Hindi-MNIST/test/9 Read 300 files from the directory- /kaggle/input/hindi-mnist/Hindi-MNIST/test/6 Read 0 files from the directory- /kaggle/input/hindi-mnist/Hindi-MNIST/train Read 1700 files from the directory- /kaggle/input/hindi-mnist/Hindi-MNIST/train/7 Read 1700 files from the directory- /kaggle/input/hindi-mnist/Hindi-MNIST/train/2 Read 1700 files from the directory- /kaggle/input/hindi-mnist/Hindi-MNIST/train/5 Read 1700 files from the directory- /kaggle/input/hindi-mnist/Hindi-MNIST/train/8 Read 1700 files from the directory- /kaggle/input/hindi-mnist/Hindi-MNIST/train/0 Read 1700 files from the directory- /kaggle/input/hindi-mnist/Hindi-MNIST/train/3 Read 1700 files from the directory- /kaggle/input/hindi-mnist/Hindi-MNIST/train/1 Read 1700 files from the directory- /kaggle/input/hindi-mnist/Hindi-MNIST/train/4 Read 1700 files from the directory- /kaggle/input/hindi-mnist/Hindi-MNIST/train/9 Read 1700 files from the directory- /kaggle/input/hindi-mnist/Hindi-MNIST/train/6 . Define train and validation paths . train_dir = &quot;/kaggle/input/hindi-mnist/Hindi-MNIST/train&quot; train_path = Path(train_dir) valid_dir = &quot;/kaggle/input/hindi-mnist/Hindi-MNIST/test&quot; valid_path = Path(valid_dir) . zeroes = train_path.ls().sorted()[0].ls() ones = train_path.ls().sorted()[1].ls() twos = train_path.ls().sorted()[2].ls() threes = train_path.ls().sorted()[3].ls() fours = train_path.ls().sorted()[4].ls() fives = train_path.ls().sorted()[5].ls() sixes = train_path.ls().sorted()[6].ls() sevens = train_path.ls().sorted()[7].ls() eights = train_path.ls().sorted()[8].ls() nines = train_path.ls().sorted()[9].ls() . Print a digit to see what the data is . im = Image.open(sixes[0]) im . tensor(im)[4:10,4:10] . tensor([[ 4, 28, 97, 185, 236, 254], [ 50, 164, 245, 255, 255, 255], [184, 251, 255, 255, 254, 236], [253, 255, 255, 253, 191, 79], [255, 255, 255, 190, 54, 7], [255, 255, 254, 139, 22, 1]], dtype=torch.uint8) . im.shape . (32, 32) . The size of the images - 32 x 32 . Pandas has a nice background gradient feature . im_t = tensor(im) df = pd.DataFrame(im_t[4:15,4:22]) df.style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) . &nbsp; 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 . 0 4 | 28 | 97 | 185 | 236 | 254 | 255 | 255 | 255 | 255 | 255 | 255 | 255 | 255 | 255 | 255 | 255 | 233 | . 1 50 | 164 | 245 | 255 | 255 | 255 | 255 | 252 | 234 | 208 | 181 | 159 | 154 | 183 | 235 | 254 | 255 | 255 | . 2 184 | 251 | 255 | 255 | 254 | 236 | 177 | 107 | 56 | 29 | 17 | 11 | 10 | 21 | 75 | 185 | 245 | 254 | . 3 253 | 255 | 255 | 253 | 191 | 79 | 23 | 6 | 1 | 0 | 0 | 0 | 0 | 0 | 7 | 40 | 98 | 130 | . 4 255 | 255 | 255 | 190 | 54 | 7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 4 | 6 | . 5 255 | 255 | 254 | 139 | 22 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 6 255 | 255 | 255 | 213 | 87 | 25 | 10 | 5 | 3 | 4 | 7 | 8 | 7 | 4 | 2 | 0 | 0 | 0 | . 7 248 | 255 | 255 | 255 | 239 | 195 | 156 | 128 | 111 | 117 | 138 | 150 | 140 | 98 | 42 | 9 | 1 | 0 | . 8 161 | 245 | 255 | 255 | 255 | 255 | 255 | 254 | 252 | 253 | 255 | 255 | 254 | 247 | 154 | 34 | 3 | 0 | . 9 45 | 143 | 245 | 255 | 255 | 255 | 255 | 255 | 255 | 255 | 255 | 255 | 255 | 250 | 149 | 32 | 2 | 0 | . 10 135 | 216 | 253 | 255 | 255 | 255 | 255 | 255 | 255 | 255 | 255 | 244 | 208 | 123 | 40 | 7 | 0 | 0 | . Baseline Model . Create a model that is simple and works fine enough(ideally, it should beat the chance i.e. it should have at least 50% accurate) . Distance from mean - Create a mean image for each of the digit from 0 to 9 . Dict = {0: zeroes,1:ones, 2: twos, 3: threes, 4: fours, 5: fives, 6: sixes, 7:sevens, 8:eights, 9:nines} . train_tensors = [] valid_tensors = [] for key in Dict: train_tensors.append([tensor(Image.open(o)) for o in Dict[key]]) valid_inf = valid_path.ls().sorted()[key].ls() valid_tensors.append([tensor(Image.open(o)) for o in valid_inf]) . Keep checking the size of the variables involved, it is one of the best practices . len(train_tensors), len(valid_tensors), len(train_tensors[0]), len(valid_tensors[0]) . (10, 10, 1700, 300) . Stack the images for each of the digit class and find the mean digit(average of all the images for a digit) . **Each image is 2D with size 3232, by stacking all 1700 images(for one digit) in the train set, we create a 3d shape of 17003232 . stacked_train_tensors = [] for i in range(len(train_tensors)): stacked_train_tensors.append((torch.stack(train_tensors[i]).float()/255)) #print(i) print(len(stacked_train_tensors)), print(stacked_train_tensors[0].shape) . 10 torch.Size([1700, 32, 32]) . (None, None) . stacked_train_tensors_mean = [] for i in range(len(train_tensors)): stacked_train_tensors_mean.append((torch.stack(train_tensors[i]).float()/255).mean(0)) #print(i) print(len(stacked_train_tensors_mean)) . 10 . stacked_valid_tensors = [] for i in range(len(valid_tensors)): stacked_valid_tensors.append((torch.stack(valid_tensors[i]).float()/255)) #print(i) print(len(stacked_valid_tensors)), print(stacked_valid_tensors[0].shape) . 10 torch.Size([300, 32, 32]) . (None, None) . stacked_train_tensors_mean[0].shape . torch.Size([32, 32]) . Plot one of the mean images, it will be blurry as it is a mean value . show_image(stacked_train_tensors_mean[4]) . &lt;AxesSubplot:&gt; . show_image(train_tensors[4][0]) . &lt;AxesSubplot:&gt; . Take a sample image and fine the distance between the image from its respective mean image i.e. compare a 4 with the mean 4. . Here, Root mean square and mean absolute errors are calculated . dist_4_abs = (train_tensors[4][0] - stacked_train_tensors_mean[4]).abs().mean() dist_4_sqr = ((train_tensors[4][0] - stacked_train_tensors_mean[4])**2).mean().sqrt() dist_4_abs, dist_4_sqr . (tensor(49.9770), tensor(104.4630)) . dist_3_abs = (train_tensors[3][0] - stacked_train_tensors_mean[4]).abs().mean() dist_3_sqr = ((train_tensors[3][0] - stacked_train_tensors_mean[4])**2).mean().sqrt() dist_3_abs, dist_3_sqr . (tensor(67.0818), tensor(122.1938)) . The error checks out i.e. the distance between mean 4 and 4 is less than mean 4 and 3(or any other number). Let&#39;s investigate it more . Let&#39;s define Error functions . def rms_error(a,b): return ((a-b)**2).mean((-1, -2)).sqrt() . RMS error . for i in range(10): err = rms_error(train_tensors[4][0],stacked_train_tensors_mean[i]) print(err) . tensor(104.5548) tensor(104.4847) tensor(104.5112) tensor(104.4911) tensor(104.4630) tensor(104.4989) tensor(104.5112) tensor(104.4960) tensor(104.5195) tensor(104.4974) . L1 error . for i in range(10): print(F.l1_loss(train_tensors[4][0].float(),stacked_train_tensors_mean[i])) . tensor(50.0465) tensor(50.0393) tensor(50.0182) tensor(50.0276) tensor(49.9770) tensor(50.0150) tensor(50.0189) tensor(50.0297) tensor(50.0313) tensor(50.0543) . MSE/L2 error . for i in range(10): print(F.mse_loss(train_tensors[4][0].float(),stacked_train_tensors_mean[i])) . tensor(10931.7139) tensor(10917.0586) tensor(10922.5928) tensor(10918.3809) tensor(10912.5215) tensor(10920.0205) tensor(10922.5967) tensor(10919.4229) tensor(10924.3340) tensor(10919.7129) . _All the error functions have lowest error values for the distance between mean digits and sample image - traintensors[4][0] which is a 4 . Broadcasting happens here, despite different shapes of the two tensors, the results are calculated . All tensors in the validation set for a particular digit will be compared against the mean digit . print(stacked_valid_tensors[4].shape), print(stacked_train_tensors_mean[4].shape) error = rms_error(stacked_valid_tensors[4], stacked_train_tensors_mean[4]) error.shape, error[0:15] . torch.Size([300, 32, 32]) torch.Size([32, 32]) . (torch.Size([300]), tensor([0.3021, 0.3300, 0.2755, 0.2987, 0.3047, 0.3128, 0.3670, 0.3253, 0.3164, 0.3172, 0.3009, 0.3079, 0.3204, 0.2997, 0.2655])) . def predict_input(input_tensor): errors_in_pred = [] # errors = rms_error(input_tensor, stacked_train_tensors_mean[x]) for i in range(10): errors = rms_error(input_tensor, stacked_train_tensors_mean[i]) errors_in_pred.append(errors) #return torch.argmin(torch.stack(errors_in_pred), 0) # across the first axis, 0 specifies the axis return torch.argmin(torch.stack(errors_in_pred), 0) . y = predict_input(stacked_valid_tensors[9]) y, y.shape . (tensor([9, 9, 9, 9, 6, 9, 9, 9, 9, 9, 9, 9, 9, 6, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 1, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 0, 1, 9, 0, 9, 9, 9, 6, 9, 9, 9, 9, 9, 9, 8, 9, 9, 9, 9, 9, 9, 9, 8, 9, 9, 9, 9, 9, 9, 6, 6, 9, 9, 9, 9, 9, 9, 9, 6, 9, 9, 9, 1, 9, 9, 9, 9, 9, 8, 9, 9, 6, 9, 9, 9, 9, 9, 9, 1, 9, 9, 0, 9, 9, 9, 9, 9, 9, 9, 9, 9, 0, 9, 9, 9, 9, 9, 9, 9, 9, 1, 9, 9, 9, 9, 1, 9, 9, 9, 9, 9, 9, 9, 4, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 2, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 6, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 0, 9, 9, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 4, 9, 4, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 5, 9, 9, 9, 1, 9, 9, 9, 9, 6, 8, 0, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 4, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 0, 9, 9, 9, 9, 1, 9]), torch.Size([300])) . (y == 9).float().mean() . tensor(0.8833) . accuracies = [] for i in range(10): #print(i) preds = predict_input(stacked_valid_tensors[i]) acc = (preds == i).float().mean() accuracies.append(acc) #print(preds) #pred_e = torch.argmin(err, 0) # print(preds) #accuracies.append((pred_e == i).float().mean()) accuracies . [tensor(0.9667), tensor(0.9033), tensor(0.7300), tensor(0.5867), tensor(0.9267), tensor(0.7433), tensor(0.8567), tensor(0.7900), tensor(0.8767), tensor(0.8833)] . print(&#39;baseline model accuracy:&#39;, torch.stack(accuracies).mean()) . baseline model accuracy: tensor(0.8263) . 82% baseline accuracy, let&#39;s try to beat that . Prepare for Stochastic gradient descent . stacked_train_tensors[0][0].shape # one image from digit 0 . torch.Size([32, 32]) . Entire data in row column format . lst = [stacked_train_tensors[i] for i in range(10)] # one row represents one image. image is flattened to 32*32 = 1024 pixels train_x = torch.cat(lst).view(-1, 32*32) train_x.shape . torch.Size([17000, 1024]) . y_tensor = torch.tensor([]) for i in range(10): a = tensor(np.full(len(stacked_train_tensors[i]),i)) y_tensor = torch.cat([y_tensor, a]) y_tensor = y_tensor.unsqueeze(1) . PyTorch won&#39;t accept a FloatTensor as categorical target, so you&#39;ve to cast your tensor to LongTensor . y_tensor . tensor([[0.], [0.], [0.], ..., [9.], [9.], [9.]]) . y_tensor = y_tensor.type(torch.LongTensor) . y_tensor.shape . torch.Size([17000, 1]) . This is an important step, it will create tuples of input and output . dset = list(zip(train_x,y_tensor)) . Same processing for validation set . valid_lst = [stacked_valid_tensors[i] for i in range(10)] # one row represents one image. image is flattened to 32*32 = 1024 pixels valid_x = torch.cat(valid_lst).view(-1, 32*32) valid_x.shape #train_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1) valid_y_tensor = torch.tensor([]) for i in range(10): a = tensor(np.full(len(stacked_valid_tensors[i]),i)) valid_y_tensor = torch.cat([valid_y_tensor, a]) valid_y_tensor = valid_y_tensor.unsqueeze(1) valid_dset = list(zip(valid_x,valid_y_tensor)) . valid_y_tensor.shape . torch.Size([3000, 1]) . Testing a few things, ignore . def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_() weights = init_params((32*32,1)) bias = init_params(1) . weights.shape, bias.shape . (torch.Size([1024, 1]), torch.Size([1])) . (train_x[0]*weights.T).sum() + bias . tensor([14.7308], grad_fn=&lt;AddBackward0&gt;) . def linear1(xb): return xb@weights + bias preds = linear1(train_x) preds . tensor([[ 14.7308], [ 15.2412], [ 12.4542], ..., [ 7.2494], [-17.0967], [ 6.3935]], grad_fn=&lt;AddBackward0&gt;) . train_x.shape . torch.Size([17000, 1024]) . preds.shape . torch.Size([17000, 1]) . Testing ends here . Let&#39;s build the Neural Net model . Dataloaders . dl_train = DataLoader(dset, batch_size=256, shuffle=True) dl_valid = DataLoader(valid_dset, batch_size=256) . dls = DataLoaders(dl_train, dl_valid) . 17000 train samples divided in 256 batches 17000/256 ~ 67 . len(dls.train) . 67 . Loss functions and Accuracy metric . Sigmoid transforms everything between 0 and 1, helps in taking probability . def loss_func(predictions, targets): predictions = predictions.sigmoid() return torch.where(targets==1, 1-predictions, predictions).mean() . def accuracy_metric(prediction, y): idx = torch.argmax(prediction, axis=1) # returns the index of the highest value return (idx==y.T).float().mean() . Define the model. we&#39;ll use 30 neurons in the hidden layer. 1024 in input, 30 in hidden, 10 in output(because 10 classes are there (0 to 9)) . model = nn.Sequential( nn.Linear(32*32, 30), # 1024 input features and 30 output features nn.ReLU(), nn.Linear(30,10), ) . learn_loss_func = Learner(dls, model, loss_func=loss_func, opt_func=SGD, metrics=accuracy_metric) . learn_loss_func.fit(n_epoch=10, lr=0.1) . epoch train_loss valid_loss accuracy_metric time . 0 | 0.214083 | 0.123915 | 0.091667 | 00:00 | . 1 | 0.133767 | 0.108301 | 0.091000 | 00:00 | . 2 | 0.113340 | 0.104889 | 0.090667 | 00:00 | . 3 | 0.108106 | 0.103433 | 0.090667 | 00:00 | . 4 | 0.103318 | 0.102633 | 0.090333 | 00:00 | . 5 | 0.102949 | 0.102128 | 0.089667 | 00:00 | . 6 | 0.102660 | 0.101779 | 0.090667 | 00:00 | . 7 | 0.102763 | 0.101527 | 0.092333 | 00:00 | . 8 | 0.102156 | 0.101335 | 0.095333 | 00:00 | . 9 | 0.101422 | 0.101184 | 0.097000 | 00:00 | . This model didn&#39;t learn or what? . def softmax_loss(prediction, y): soft_m = torch.softmax(prediction, dim=1) index = tensor(range(len(y))) return soft_m[index.long(), y.long()].mean() . learn_softmax = Learner(dls, model, loss_func=softmax_loss, opt_func=SGD, metrics=accuracy_metric) . learn_softmax.fit(n_epoch=10, lr=0.1) . epoch train_loss valid_loss accuracy_metric time . 0 | 0.100436 | 0.099736 | 0.098333 | 00:00 | . 1 | 0.100521 | 0.099690 | 0.096000 | 00:00 | . 2 | 0.100557 | 0.099638 | 0.100000 | 00:00 | . 3 | 0.100653 | 0.099551 | 0.098667 | 00:01 | . 4 | 0.100606 | 0.099459 | 0.100667 | 00:00 | . 5 | 0.100483 | 0.099377 | 0.100333 | 00:00 | . 6 | 0.100640 | 0.099186 | 0.099333 | 00:00 | . 7 | 0.100577 | 0.099071 | 0.099000 | 00:00 | . 8 | 0.100639 | 0.098888 | 0.097000 | 00:00 | . 9 | 0.100628 | 0.098917 | 0.096333 | 00:00 | . This doesn&#39;t work as well, it&#39;s not learning. . I cheated and looked on forums and people said there can be precision issues, so use something logarithmic . def loss_entropy(pred, y): #print(y.shape) y = y.long() if y.ndim &gt; 1: y = y.squeeze() # print(y.shape) return F.cross_entropy(pred, y) . learn_entropy = Learner(dls, model, loss_func=loss_entropy, opt_func=SGD, metrics=accuracy_metric) . learn_entropy.fit(n_epoch=30, lr=0.1) . epoch train_loss valid_loss accuracy_metric time . 0 | 0.989585 | 0.521992 | 0.858000 | 00:00 | . 1 | 0.518371 | 0.347884 | 0.895667 | 00:00 | . 2 | 0.346704 | 0.284983 | 0.909667 | 00:00 | . 3 | 0.272468 | 0.256439 | 0.920333 | 00:00 | . 4 | 0.230199 | 0.212597 | 0.945333 | 00:00 | . 5 | 0.207443 | 0.198280 | 0.949000 | 00:00 | . 6 | 0.190783 | 0.185562 | 0.947667 | 00:00 | . 7 | 0.174779 | 0.179351 | 0.951333 | 00:00 | . 8 | 0.163493 | 0.172312 | 0.955000 | 00:00 | . 9 | 0.155247 | 0.166811 | 0.956000 | 00:00 | . 10 | 0.148723 | 0.162777 | 0.953333 | 00:00 | . 11 | 0.142068 | 0.157631 | 0.957000 | 00:00 | . 12 | 0.136054 | 0.156244 | 0.954667 | 00:00 | . 13 | 0.133357 | 0.153135 | 0.958000 | 00:00 | . 14 | 0.127693 | 0.165472 | 0.954667 | 00:00 | . 15 | 0.125129 | 0.144914 | 0.959667 | 00:00 | . 16 | 0.118554 | 0.145416 | 0.962333 | 00:00 | . 17 | 0.114223 | 0.141651 | 0.960333 | 00:00 | . 18 | 0.112119 | 0.148653 | 0.956333 | 00:00 | . 19 | 0.108679 | 0.138428 | 0.963000 | 00:00 | . 20 | 0.104451 | 0.136940 | 0.964667 | 00:00 | . 21 | 0.104604 | 0.151451 | 0.960333 | 00:00 | . 22 | 0.100410 | 0.141625 | 0.960333 | 00:00 | . 23 | 0.096883 | 0.132164 | 0.963667 | 00:00 | . 24 | 0.093484 | 0.129140 | 0.965667 | 00:00 | . 25 | 0.092321 | 0.130651 | 0.962000 | 00:00 | . 26 | 0.089158 | 0.128698 | 0.964667 | 00:01 | . 27 | 0.087250 | 0.128824 | 0.964000 | 00:00 | . 28 | 0.083911 | 0.130542 | 0.964667 | 00:00 | . 29 | 0.081479 | 0.127400 | 0.964000 | 00:00 | . It learns now . plt.plot(L(learn_loss_func.recorder.values).itemgot(2), label=&#39;w/ simple_loss&#39;); plt.plot(L(learn_entropy.recorder.values).itemgot(2), label=&#39;w/ entropy&#39;); plt.plot(L(learn_softmax.recorder.values).itemgot(2), label=&#39;w/ softmax&#39;); plt.title(&#39;accuracy&#39;); plt.legend(loc=&#39;best&#39;); plt.xlabel(&#39;epoch&#39;); . The FastAI model has beaten the baseline model. . Although, I am still not very sure why the softmax or simple loss didn&#39;t work . Just checking what&#39;s happening inside the model. . y_tensor[10000:10010] . tensor([[5], [5], [5], [5], [5], [5], [5], [5], [5], [5]]) . The index corresponding to y_tensor value will have ideally highest value in the predictions. . model(train_x)[10000:10010] . tensor([[-12.5915, -5.5043, -8.8870, -3.4508, -3.3452, 5.5960, -1.0710, -1.8192, -10.9644, -10.7473], [-18.2513, -23.7355, -2.3928, -1.6395, -5.8924, 5.2856, -11.4164, -9.5124, -9.5765, -15.0326], [-19.6813, -13.3664, 0.4013, 0.3663, -1.9135, 8.4236, 0.1444, -0.3372, -20.2419, -11.6217], [ -9.4473, -9.3769, -1.5958, -1.1041, -5.1527, 3.5932, -4.2382, -3.6048, -5.3509, -12.6652], [-15.1676, -7.6468, -1.3655, -4.8876, -5.6747, 5.9361, -6.5804, -2.6483, -7.4650, -12.2578], [ -8.7004, -6.9439, -2.0877, -1.5366, -2.4985, 2.0714, -7.3040, -7.5453, -5.1091, -2.5851], [ -9.2084, -6.5548, -2.2084, -0.6765, 0.5429, 5.1066, -5.8337, -5.4852, -6.9861, -5.3916], [-18.3216, -9.8647, -0.6680, -2.3356, -5.4501, 6.5298, -3.1101, -2.1203, -12.1218, -13.7853], [ -7.9667, -18.4219, -8.8755, -7.8094, -1.1771, 2.8631, -9.9230, -2.7488, -3.8866, -11.4819], [-18.9722, -17.4550, -6.5690, -3.4893, 0.5602, 6.4109, -10.1749, -3.2340, -13.9268, -12.4225]], grad_fn=&lt;SliceBackward0&gt;) . m = learn_entropy.model m . Sequential( (0): Linear(in_features=1024, out_features=30, bias=True) (1): ReLU() (2): Linear(in_features=30, out_features=10, bias=True) ) . w, b = m[0].parameters() . The first layer learns about specific features and patterns from the data but here I am not sure what is being learnt. Maybe computer knows it better . for i in range(w.shape[0]): show_image(w[i].view(32,32)) . /opt/conda/lib/python3.7/site-packages/fastai/torch_core.py:77: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). if ax is None: _,ax = plt.subplots(figsize=figsize) . End of the notebook .",
            "url": "https://prashantmdgl9.github.io/ml_experiments/2022/08/28/hindi-mnist-computer-vision-from-scratch-fastai.html",
            "relUrl": "/2022/08/28/hindi-mnist-computer-vision-from-scratch-fastai.html",
            "date": " • Aug 28, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "Proton Collision Identifier",
            "content": "from fastai.vision.all import * from fastai import * from fastai.vision import * from fastai.vision.widgets import * . # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python # For example, here&#39;s several helpful packages to load import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # Input data files are available in the read-only &quot;../input/&quot; directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): counter = 0 for filename in filenames: counter = counter + 1 #print(os.path.join(dirname, filename)) print(f&quot;Loaded all files from the directory: {dirname}&quot;) print(f&quot;Number of files loaded:{counter}&quot;) # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using &quot;Save &amp; Run All&quot; # You can also write temporary files to /kaggle/temp/, but they won&#39;t be saved outside of the current session . Loaded all files from the directory: /kaggle/input/proton-collision-image-set/Proton Collision 13TeV/Train/WJets Number of files loaded:8444 . dir_name = &#39;../input/proton-collision-image-set/Proton Collision 13TeV/Train&#39; . path = Path(dir_name) . path.ls() . (#3) [Path(&#39;../input/proton-collision-image-set/Proton Collision 13TeV/Train/TTbar&#39;),Path(&#39;../input/proton-collision-image-set/Proton Collision 13TeV/Train/QCD&#39;),Path(&#39;../input/proton-collision-image-set/Proton Collision 13TeV/Train/WJets&#39;)] . dls = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=[Resize(128, method=&#39;squish&#39;)] ).dataloaders(path, bs=32) . dls.show_batch(max_n=9) . #learn.fine_tune(3) learn = vision_learner(dls, resnet18, metrics = error_rate) learn.lr_find() . Downloading: &#34;https://download.pytorch.org/models/resnet18-f37072fd.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth . SuggestedLRs(valley=0.0010000000474974513) . learn.fine_tune(4, 4.7e-3) . epoch train_loss valid_loss error_rate time . 0 | 1.076869 | 0.994646 | 0.488542 | 04:20 | . epoch train_loss valid_loss error_rate time . 0 | 0.823417 | 0.794527 | 0.380680 | 04:06 | . 1 | 0.761839 | 0.736746 | 0.344923 | 04:06 | . 2 | 0.621185 | 0.706744 | 0.317859 | 04:05 | . 3 | 0.416418 | 0.804867 | 0.329514 | 04:03 | . img = PILImage.create(&#39;../input/proton-collision-image-set/Proton Collision 13TeV/Test/TTbar/ttbar_lepFilter_13TeV_1087_1934.png&#39;) what_proton,_,probs = learn.predict(img) idx = 0 if what_proton == &quot;TTbar&quot;: idx = 1 elif what_proton == &quot;WJets&quot;: idx = 2 print(f&quot;This collision belongs to category of: {what_proton}&quot;) print(f&quot;Probability it belongs to that category: {probs[idx]:.4f}&quot;) img . This collision belongs to category of: TTbar Probability it belongs to that category: 0.9989 . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . learn.show_results() . interp.plot_top_losses(5, nrows=1) . cleaner = ImageClassifierCleaner(learn) cleaner . End of notebook (for now) .",
            "url": "https://prashantmdgl9.github.io/ml_experiments/2022/08/25/proton-collision-classifier-fastai.html",
            "relUrl": "/2022/08/25/proton-collision-classifier-fastai.html",
            "date": " • Aug 25, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "Forward and Backpropagation with High School Mathematics",
            "content": "Problem statement - Classify the hand written digits with help of a basic neural net designed from scratch . Data - MNIST data present at https://www.kaggle.com/datasets/animatronbot/mnist-digit-recognizer . Methodology Write the forward and backpropagation functions from scratch using simple maths and calculus . Imports and Data downloads . import numpy as np import pandas as pd from matplotlib import pyplot as plt data = pd.read_csv(&#39;/kaggle/input/mnist-digit-recognizer/train.csv&#39;) . data.head(10) . label pixel0 pixel1 pixel2 pixel3 pixel4 pixel5 pixel6 pixel7 pixel8 ... pixel774 pixel775 pixel776 pixel777 pixel778 pixel779 pixel780 pixel781 pixel782 pixel783 . 0 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 4 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 5 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 6 7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 7 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 8 5 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 9 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 10 rows × 785 columns . data.shape . (42000, 785) . Split the data into train and test sets . Transpose the data, so that one column is one image. The shape would be 785*42000 after transpose . One very important step is to normalise the train and test sets, without which training will be either slow or just not effective at all. X/255 takes care of the same . data = np.array(data) m, n = data.shape np.random.shuffle(data) # shuffle before splitting into test and training sets data_test = data[0:2000].T Y_test = data_test[0] X_test = data_test[1:n] X_test = X_test / 255. data_train = data[2000:m].T Y_train = data_train[0] X_train = data_train[1:n] X_train = X_train / 255. _,m_train = X_train.shape . X_train.shape . (784, 40000) . Y_train . array([2, 6, 7, ..., 9, 5, 6]) . Neural Net Design . . Let&#39;s conceptualise the mathematics in the form of equations . Forward Propagation . . Backward Propagation . . Updating the weights . . The calculus involves the chain rule . . In the above figure, if we want to update the weight of w5 then, we need to take the partial derivative of Total Error function wrt w5. . . But total error doesn&#39;t contain any term with w5, so we use chain rule . Let&#39;s code the above mathematics into python functions . def init_params(): W1 = np.random.rand(10, 784) - 0.5 b1 = np.random.rand(10, 1) - 0.5 W2 = np.random.rand(10, 10) - 0.5 b2 = np.random.rand(10, 1) - 0.5 return W1, b1, W2, b2 def ReLU(Z): return np.maximum(Z, 0) def softmax(Z): A = np.exp(Z) / sum(np.exp(Z)) return A def forward_prop(W1, b1, W2, b2, X): Z1 = W1.dot(X) + b1 A1 = ReLU(Z1) Z2 = W2.dot(A1) + b2 A2 = softmax(Z2) return Z1, A1, Z2, A2 def ReLU_deriv(Z): return Z &gt; 0 def one_hot(Y): one_hot_Y = np.zeros((Y.size, Y.max() + 1)) one_hot_Y[np.arange(Y.size), Y] = 1 one_hot_Y = one_hot_Y.T return one_hot_Y def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y): one_hot_Y = one_hot(Y) dZ2 = 2*(A2 - one_hot_Y) dW2 = 1 / m * dZ2.dot(A1.T) db2 = 1 / m * np.sum(dZ2) dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1) dW1 = 1 / m * dZ1.dot(X.T) db1 = 1 / m * np.sum(dZ1) return dW1, db1, dW2, db2 def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha): # print(W1.shape) # print(dW1.shape) W1 = W1 - alpha * dW1 b1 = b1 - alpha * db1 W2 = W2 - alpha * dW2 b2 = b2 - alpha * db2 return W1, b1, W2, b2 . That&#39;s where we do the Gradient Descent. We call it gradient descent not Stochastic Gradient Descent because we are running the net on entire dataset at once and will tweak all the weights and biases to update the parameters . def get_predictions(A2): return np.argmax(A2, 0) def get_accuracy(predictions, Y): print(predictions, Y) return np.sum(predictions == Y) / Y.size def gradient_descent(X, Y, alpha, iterations): W1, b1, W2, b2 = init_params() for i in range(iterations): Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X) dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y) #print(W1.shape) #print(dW1.shape) W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha) if i % 10 == 0: print(&quot;Iteration: &quot;, i) predictions = get_predictions(A2) print(get_accuracy(predictions, Y)) return W1, b1, W2, b2 . Let&#39;s run for 500 iterations . W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.10, 500) . Iteration: 0 [1 6 4 ... 6 1 6] [2 6 7 ... 9 5 6] 0.115775 Iteration: 10 [1 6 4 ... 4 1 6] [2 6 7 ... 9 5 6] 0.21905 Iteration: 20 [1 6 0 ... 0 1 0] [2 6 7 ... 9 5 6] 0.25965 Iteration: 30 [1 6 0 ... 0 3 6] [2 6 7 ... 9 5 6] 0.336025 Iteration: 40 [2 6 0 ... 0 3 6] [2 6 7 ... 9 5 6] 0.421125 Iteration: 50 [2 6 9 ... 0 5 6] [2 6 7 ... 9 5 6] 0.50025 Iteration: 60 [2 6 7 ... 0 5 6] [2 6 7 ... 9 5 6] 0.5673 Iteration: 70 [2 6 7 ... 4 5 6] [2 6 7 ... 9 5 6] 0.6161 Iteration: 80 [2 6 9 ... 9 5 6] [2 6 7 ... 9 5 6] 0.655525 Iteration: 90 [2 6 9 ... 4 5 6] [2 6 7 ... 9 5 6] 0.684975 Iteration: 100 [2 6 7 ... 4 5 6] [2 6 7 ... 9 5 6] 0.709775 Iteration: 110 [2 6 7 ... 4 5 6] [2 6 7 ... 9 5 6] 0.727575 Iteration: 120 [2 6 7 ... 4 5 6] [2 6 7 ... 9 5 6] 0.7433 Iteration: 130 [2 6 7 ... 4 5 6] [2 6 7 ... 9 5 6] 0.7566 Iteration: 140 [2 6 7 ... 4 5 6] [2 6 7 ... 9 5 6] 0.76835 Iteration: 150 [2 6 7 ... 4 5 6] [2 6 7 ... 9 5 6] 0.77935 Iteration: 160 [2 6 7 ... 4 5 6] [2 6 7 ... 9 5 6] 0.788825 Iteration: 170 [2 6 7 ... 4 5 6] [2 6 7 ... 9 5 6] 0.797025 Iteration: 180 [2 6 7 ... 4 5 6] [2 6 7 ... 9 5 6] 0.803525 Iteration: 190 [2 6 7 ... 4 5 6] [2 6 7 ... 9 5 6] 0.809125 Iteration: 200 [2 6 7 ... 4 5 6] [2 6 7 ... 9 5 6] 0.8141 Iteration: 210 [2 6 7 ... 9 5 6] [2 6 7 ... 9 5 6] 0.81925 Iteration: 220 [2 6 7 ... 9 5 6] [2 6 7 ... 9 5 6] 0.823775 Iteration: 230 [2 6 7 ... 9 5 6] [2 6 7 ... 9 5 6] 0.82825 Iteration: 240 [2 6 7 ... 9 5 6] [2 6 7 ... 9 5 6] 0.832775 Iteration: 250 [2 6 7 ... 9 5 6] [2 6 7 ... 9 5 6] 0.83605 Iteration: 260 [2 6 7 ... 9 5 6] [2 6 7 ... 9 5 6] 0.839275 Iteration: 270 [2 6 7 ... 9 5 6] [2 6 7 ... 9 5 6] 0.8428 Iteration: 280 [2 6 7 ... 9 5 6] [2 6 7 ... 9 5 6] 0.8452 Iteration: 290 [2 6 7 ... 9 5 6] [2 6 7 ... 9 5 6] 0.84755 Iteration: 300 [2 6 7 ... 9 5 6] [2 6 7 ... 9 5 6] 0.8499 Iteration: 310 [2 6 7 ... 9 5 6] [2 6 7 ... 9 5 6] 0.85185 Iteration: 320 [2 6 7 ... 9 5 6] [2 6 7 ... 9 5 6] 0.853975 Iteration: 330 [2 6 7 ... 9 5 6] [2 6 7 ... 9 5 6] 0.85605 Iteration: 340 [2 6 7 ... 9 5 6] [2 6 7 ... 9 5 6] 0.857625 Iteration: 350 [2 6 7 ... 9 5 6] [2 6 7 ... 9 5 6] 0.859125 Iteration: 360 [2 6 7 ... 9 5 6] [2 6 7 ... 9 5 6] 0.860575 Iteration: 370 [2 6 7 ... 9 5 6] [2 6 7 ... 9 5 6] 0.861725 Iteration: 380 [2 6 7 ... 9 5 6] [2 6 7 ... 9 5 6] 0.86305 Iteration: 390 [2 6 7 ... 9 5 6] [2 6 7 ... 9 5 6] 0.864325 Iteration: 400 [2 6 7 ... 9 5 6] [2 6 7 ... 9 5 6] 0.8654 Iteration: 410 [2 6 7 ... 9 5 6] [2 6 7 ... 9 5 6] 0.8664 Iteration: 420 [2 6 7 ... 9 5 6] [2 6 7 ... 9 5 6] 0.867725 Iteration: 430 [2 6 7 ... 9 5 6] [2 6 7 ... 9 5 6] 0.869125 Iteration: 440 [2 6 7 ... 9 5 6] [2 6 7 ... 9 5 6] 0.870275 Iteration: 450 [2 6 7 ... 9 5 6] [2 6 7 ... 9 5 6] 0.8711 Iteration: 460 [2 6 7 ... 9 5 6] [2 6 7 ... 9 5 6] 0.871975 Iteration: 470 [2 6 7 ... 9 5 6] [2 6 7 ... 9 5 6] 0.873 Iteration: 480 [2 6 7 ... 9 5 6] [2 6 7 ... 9 5 6] 0.87385 Iteration: 490 [2 6 7 ... 9 5 6] [2 6 7 ... 9 5 6] 0.874775 . def make_predictions(X, W1, b1, W2, b2): _, _, _, A2 = forward_prop(W1, b1, W2, b2, X) predictions = get_predictions(A2) return predictions def test_prediction(index, W1, b1, W2, b2): current_image = X_train[:, index, None] prediction = make_predictions(X_train[:, index, None], W1, b1, W2, b2) label = Y_train[index] print(&quot;Prediction: &quot;, prediction) print(&quot;Label: &quot;, label) current_image = current_image.reshape((28, 28)) * 255 plt.gray() plt.imshow(current_image, interpolation=&#39;nearest&#39;) plt.show() . test_prediction(0, W1, b1, W2, b2) test_prediction(1, W1, b1, W2, b2) test_prediction(2, W1, b1, W2, b2) test_prediction(100, W1, b1, W2, b2) test_prediction(200, W1, b1, W2, b2) . Prediction: [2] Label: 2 . Prediction: [6] Label: 6 . Prediction: [7] Label: 7 . Prediction: [8] Label: 8 . Prediction: [9] Label: 9 . test_predictions = make_predictions(X_test, W1, b1, W2, b2) get_accuracy(test_predictions, Y_test) . [5 2 5 ... 4 1 0] [5 2 5 ... 4 1 0] . 0.867 . Accuracy of 86% on the test set. . End of notebook .",
            "url": "https://prashantmdgl9.github.io/ml_experiments/2022/08/19/mnist-neural-network-from-scratch.html",
            "relUrl": "/2022/08/19/mnist-neural-network-from-scratch.html",
            "date": " • Aug 19, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "Understanding Datablocks",
            "content": "Imports . from fastai.data.all import * from fastai.vision.all import * . The first step is to download and decompress our data (if it&#8217;s not already done) and get its location: . path = untar_data(URLs.PETS) . . 100.00% [811712512/811706944 00:24&lt;00:00] Path.BASE_PATH = path . path.ls() . (#2) [Path(&#39;images&#39;),Path(&#39;annotations&#39;)] . The filenames are in the &#8220;images&#8221; folder. The get_image_files function helps get all the images in subfolders . fnames = get_image_files(path/&quot;images&quot;) . Empty DataBlock. . dblock = DataBlock() . By itself, a DataBlock is just a blue print on how to assemble your data. It does not do anything until you pass it a source. You can choose to then convert that source into a Datasets or a DataLoaders by using the DataBlock.datasets or DataBlock.dataloaders method. Since we haven&#8217;t done anything to get our data ready for batches, the dataloaders method will fail here, but we can have a look at how it gets converted in Datasets. This is where we pass the source of our data, here all our filenames . dsets = dblock.datasets(fnames) dsets.train[0] . (Path(&#39;images/scottish_terrier_99.jpg&#39;), Path(&#39;images/scottish_terrier_99.jpg&#39;)) . dsets . (#7390) [(Path(&#39;images/beagle_115.jpg&#39;), Path(&#39;images/beagle_115.jpg&#39;)),(Path(&#39;images/boxer_18.jpg&#39;), Path(&#39;images/boxer_18.jpg&#39;)),(Path(&#39;images/Maine_Coon_157.jpg&#39;), Path(&#39;images/Maine_Coon_157.jpg&#39;)),(Path(&#39;images/scottish_terrier_28.jpg&#39;), Path(&#39;images/scottish_terrier_28.jpg&#39;)),(Path(&#39;images/english_setter_6.jpg&#39;), Path(&#39;images/english_setter_6.jpg&#39;)),(Path(&#39;images/american_pit_bull_terrier_79.jpg&#39;), Path(&#39;images/american_pit_bull_terrier_79.jpg&#39;)),(Path(&#39;images/boxer_128.jpg&#39;), Path(&#39;images/boxer_128.jpg&#39;)),(Path(&#39;images/Persian_265.jpg&#39;), Path(&#39;images/Persian_265.jpg&#39;)),(Path(&#39;images/Maine_Coon_182.jpg&#39;), Path(&#39;images/Maine_Coon_182.jpg&#39;)),(Path(&#39;images/keeshond_89.jpg&#39;), Path(&#39;images/keeshond_89.jpg&#39;))...] . By default, the data block API assumes we have an input and a target, which is why we see our filename repeated twice. . _The first thing we can do is use a getitems function to actually assemble our items inside the data block . dblock = DataBlock(get_items = get_image_files) . get_image_files . &lt;function fastai.data.transforms.get_image_files(path, recurse=True, folders=None)&gt; . Pass the folder name . dsets = dblock.datasets(path/&quot;images&quot;) dsets.valid[0] . (Path(&#39;images/Siamese_158.jpg&#39;), Path(&#39;images/Siamese_158.jpg&#39;)) . Labeling the data is important, if capitalised name then classify as cat otherwise dog . def label_func(fname): return &quot;cat&quot; if fname.name[0].isupper() else &quot;dog&quot; . dblock = DataBlock(get_items = get_image_files, get_y = label_func) . dsets = dblock.datasets(path/&quot;images&quot;) dsets.train[0] . (Path(&#39;images/newfoundland_74.jpg&#39;), &#39;dog&#39;) . Now that our inputs and targets are ready, we can specify types to tell the data block API that our inputs are images and our targets are categories. Types are represented by blocks in the data block API, here we use ImageBlock and CategoryBlock: . dblock = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items = get_image_files, get_y = label_func) . dsets = dblock.datasets(path/&quot;images&quot;) dsets.train[0] . (PILImage mode=RGB size=500x333, TensorCategory(0)) . dsets.vocab . [&#39;cat&#39;, &#39;dog&#39;] . dblock = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items = get_image_files, get_y = label_func, splitter = RandomSplitter()) dsets = dblock.datasets(path/&quot;images&quot;) dsets.train[0] . (PILImage mode=RGB size=500x399, TensorCategory(0)) . The next step is to control how our validation set is created. We do this by passing a splitter to DataBlock. For instance, here is how to do a random split. . Also, resize the images . dblock = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items = get_image_files, get_y = label_func, splitter = RandomSplitter(), item_tfms = Resize(224)) . dls = dblock.dataloaders(path/&quot;images&quot;) dls.show_batch() . The way we usually build the data block in one go is by answering a list of questions: . what is the types of your inputs/targets? Here images and categories | where is your data? Here in filenames in subfolders | does something need to be applied to inputs? Here no | does something need to be applied to the target? Here the label_func function | how to split the data? Here randomly | do we need to apply something on formed items? Here a resize | do we need to apply something on formed batches? Here no | . Image classification . Grandparents Spiltter splits the items from the grand parent folder names (train_name and valid_name) . mnist = DataBlock(blocks=(ImageBlock(cls=PILImageBW), CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(), get_y=parent_label) . dls = mnist.dataloaders(untar_data(URLs.MNIST_TINY)) dls.show_batch(max_n=9, figsize=(4,4)) . . 100.54% [344064/342207 00:00&lt;00:00] One can use Random Splitter as well . pets = DataBlock(blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(), get_y=Pipeline([attrgetter(&quot;name&quot;), RegexLabeller(pat = r&#39;^(.*)_ d+.jpg$&#39;)]), item_tfms=Resize(128), batch_tfms=aug_transforms()) . dls = pets.dataloaders(untar_data(URLs.PETS)/&quot;images&quot;) dls.show_batch(max_n=9) . The Pascal dataset is originally an object detection dataset (we have to predict where some objects are in pictures). But it contains lots of pictures with various objects in them, so it gives a great example for a multi-label problem. Let&#8217;s download it and have a look at the data: . pascal_source = untar_data(URLs.PASCAL_2007) df = pd.read_csv(pascal_source/&quot;train.csv&quot;) . . 100.00% [1637801984/1637796771 00:54&lt;00:00] df.head(5) . fname labels is_valid . 0 000005.jpg | chair | True | . 1 000007.jpg | car | True | . 2 000009.jpg | horse person | True | . 3 000012.jpg | car | False | . 4 000016.jpg | bicycle | True | . pascal = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), splitter=ColSplitter(), get_x=ColReader(0, pref=pascal_source/&quot;train&quot;), get_y=ColReader(1, label_delim=&#39; &#39;), item_tfms=Resize(224), batch_tfms=aug_transforms()) . dls = pascal.dataloaders(df) dls.show_batch() . Alternative way to write the data block . pascal = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), splitter=ColSplitter(), get_x=lambda x:pascal_source/&quot;train&quot;/f&#39;{x[0]}&#39;, get_y=lambda x:x[1].split(&#39; &#39;), item_tfms=Resize(224), batch_tfms=aug_transforms()) dls = pascal.dataloaders(df) dls.show_batch() . Image Localization . There are various problems that fall in the image localization category:image segmentation (which is a task where you have to predict the class of each pixel of an image), coordinate predictions (predict one or several key points on an image) and object detection (draw a box around objects to detect). . path = untar_data(URLs.CAMVID_TINY) . . 100.18% [2318336/2314212 00:00&lt;00:00] path.ls() . (#3) [Path(&#39;/root/.fastai/data/camvid_tiny/images&#39;),Path(&#39;/root/.fastai/data/camvid_tiny/codes.txt&#39;),Path(&#39;/root/.fastai/data/camvid_tiny/labels&#39;)] . The MaskBlock is generated with the codes that give the correpondence between pixel value of the masks and the object they correspond to (like car, road, pedestrian&#8230;). . camvid = DataBlock(blocks=(ImageBlock, MaskBlock(codes = np.loadtxt(path/&#39;codes.txt&#39;, dtype=str))), get_items=get_image_files, splitter=RandomSplitter(), get_y=lambda o: path/&#39;labels&#39;/f&#39;{o.stem}_P{o.suffix}&#39;, batch_tfms=aug_transforms()) . dls = camvid.dataloaders(path/&quot;images&quot;) dls.show_batch() . Points . biwi_source = untar_data(URLs.BIWI_SAMPLE) fn2ctr = load_pickle(biwi_source/&#39;centers.pkl&#39;) . . 100.71% [598016/593774 00:00&lt;00:00] biwi = DataBlock(blocks=(ImageBlock, PointBlock), get_items=get_image_files, splitter=RandomSplitter(), get_y=lambda o:fn2ctr[o.name].flip(0), batch_tfms=aug_transforms()) . dls = biwi.dataloaders(biwi_source) dls.show_batch(max_n=9) . coco_source = untar_data(URLs.COCO_TINY) images, lbl_bbox = get_annotations(coco_source/&#39;train.json&#39;) img2bbox = dict(zip(images, lbl_bbox)) . . 100.22% [802816/801038 00:00&lt;00:00] Bounding Boxes . coco_source = untar_data(URLs.COCO_TINY) images, lbl_bbox = get_annotations(coco_source/&#39;train.json&#39;) img2bbox = dict(zip(images, lbl_bbox)) . . 100.22% [802816/801038 00:00&lt;00:00] _We provide three types, because we have two targets:the bounding boxes and the labels. That&#8217;s why we pass ninp=1 at the end, to tell the library where the inputs stop and the targets begin. . coco = DataBlock(blocks=(ImageBlock, BBoxBlock, BBoxLblBlock), get_items=get_image_files, splitter=RandomSplitter(), get_y=[lambda o: img2bbox[o.name][0], lambda o: img2bbox[o.name][1]], item_tfms=Resize(128), batch_tfms=aug_transforms(), n_inp=1) . dls = coco.dataloaders(coco_source) dls.show_batch(max_n=9) . Text . from fastai.text.all import * . path = untar_data(URLs.IMDB_SAMPLE) df = pd.read_csv(path/&#39;texts.csv&#39;) df.head() . . 100.28% [573440/571827 00:00&lt;00:00] label text is_valid . 0 negative | Un-bleeping-believable! Meg Ryan doesn&#39;t even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff! | False | . 1 positive | This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.&lt;br /&gt;&lt;br /&gt;But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som... | False | . 2 negative | Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.&lt;br /&gt;&lt;br /&gt;Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li... | False | . 3 positive | Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man&#39;s life - interestingly enough the man&#39;s entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.&lt;br /&gt;&lt;br /&gt;Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie &quot;Duty, Honor, Country&quot; are not just mere words blathered from the lips of a high-brassed offic... | False | . 4 negative | This movie succeeds at being one of the most unique movies you&#39;ve seen. However this comes from the fact that you can&#39;t make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don&#39;t want to feel slighted you&#39;ll sit through this horrible film and develop a real sense of pity for the actors involved, they&#39;ve all seen better days, but then you realize they actually got paid quite a bit of money to do this and you&#39;ll lose pity for them just like you&#39;ve alr... | False | . imdb_lm = DataBlock(blocks=TextBlock.from_df(&#39;text&#39;, is_lm=True), get_x=ColReader(&#39;text&#39;), splitter=ColSplitter()) . dls = imdb_lm.dataloaders(df, bs=64, seq_len=72) dls.show_batch(max_n=6) . text text_ . 0 xxbos xxmaj my kids recently started watching the xxunk of this show - both the early episodes on the xxup n , and the later ones on xxup abc xxmaj family - and they love it . ( i was n&#39;t aware the show had even lasted past the first or second season ) xxmaj i &#39;m curious as to what xxunk all of the cast changes - xxmaj i &#39;ve seen | xxmaj my kids recently started watching the xxunk of this show - both the early episodes on the xxup n , and the later ones on xxup abc xxmaj family - and they love it . ( i was n&#39;t aware the show had even lasted past the first or second season ) xxmaj i &#39;m curious as to what xxunk all of the cast changes - xxmaj i &#39;ve seen them | . 1 and junior had n&#39;t xxunk her wings . xxmaj xxunk gene , i suppose . xxmaj by the way , we can now make an educated guess that xxmaj grendel &#39;s pop was probably xxmaj xxunk xxmaj thing . n n - xxmaj grendel and mom chose to randomly kill , fly away with or drag away their prey based only on a close reading of the next few xxunk of the script | junior had n&#39;t xxunk her wings . xxmaj xxunk gene , i suppose . xxmaj by the way , we can now make an educated guess that xxmaj grendel &#39;s pop was probably xxmaj xxunk xxmaj thing . n n - xxmaj grendel and mom chose to randomly kill , fly away with or drag away their prey based only on a close reading of the next few xxunk of the script . | . 2 a very funny show . xxmaj let &#39;s hope more episodes turn up on youtube and lets hope that someone will release &quot; the xxmaj fosters &quot; on xxup dvd in xxmaj england . n n xxmaj best xxmaj episode : xxmaj sex and the xxmaj evans xxunk xxmaj series 1 episode 6 . xxmaj the xxmaj foster &#39;s episode of it was called xxmaj sex in the xxmaj black xxmaj community . | very funny show . xxmaj let &#39;s hope more episodes turn up on youtube and lets hope that someone will release &quot; the xxmaj fosters &quot; on xxup dvd in xxmaj england . n n xxmaj best xxmaj episode : xxmaj sex and the xxmaj evans xxunk xxmaj series 1 episode 6 . xxmaj the xxmaj foster &#39;s episode of it was called xxmaj sex in the xxmaj black xxmaj community . xxmaj | . 3 forces ( who , amusingly , are made to speak in xxunk - up xxunk ! ) are xxunk by our heroic trio alone , much to the king &#39;s xxunk who , as portrayed by xxmaj marcel xxmaj xxunk  best - known for his role of leader of the xxmaj parisian xxunk in xxmaj marcel xxmaj xxunk &#39; &#39;s xxup children xxup of xxup paradise ( xxunk )  is | ( who , amusingly , are made to speak in xxunk - up xxunk ! ) are xxunk by our heroic trio alone , much to the king &#39;s xxunk who , as portrayed by xxmaj marcel xxmaj xxunk  best - known for his role of leader of the xxmaj parisian xxunk in xxmaj marcel xxmaj xxunk &#39; &#39;s xxup children xxup of xxup paradise ( xxunk )  is himself | . 4 cost , because it does n&#39;t project the true image of xxmaj batman . xxmaj this cartoon is more like a xxunk xxmaj kung xxmaj fu xxmaj flick and if you really wanna see a classic xxmaj batman cartoon i strongly recommend xxmaj batman the xxmaj animated xxmaj series , but this cartoon is nothing more than a piece of s xxrep 3 - xxup t ! xxmaj get xxmaj batman : | , because it does n&#39;t project the true image of xxmaj batman . xxmaj this cartoon is more like a xxunk xxmaj kung xxmaj fu xxmaj flick and if you really wanna see a classic xxmaj batman cartoon i strongly recommend xxmaj batman the xxmaj animated xxmaj series , but this cartoon is nothing more than a piece of s xxrep 3 - xxup t ! xxmaj get xxmaj batman : xxmaj | . 5 said that the book is better . xxmaj i &#39;m sure it &#39;s not and i do n&#39;t care anyway i loved the movie . xxmaj as in all of xxmaj arnold &#39;s films the acting is what you would expect with classic one liners from xxmaj arnold and even xxmaj xxunk gets a couple in . xxmaj but without a doubt xxmaj richard xxmaj dawson is the standout in this film | that the book is better . xxmaj i &#39;m sure it &#39;s not and i do n&#39;t care anyway i loved the movie . xxmaj as in all of xxmaj arnold &#39;s films the acting is what you would expect with classic one liners from xxmaj arnold and even xxmaj xxunk gets a couple in . xxmaj but without a doubt xxmaj richard xxmaj dawson is the standout in this film . | . Tabular . from fastai.tabular.core import * . adult_source = untar_data(URLs.ADULT_SAMPLE) df = pd.read_csv(adult_source/&#39;adult.csv&#39;) df.head() . . 100.69% [974848/968212 00:00&lt;00:00] age workclass fnlwgt education education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country salary . 0 49 | Private | 101320 | Assoc-acdm | 12.0 | Married-civ-spouse | NaN | Wife | White | Female | 0 | 1902 | 40 | United-States | &gt;=50k | . 1 44 | Private | 236746 | Masters | 14.0 | Divorced | Exec-managerial | Not-in-family | White | Male | 10520 | 0 | 45 | United-States | &gt;=50k | . 2 38 | Private | 96185 | HS-grad | NaN | Divorced | NaN | Unmarried | Black | Female | 0 | 0 | 32 | United-States | &lt;50k | . 3 38 | Self-emp-inc | 112847 | Prof-school | 15.0 | Married-civ-spouse | Prof-specialty | Husband | Asian-Pac-Islander | Male | 0 | 0 | 40 | United-States | &gt;=50k | . 4 42 | Self-emp-not-inc | 82297 | 7th-8th | NaN | Married-civ-spouse | Other-service | Wife | Black | Female | 0 | 0 | 50 | United-States | &lt;50k | . cat_names = [&#39;workclass&#39;, &#39;education&#39;, &#39;marital-status&#39;, &#39;occupation&#39;, &#39;relationship&#39;, &#39;race&#39;] cont_names = [&#39;age&#39;, &#39;fnlwgt&#39;, &#39;education-num&#39;] . Standard preprocessing in fastai, use those pre-processors: . procs = [Categorify, FillMissing, Normalize] . splits = RandomSplitter()(range_of(df)) to = TabularPandas(df, procs, cat_names, cont_names, y_names=&quot;salary&quot;, splits=splits, y_block=CategoryBlock) . dls = to.dataloaders() dls.show_batch() . workclass education marital-status occupation relationship race education-num_na age fnlwgt education-num salary . 0 Private | 10th | Never-married | Machine-op-inspct | Not-in-family | White | False | 33.0 | 67005.996152 | 6.0 | &lt;50k | . 1 Private | 12th | Married-civ-spouse | Craft-repair | Husband | White | False | 21.0 | 83703.995755 | 8.0 | &lt;50k | . 2 Private | Preschool | Married-civ-spouse | Other-service | Not-in-family | White | False | 52.0 | 416129.003576 | 1.0 | &lt;50k | . 3 Local-gov | HS-grad | Married-civ-spouse | Protective-serv | Husband | White | False | 34.0 | 155780.998653 | 9.0 | &lt;50k | . 4 Private | HS-grad | Never-married | Adm-clerical | Not-in-family | White | False | 19.0 | 184758.999919 | 9.0 | &lt;50k | . 5 ? | Bachelors | Never-married | ? | Own-child | White | False | 25.0 | 47010.997235 | 13.0 | &lt;50k | . 6 Private | Masters | Never-married | Prof-specialty | Not-in-family | White | False | 30.0 | 196342.000092 | 14.0 | &lt;50k | . 7 Private | HS-grad | Married-civ-spouse | Handlers-cleaners | Husband | Black | False | 27.0 | 275110.002518 | 9.0 | &gt;=50k | . 8 ? | Bachelors | Never-married | ? | Unmarried | Asian-Pac-Islander | False | 27.0 | 190650.000040 | 13.0 | &lt;50k | . 9 Private | Assoc-voc | Never-married | Craft-repair | Not-in-family | White | False | 32.0 | 38797.002375 | 11.0 | &lt;50k | . End of Notebook .",
            "url": "https://prashantmdgl9.github.io/ml_experiments/2022/08/17/data-block-from-scratch-fastai.html",
            "relUrl": "/2022/08/17/data-block-from-scratch-fastai.html",
            "date": " • Aug 17, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "Tau to Muon Decay - Finding New Physics",
            "content": "Description :The aim of this notebook is to find a phenomenon that is not already known to exist &#8211; charged lepton flavour violation &#8211; thereby helping to establish &quot;new physics&quot;. . Problem Statement: From a list of collision events and their properties, predict whether a &#964; &#8594; 3&#956; decay happened in this collision. This &#964; &#8594; 3&#956; is currently assumed by scientists not to happen, and the goal of this competition is to discover &#964; &#8594; 3&#956; happening more frequently than scientists currently can understand. . Data : https://www.kaggle.com/competitions/flavours-of-physics-kernels-only/data . Imports and File Reads . # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python # For example, here&#39;s several helpful packages to load import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # Input data files are available in the read-only &quot;../input/&quot; directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) # You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using &quot;Save &amp; Run All&quot; # You can also write temporary files to /kaggle/temp/, but they won&#39;t be saved outside of the current session . /kaggle/input/flavours-of-physics-kernels-only/check_correlation.csv.zip /kaggle/input/flavours-of-physics-kernels-only/sample_submission.csv.zip /kaggle/input/flavours-of-physics-kernels-only/training.csv.zip /kaggle/input/flavours-of-physics-kernels-only/check_agreement.csv.zip /kaggle/input/flavours-of-physics-kernels-only/test.csv.zip . import pandas as pd from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier from IPython.display import display from sklearn import metrics import matplotlib.pyplot as plt import seaborn as sns import math import dateutil.parser from sklearn.preprocessing import MinMaxScaler import numpy as np from sklearn.ensemble import GradientBoostingClassifier from sklearn.ensemble import RandomForestRegressor from sklearn.tree import DecisionTreeRegressor import sklearn.metrics as metrics from sklearn.metrics import accuracy_score,confusion_matrix, f1_score, confusion_matrix, f1_score, classification_report, roc_curve, roc_auc_score, auc, precision_recall_curve, precision_score, recall_score . import xgboost as xgb . np.set_printoptions(linewidth=140) pd.set_option(&#39;display.width&#39;, 140) . Read in the data . path = &#39;/kaggle/input/flavours-of-physics-kernels-only/&#39; train = pd.read_csv(path+&#39;training.csv.zip&#39;, index_col=&#39;id&#39;) test = pd.read_csv(path+&#39;test.csv.zip&#39;, index_col = &quot;id&quot;) . train.head(3) . LifeTime dira FlightDistance FlightDistanceError IP IPSig VertexChi2 pt DOCAone DOCAtwo ... p1_p p2_p p0_eta p1_eta p2_eta SPDhits production signal mass min_ANNmuon . id . 18453471 0.001578 | 0.999999 | 14.033335 | 0.681401 | 0.016039 | 0.451886 | 1.900433 | 1482.037476 | 0.066667 | 0.060602 | ... | 12290.760742 | 39264.398438 | 3.076006 | 4.003800 | 4.031514 | 458 | -99 | 0 | 1866.300049 | 0.277559 | . 5364094 0.000988 | 0.999705 | 5.536157 | 0.302341 | 0.142163 | 9.564503 | 0.865666 | 3050.720703 | 0.024022 | 0.019245 | ... | 16562.667969 | 7341.257812 | 3.228553 | 2.786543 | 2.975564 | 406 | -99 | 0 | 1727.095947 | 0.225924 | . 11130990 0.000877 | 0.999984 | 6.117302 | 0.276463 | 0.034746 | 1.970751 | 10.975849 | 3895.908691 | 0.055044 | 0.047947 | ... | 22695.388672 | 10225.309570 | 3.536903 | 2.865686 | 3.052810 | 196 | -99 | 0 | 1898.588013 | 0.368630 | . 3 rows × 50 columns . train.shape, test.shape . ((67553, 50), (855819, 46)) . Nothing is null, that&#39;s good . train.isna().sum() . LifeTime 0 dira 0 FlightDistance 0 FlightDistanceError 0 IP 0 IPSig 0 VertexChi2 0 pt 0 DOCAone 0 DOCAtwo 0 DOCAthree 0 IP_p0p2 0 IP_p1p2 0 isolationa 0 isolationb 0 isolationc 0 isolationd 0 isolatione 0 isolationf 0 iso 0 CDF1 0 CDF2 0 CDF3 0 ISO_SumBDT 0 p0_IsoBDT 0 p1_IsoBDT 0 p2_IsoBDT 0 p0_track_Chi2Dof 0 p1_track_Chi2Dof 0 p2_track_Chi2Dof 0 p0_IP 0 p1_IP 0 p2_IP 0 p0_IPSig 0 p1_IPSig 0 p2_IPSig 0 p0_pt 0 p1_pt 0 p2_pt 0 p0_p 0 p1_p 0 p2_p 0 p0_eta 0 p1_eta 0 p2_eta 0 SPDhits 0 production 0 signal 0 mass 0 min_ANNmuon 0 dtype: int64 . There are a few variables such as SPDhits and p0IPSig that have larger numerical values than the others. . train.describe() . LifeTime dira FlightDistance FlightDistanceError IP IPSig VertexChi2 pt DOCAone DOCAtwo ... p1_p p2_p p0_eta p1_eta p2_eta SPDhits production signal mass min_ANNmuon . count 67553.000000 | 67553.000000 | 67553.000000 | 67553.000000 | 67553.000000 | 67553.000000 | 67553.000000 | 67553.000000 | 6.755300e+04 | 6.755300e+04 | ... | 67553.000000 | 67553.000000 | 67553.000000 | 67553.000000 | 67553.000000 | 67553.000000 | 67553.000000 | 67553.000000 | 67553.000000 | 67553.000000 | . mean 0.001255 | 0.999846 | 15.153986 | 0.501794 | 0.090363 | 5.305426 | 5.132504 | 5027.967460 | 4.496067e-02 | 3.760934e-02 | ... | 33050.376570 | 24407.169131 | 3.280283 | 3.197816 | 3.233437 | 269.119832 | -36.657765 | 0.616908 | 1779.389403 | 0.488508 | . std 0.000779 | 0.000540 | 15.060723 | 0.426345 | 0.085216 | 4.331420 | 3.849261 | 3010.766079 | 6.052672e-02 | 4.129047e-02 | ... | 30190.555855 | 25076.484396 | 0.592017 | 0.592466 | 0.588628 | 127.446169 | 49.145483 | 0.486144 | 66.956336 | 0.238141 | . min 0.000144 | 0.990090 | 0.804510 | 0.075224 | 0.000264 | 0.020823 | 0.001895 | 314.100067 | 9.142349e-08 | 2.434845e-08 | ... | 3052.763672 | 3008.329102 | 1.638945 | 1.673761 | 1.746441 | 5.000000 | -99.000000 | 0.000000 | 1480.453491 | 0.000177 | . 25% 0.000725 | 0.999916 | 6.225103 | 0.236352 | 0.027773 | 1.830687 | 1.995162 | 3157.041748 | 1.298088e-02 | 1.131008e-02 | ... | 14357.550781 | 9325.879883 | 2.848309 | 2.746091 | 2.792214 | 171.000000 | -99.000000 | 0.000000 | 1766.956787 | 0.277863 | . 50% 0.001061 | 0.999985 | 10.604475 | 0.366318 | 0.058136 | 3.628297 | 4.116704 | 4366.348145 | 2.912052e-02 | 2.549105e-02 | ... | 23996.681641 | 16510.722656 | 3.249017 | 3.166046 | 3.205477 | 254.000000 | 1.000000 | 1.000000 | 1777.976562 | 0.455296 | . 75% 0.001559 | 0.999997 | 18.554255 | 0.598817 | 0.134010 | 8.221298 | 7.598480 | 6079.195801 | 5.754587e-02 | 4.908246e-02 | ... | 40976.976562 | 30108.750000 | 3.688323 | 3.623785 | 3.647688 | 354.000000 | 1.000000 | 1.000000 | 1789.937622 | 0.695368 | . max 0.022134 | 1.000000 | 449.242554 | 5.878616 | 2.245918 | 24.074131 | 14.998641 | 74390.289062 | 8.312201e+00 | 8.892956e-01 | ... | 602064.750000 | 461608.593750 | 5.164036 | 5.090231 | 5.124544 | 632.000000 | 10.000000 | 1.000000 | 1949.984009 | 0.985871 | . 8 rows × 50 columns . &#39;production&#39;, &#39;mass&#39;, &#39;min_ANNmuon&#39; aren&#39;t in the test, so they have to be removed . all_num_cols = [&#39;LifeTime&#39;, &#39;dira&#39;, &#39;FlightDistance&#39;, &#39;FlightDistanceError&#39;, &#39;IP&#39;, &#39;IPSig&#39;, &#39;VertexChi2&#39;, &#39;pt&#39;, &#39;DOCAone&#39;, &#39;DOCAtwo&#39;, &#39;DOCAthree&#39;, &#39;IP_p0p2&#39;, &#39;IP_p1p2&#39;, &#39;isolationa&#39;, &#39;isolationb&#39;, &#39;isolationc&#39;, &#39;isolationd&#39;, &#39;isolatione&#39;, &#39;isolationf&#39;, &#39;iso&#39;, &#39;CDF1&#39;, &#39;CDF2&#39;, &#39;CDF3&#39;, &#39;ISO_SumBDT&#39;, &#39;p0_IsoBDT&#39;, &#39;p1_IsoBDT&#39;, &#39;p2_IsoBDT&#39;, &#39;p0_track_Chi2Dof&#39;, &#39;p1_track_Chi2Dof&#39;, &#39;p2_track_Chi2Dof&#39;, &#39;p0_IP&#39;, &#39;p1_IP&#39;, &#39;p2_IP&#39;, &#39;p0_IPSig&#39;, &#39;p1_IPSig&#39;, &#39;p2_IPSig&#39;, &#39;p0_pt&#39;, &#39;p1_pt&#39;, &#39;p2_pt&#39;, &#39;p0_p&#39;, &#39;p1_p&#39;, &#39;p2_p&#39;, &#39;p0_eta&#39;, &#39;p1_eta&#39;, &#39;p2_eta&#39;, &#39;SPDhits&#39;] . y = train[&quot;signal&quot;] x = train[all_num_cols] . Scale the data, so everyone is speaking the same numerical precision . from sklearn.preprocessing import StandardScaler sc = StandardScaler() X_scaled= pd.DataFrame(sc.fit_transform(x), columns=all_num_cols) X_scaled.var() . LifeTime 1.000015 dira 1.000015 FlightDistance 1.000015 FlightDistanceError 1.000015 IP 1.000015 IPSig 1.000015 VertexChi2 1.000015 pt 1.000015 DOCAone 1.000015 DOCAtwo 1.000015 DOCAthree 1.000015 IP_p0p2 1.000015 IP_p1p2 1.000015 isolationa 1.000015 isolationb 1.000015 isolationc 1.000015 isolationd 1.000015 isolatione 1.000015 isolationf 1.000015 iso 1.000015 CDF1 1.000015 CDF2 1.000015 CDF3 1.000015 ISO_SumBDT 1.000015 p0_IsoBDT 1.000015 p1_IsoBDT 1.000015 p2_IsoBDT 1.000015 p0_track_Chi2Dof 1.000015 p1_track_Chi2Dof 1.000015 p2_track_Chi2Dof 1.000015 p0_IP 1.000015 p1_IP 1.000015 p2_IP 1.000015 p0_IPSig 1.000015 p1_IPSig 1.000015 p2_IPSig 1.000015 p0_pt 1.000015 p1_pt 1.000015 p2_pt 1.000015 p0_p 1.000015 p1_p 1.000015 p2_p 1.000015 p0_eta 1.000015 p1_eta 1.000015 p2_eta 1.000015 SPDhits 1.000015 dtype: float64 . X_scaled.head() . LifeTime dira FlightDistance FlightDistanceError IP IPSig VertexChi2 pt DOCAone DOCAtwo ... p0_pt p1_pt p2_pt p0_p p1_p p2_p p0_eta p1_eta p2_eta SPDhits . 0 0.415529 | 0.283944 | -0.074409 | 0.421273 | -0.872181 | -1.120551 | -0.839666 | -1.177759 | 0.358619 | 0.556846 | ... | -0.891851 | -1.166085 | -0.252180 | -0.808179 | -0.687625 | 0.592481 | -0.345054 | 1.360399 | 1.355836 | 1.482050 | . 1 -0.342113 | -0.261629 | -0.638608 | -0.467825 | 0.607873 | 0.983305 | -1.108491 | -0.656730 | -0.345945 | -0.444765 | ... | -0.526101 | -0.223803 | -0.687607 | -0.501196 | -0.546125 | -0.680559 | -0.087379 | -0.694178 | -0.438094 | 1.074031 | . 2 -0.485059 | 0.255793 | -0.600021 | -0.528523 | -0.652664 | -0.769886 | 1.518055 | -0.376006 | 0.166591 | 0.250368 | ... | -0.523686 | 0.098579 | -0.541693 | -0.292941 | -0.342990 | -0.565548 | 0.433471 | -0.560594 | -0.306864 | -0.573735 | . 3 -0.513937 | 0.105325 | -0.659065 | -0.659225 | -0.163987 | -0.238744 | -0.482212 | -0.337852 | 0.145702 | -0.755444 | ... | -0.054039 | -0.629221 | -0.055558 | -0.280617 | -0.534637 | -0.608772 | -0.325705 | 0.034125 | -1.457376 | -1.036679 | . 4 -0.160761 | 0.276437 | 1.587953 | 3.275310 | 0.358768 | -0.073982 | -1.211711 | -0.293423 | -0.668629 | -0.006868 | ... | 0.423617 | -0.389092 | -0.326862 | 4.031841 | 2.138513 | 0.905700 | 2.283754 | 2.554325 | 1.806658 | 1.631133 | . 5 rows × 46 columns . Train Validation Split . The test data that&#39;s supplied doesn&#39;t contain a signal column, so it can&#39;t be used for testing the robustness of the model. It will be used only for making the predictions at the end. So, split the train into train and validation sets and the latter would be used as the &quot;test&quot; set. . from sklearn.model_selection import train_test_split X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.25, random_state=42) . train.shape, x.shape, X_train.shape, X_val.shape . ((67553, 50), (67553, 46), (50664, 46), (16889, 46)) . Fit a model without any adjustments to check what variables are important . print(&quot;Random Forest&quot;) rf = RandomForestClassifier(n_estimators=100, random_state=11) rf.fit(X_train, y_train) . Random Forest . RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=11, verbose=0, warm_start=False) . def rf_feat_importance(m, df): return pd.DataFrame({&#39;cols&#39;:df.columns, &#39;imp&#39;:m.feature_importances_} ).sort_values(&#39;imp&#39;, ascending=False) . fi = rf_feat_importance(rf, X_train) fi.sort_values(&quot;imp&quot;, ascending = False)[:10] . cols imp . 5 IPSig | 0.121363 | . 4 IP | 0.094929 | . 1 dira | 0.077996 | . 45 SPDhits | 0.052077 | . 27 p0_track_Chi2Dof | 0.039788 | . 6 VertexChi2 | 0.039060 | . 23 ISO_SumBDT | 0.037675 | . 19 iso | 0.026700 | . 24 p0_IsoBDT | 0.026252 | . 0 LifeTime | 0.026069 | . def plot_fi(fi): return fi.plot(&#39;cols&#39;, &#39;imp&#39;, &#39;barh&#39;, figsize=(12,7), legend=False) plot_fi(fi[:20]); . Use the selected columns to create a slice of the training data . selected_cols = [&quot;IPSig&quot;, &quot;IP&quot;, &quot;dira&quot;, &quot;SPDhits&quot;, &quot;VertexChi2&quot;, &quot;p0_track_Chi2Dof&quot;, &quot;ISO_SumBDT&quot;, &quot;iso&quot;, &quot;LifeTime&quot;, &quot;p0_IsoBDT&quot;, &quot;p0_IPSig&quot;, &quot;isolatione&quot; ] . X_train = X_train[selected_cols] . X_train.shape . (50664, 12) . Not ideal profiles but that&#39;s expected in the experiments. Hopefully, Random Forests and Boosting algos will take care of them . X_train.hist(bins=30, figsize=(20, 14)) . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f70a83472d0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f70a82227d0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f70a81d4e10&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f70a8198650&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f70a814ce50&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f70a810e690&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f70a80c1e90&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f70a80856d0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f70a808e250&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f70a8041bd0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f7095bfcf10&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f7095bbe750&gt;]], dtype=object) . Good that data isn&#39;t just only signal or only background. 3:2 ratio seems ok! . y_train.hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f7095501290&gt; . Don&#39;t run these, they actually made things worse . X_train[&#39;SPDhits&#39;] = np.log(X_train[&#39;SPDhits&#39;]) X_train[&#39;p0_IPSig&#39;] = np.log(X_train[&#39;p0_IPSig&#39;]) . X_train[&quot;SPDhits&quot;].hist(bins=30, figsize=(5, 3)) . X_train[&quot;p0_IPSig&quot;].hist(bins=30, figsize=(5, 3)) . Don&#39;t run ends . Correlation Matrix for the variables selected. Some strong correlations owe their values to the method of production . plt.figure(figsize=(15,10)) sns.heatmap(X_train.corr(), annot=True, cbar=False, fmt=&#39;.1f&#39;, cmap=&#39;summer_r&#39;) plt.show() . Helper Functions . Root mean squared error . def r_mse(pred,y): return round(math.sqrt(((pred-y)**2).mean()), 6) . Confusion matrix . def con_matrix(y_val, val_preds_bin): matrix = confusion_matrix(y_val, val_preds_bin) matrix = matrix.astype(&#39;float&#39;) / matrix.sum(axis=1)[:, np.newaxis] # Build the plot plt.figure(figsize=(8,5)) sns.set(font_scale=1.4) sns.heatmap(matrix, annot=True, annot_kws={&#39;size&#39;:10}, cmap=plt.cm.Greens, linewidths=0.2) # Add labels to the plot class_names = [&#39;Signal&#39;, &#39;Background&#39;] tick_marks = np.arange(len(class_names)) tick_marks2 = tick_marks + 0.5 plt.xticks(tick_marks, class_names, rotation=0) plt.yticks(tick_marks2, class_names, rotation=0) plt.xlabel(&#39;Predicted label&#39;) plt.ylabel(&#39;True label&#39;) plt.title(&#39;Confusion Matrix for Ensemble Model&#39;) plt.show() . Metrics . def metric_report(y_val, pred_val): print(&quot;Accuracy Random_Forest:&quot;, accuracy_score(y_val, pred_val)) print(&quot;Precision Random_Forest:&quot;, precision_score(y_val, pred_val)) print(&quot;Recall Random_Forest:&quot;, recall_score(y_val, pred_val)) print(&quot;F1 Score Random_Forest:&quot;, f1_score(y_val, pred_val)) . ROC Curve . def roc_curvePlot(y_val, val_preds): fpr, tpr, threshold = metrics.roc_curve(y_val, val_preds) roc_auc = metrics.auc(fpr, tpr) import matplotlib.pyplot as plt plt.style.use(&#39;default&#39;) plt.title(&#39;ROC curve&#39;) plt.plot(tpr, 1-fpr, &#39;b&#39;, label = &#39;AUC = %0.2f&#39; % roc_auc) plt.legend(loc = &#39;lower right&#39;) plt.plot([0, 1], [1, 0],&#39;r--&#39;) plt.xlim([0, 1]) plt.ylim([0, 1]) plt.xlabel(&#39;Signal efficiency (TPR)&#39;) plt.ylabel(&#39;Background rejection (1-FPR)&#39;) plt.show() . Models . Decision Tree . m = DecisionTreeRegressor() m.fit(X_train, y_train); . X_val = X_val[selected_cols] X_val.shape . (16889, 12) . pred_val = m.predict(X_val) . y_val.shape, pred_val.shape . ((16889,), (16889,)) . r_mse(pred_val, y_val) . 0.450787 . Random Forests . print(&quot;Random Forest&quot;) trees = [10, 50, 100, 150, 200,250, 300,350, 400,450, 500, 550, 600, 650] err = [] tm = [] for t in trees: start = time.time() print(&quot;Preparing trees: &quot;, t) rf = RandomForestClassifier(n_estimators=t, random_state=11) rf.fit(X_train, y_train) pred_val_rf = rf.predict(X_val) err.append(r_mse(pred_val_rf, y_val)) end = time.time() tm.append(end-start) print(f&quot; For Tree with depth {i}, it took {end - start} second&quot;) . Random Forest Preparing trees: 10 For Tree with depth 9, it took 1.905045986175537 second Preparing trees: 50 For Tree with depth 9, it took 9.269953727722168 second Preparing trees: 100 For Tree with depth 9, it took 18.74191951751709 second Preparing trees: 150 For Tree with depth 9, it took 27.8441321849823 second Preparing trees: 200 For Tree with depth 9, it took 37.205913066864014 second Preparing trees: 250 For Tree with depth 9, it took 46.68583273887634 second Preparing trees: 300 For Tree with depth 9, it took 55.99767518043518 second Preparing trees: 350 For Tree with depth 9, it took 65.16085720062256 second Preparing trees: 400 For Tree with depth 9, it took 74.60902190208435 second Preparing trees: 450 For Tree with depth 9, it took 83.86420130729675 second Preparing trees: 500 For Tree with depth 9, it took 93.22141027450562 second Preparing trees: 550 For Tree with depth 9, it took 104.05555772781372 second Preparing trees: 600 For Tree with depth 9, it took 112.27986359596252 second Preparing trees: 650 For Tree with depth 9, it took 121.98293781280518 second . Adding trees after a while doesn&#39;t help! . plt.plot(trees, err) . [&lt;matplotlib.lines.Line2D at 0x7f70838c83d0&gt;] . Time increases linearly, I was expecting exponential . plt.plot(trees, tm) . [&lt;matplotlib.lines.Line2D at 0x7f70839a41d0&gt;] . plt.plot([r_mse(pred_val[:i+1].mean(0), y_val) for i in range(200)]); . Decent scores . metric_report(y_val, pred_val_rf) . Accuracy Random_Forest: 0.8670140328024157 Precision Random_Forest: 0.8891304347826087 Recall Random_Forest: 0.8976145038167939 F1 Score Random_Forest: 0.89335232668566 . confusion_matrix(y_val, pred_val_rf) . array([[5236, 1173], [1073, 9407]]) . Confusion matrix . con_matrix(y_val, pred_val_rf) . Xgboost . print(&quot;XGBoost&quot;) params = {&quot;objective&quot;: &quot;binary:logistic&quot;, &quot;base_Score&quot;: 0.5, &quot;eta&quot;: 0.3, &quot;max_depth&quot;: 5, &quot;min_child_weight&quot;: 3, &quot;silent&quot;: 1, &quot;subsample&quot;: 0.7, &quot;colsample_bytree&quot;: 0.7, &quot;seed&quot;: 1} num_trees=200 . XGBoost . gbm = xgb.train(params, xgb.DMatrix(X_train, y_train), num_trees) . pred_gbm = gbm.predict(xgb.DMatrix(X_val)) . r_mse(pred_gbm, y_val) . 0.314701 . Xgboost doesn&#39;t push the values to floor or ceiling, so we&#39;ll have to do it explicitly for the accuracy score and other metrics . pred_gbm . array([0.05536356, 0.97199875, 0.02173601, ..., 0.9915537 , 0.96224195, 0.87197673], dtype=float32) . pred_gbm_bin =[] for val in pred_gbm: if val &gt; 0.5: pred_gbm_bin.append(1) else: pred_gbm_bin.append(0) . metric_report(y_val, pred_gbm_bin) . Accuracy Random_Forest: 0.8636982651429925 Precision Random_Forest: 0.884015777610819 Recall Random_Forest: 0.8981870229007634 F1 Score Random_Forest: 0.8910450586898903 . confusion_matrix(y_val, pred_gbm_bin) . array([[5174, 1235], [1067, 9413]]) . These models are not performing amazingly well! MSE is quite high but still classification seems not too bad. Out of ~16k, 14k seem to be correctly classified. . Just for the sake of it, what does old Logistic Regression says? . from sklearn.linear_model import LogisticRegression clf = LogisticRegression(random_state=0).fit(X_train, y_train) pred_lr = clf.predict(X_val) . metric_report(y_val, pred_lr) . Accuracy Random_Forest: 0.8433891882290248 Precision Random_Forest: 0.8570777504329596 Recall Random_Forest: 0.8972328244274809 F1 Score Random_Forest: 0.8766957251410191 . Hmmm...slightly worse. . Ensemble . val_preds = (pred_lr + pred_gbm_bin + pred_val_rf)/3 . val_rmse = r_mse(val_preds , y_val) val_rmse . 0.340731 . np.unique(val_preds) . array([0. , 0.33333333, 0.66666667, 1. ]) . val_preds_bin =[] for val in val_preds: if val &gt;= 0.5: val_preds_bin.append(1) else: val_preds_bin.append(0) . metric_report(y_val, val_preds_bin) . Accuracy Random_Forest: 0.8658890402036828 Precision Random_Forest: 0.8852828064909484 Recall Random_Forest: 0.9005725190839695 F1 Score Random_Forest: 0.8928622108698737 . Classification report . target_names = [&#39;signal&#39;, &#39;Background&#39;] print(classification_report(y_val, val_preds_bin, target_names=target_names)) . precision recall f1-score support signal 0.83 0.81 0.82 6409 Background 0.89 0.90 0.89 10480 accuracy 0.87 16889 macro avg 0.86 0.85 0.86 16889 weighted avg 0.87 0.87 0.87 16889 . con_matrix(y_val, val_preds_bin) . ROC Curve . roc_curvePlot(y_val, val_preds_bin) . Final predictions for the test set . Here, just taking the Xgboost&#39;s results . X_test = test[selected_cols] cols = X_test.columns X_scaled_test = pd.DataFrame(sc.fit_transform(X_test), columns=cols) . test_probs = xgb.predict(X_scaled_test) . test_probs . array([0, 0, 1, ..., 0, 1, 1]) . np.unique(test_probs) . array([0, 1]) . Submit to Kaggle . test.shape . (855819, 46) . p = pd.read_csv(path+&#39;test.csv.zip&#39;) . df_submit = pd.DataFrame() df_submit[&quot;id&quot;] = p[&quot;id&quot;] df_submit[&quot;prediction&quot;] = test_probs . df_submit.to_csv(&#39;submission.csv&#39;, index=False, sep=&#39;,&#39;) . End of Notebook .",
            "url": "https://prashantmdgl9.github.io/ml_experiments/2021/06/09/Ensemble-model-of-rf-xgboost-lr.html",
            "relUrl": "/2021/06/09/Ensemble-model-of-rf-xgboost-lr.html",
            "date": " • Jun 9, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Understanding Cross Entropy",
            "content": "Understanding Cross Entropy . The sigmoid function always outputs a number between 0 and 1. This is an important function in deep learning, since we often want to ensure values are between 0 and 1. . . It takes any input value, positive or negative, and smooshes it onto an output value between 0 and 1. It’s also a smooth curve that only goes up, which makes it easier for SGD to find meaningful gradients. . Loss is whatever function we’ve decided to use to optimize the parameters of our model. We want to minimise the loss. . Cross-Entropy Loss . Cross-entropy loss is a loss function that has two benefits: . It works even when our dependent variable has more than two categories. | It results in faster and more reliable training. | In binary case, one can use sigmoid for compressing the numbers between 0 and 1; then use sigmoid and 1-sigmoid as the prediction probabilities. This won’t work for the multiclass classification such as MNIST digit, Pets or other datasets, so we can’t just take the sigmoid. What we want is a method which will take all the numbers and squish them between 0 and 1 in a manner that the prediction across all the classes adds to 1. # Law of probability. . This function should reflect how strong and confident our prediction is. Softmax is one such function. Just like sigmoid, it is an exponential based function. Exponential is inverse of the logarithm, it is always positive, and it increases very rapidly. Softmax = take exponent of the number / sum of all exponents of all numbers . The exponential has a nice property: if one of the numbers in our activations x is slightly bigger than the others, the exponential will amplify this (since it grows, well… exponentially), which means that in the softmax, that number will be closer to 1. . Intuitively, the softmax function really wants to pick one class among the others, so it’s ideal for training a classifier when we know each picture has a definite label. It may be less ideal during inference, as you might want your model to sometimes tell you it doesn’t recognize any of the classes that it has seen during training, and not pick a class because it has a slightly bigger activation score. I . F.nll_loss takes the softmax and then takes the negative. NLL is the Negative Log Likelihood. . Log . Cross entropy loss may involve the multiplication of many numbers. Multiplying lots of negative numbers together can cause problems like numerical underflow in computers. Therefore, we want to transform these probabilities to larger values so we can perform mathematical operations on them. Log does exactly this. It is not defined for numbers less than 0, and looks like this between 0 and 1: . . Additionally, we want to ensure our model is able to detect differences between small numbers. For example, the probabilities of .01 and .001, these numbers are very close together—but in another sense, 0.01 is 10 times more confident than 0.001. By taking the log of our probabilities, we prevent these important differences from being ignored. Log of a number approaches negative infinity as the number approaches zero. In ML models, since the result reflects the predicted probability of the correct label, we want our loss function to return a small value when the prediction is “good” (closer to 1) and a large value when the prediction is “bad” (closer to 0). We can achieve this by taking the negative of the log: . One benefit of using the log to calculate the loss is that our loss function penalizes predictions that are both confident and wrong. This kind of penalty works well in practice to aid in more effective model training. . . Softmax + Negative Log = Cross Entropy Loss . . You take the mean of the NLL to get the Loss value. In pyTorch, the implementation is log_softmax and then nll_loss(log is taken with the softmax only and nll_loss only calculates the mean of the negative logs). The gradient of the Cross entropy loss is linear, that means we won’t see sudden jumps or exponential increases in gradients, which should lead to smoother training of models. . It is called cross entropy because it is a measure of the difference between two probability distributions for a given random variable or set of event. It is also called log loss. In Machine learning, it measures the difference between the actual values vs the predicted values. Cross entropy finds its roots in the information theory and famous Claude Shannon’s equation . I(x) = -log₂P(x) ..where P(x) is probability of occurrence of x . It means lower the possibility of something happening, higher the information in that event/system e.g. all swans were considered white until Australia was discovered and there they saw the black swan for the first time. Thus the name black swan event = something highly improbable. Entropy is from thermodynamics which means higher the randomness, higher is the entropy. High randomness means more information and thus the system has higher entropy. . In ML, we are also taking negative log, which means high value of negative log = more misclassification and thus higher total loss.(Similar to entropy) We measure the loss by comparing our predictions with the real values, which is akin to comparing two probability distributions. A cross comparison of information contained in the two distributions and thus the name Cross Entropy. Cross Entropy = minimize(Softmax + NLL) = maximize(LL) LL is Log Likelihood, NLL is Negative Log Likelihood. Equation of Cross Entropy for two prob. distributions P and Q. E is the Expectation, while -log Q is logarithmic operation on one of the distribution. Bin P = {2 red, 3 green, 4 blue} Bin Q = {4 red, 4 green, 1 blue} . H(P, Q) = -[(2/9)*log₂(4/9) + (3/9)*log₂(4/9) + (4/9)*log₂(1/9)] . .",
            "url": "https://prashantmdgl9.github.io/ml_experiments/2021/05/24/Understanding-Cross-Entropy.html",
            "relUrl": "/2021/05/24/Understanding-Cross-Entropy.html",
            "date": " • May 24, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "COVID -19 GNOME Analysis using K-Means, PCA, and TSNE",
            "content": "The present work is based on the paper by Alexander N. Gorban and Andrei Y. Zinovyev. The link to the paper. . This work is essentially a representation that how data visualisation can help in the genomic sequence analysis in affirming hypothesis or other phenomenon . We will analyse a small fragment of the DNA . The DNA nucleotide contains 4 bases A, T, G, C. . One distinctive message in a genomic sequence is a piece of text, called a gene. . It was one of many great discoveries of the twentieth century that biological information is encoded in genes by means of triplets of letters, called codons in the biological literature. . Problem Statement : Prove that the sequence of letters in the genomic sequence is not random and that the information in the text is encoded by non-overlapping triplets i.e. codons. . Data : Genomic Sequence of Covid-19 . Imports and File reads . _Download the DNA sequences from NCBI site here_ . import numpy as np import itertools import preprocessing from sklearn import preprocessing from sklearn.decomposition import PCA %matplotlib inline import matplotlib import matplotlib.pyplot as plt from sklearn.cluster import KMeans . path = &quot;../input/gnomes/Gnomes&quot; . def loadGenome(path): genome=&quot;&quot; with open(path, &quot;r&quot;) as f: _ = f.readline() # ignore the header l = f.readline() while l: genome += l[:-1] # ignore new line l = f.readline() genome_size = len(genome) return genome, genome_size . genome, genome_size = loadGenome(path+&quot;/Covid.fasta&quot;) . The number of characters in the DNA fragment . genome_size . 29499 . Four bases viz. A, T, G, C . A - Adenine, T- Thiamine, G - Guanine, C - Cytosine . letters=list(set(genome)) . letters . [&#39;C&#39;, &#39;G&#39;, &#39;A&#39;, &#39;T&#39;] . Define parameters . Split the genome into block_size length blocks, here 300 . As there are only four letters, there are four possible words of length 1 (singlets) . 16 = 4^2 possible words of length 2 (duplets), 64 = 4^3 possible words . of length 3 (triplets) and 256 = 4^4 possible words of length 4 (quadruplets). . block_size = 350 word_size = [1,2,3,4] num_blocks = genome_size//block_size . num_blocks . 84 . Attempting to break the genome in blocks . blocks = [] start_idx = 0 for idx in range(num_blocks): end_idx = start_idx + block_size blocks.append(genome[start_idx:end_idx]) start_idx = end_idx if (idx + 1 == num_blocks): blocks.append(genome[end_idx:genome_size]) . Word Frequency . Number of possible words for each word size . def getFeatures(genome, num_blocks, word_size): features={x : np.zeros((num_blocks, 4**x)) for x in word_size} for ws in word_size: lookUp = { y : x for x,y in enumerate([&#39;&#39;.join(l) for l in itertools.product(*[letters]*ws)]) } for b in range(num_blocks): block = genome[b*block_size:(b+1)*block_size] for i in range(block_size//ws): word = block[i*ws:(i+1)*ws] features[ws][b,lookUp[word]] += 1 return features . features = getFeatures(genome, num_blocks, word_size) . Standardize the data using Standard Scaler . def standardize(features, word_size): for ws in word_size: std_scale = preprocessing.StandardScaler().fit(features[ws]) features[ws] = std_scale.transform(features[ws]) return features features = standardize(features, word_size) . Principal Component Analysis . def runPCA(features, word_size, n_components=2): featuresPCA = {} for ws in word_size: pca = PCA(n_components=n_components).fit(features[ws]) featuresPCA[ws] = pca.transform(features[ws]) return featuresPCA featuresPCA = runPCA(features, word_size) . from matplotlib.pyplot import figure fig, axes = plt.subplots(2,2, figsize=(15, 15)) axes[0,0].scatter(featuresPCA[1].T[0], featuresPCA[1].T[1], s=5) axes[0,1].scatter(featuresPCA[2].T[0], featuresPCA[2].T[1], s=5) axes[1,0].scatter(featuresPCA[3].T[0], featuresPCA[3].T[1], s=5) axes[1,1].scatter(featuresPCA[4].T[0], featuresPCA[4].T[1], s=5) . &lt;matplotlib.collections.PathCollection at 0x7fbad8b88dd0&gt; . PCA plots of word frequencies of different length. Third one shows the most structured distribution. The structure can be interpreted as the existence of a nonoverlapping triplet code . kmeans = KMeans(n_clusters=3, random_state=0).fit(featuresPCA[3]) . kmeans.labels_ . array([2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 1, 2, 0, 0, 1, 0, 1, 1, 1, 2, 0, 1], dtype=int32) . plt.figure(figsize=(10,10)) plt.scatter(featuresPCA[3].T[0], featuresPCA[3].T[1], s = 5, c = kmeans.labels_ ) . &lt;matplotlib.collections.PathCollection at 0x7fbad87d38d0&gt; . Genomic text contains information that is encoded by non-overlapping triplets, because the plot corresponding to the triplets is evidently highly structured as opposed to the pictures of singlets, duplets and quadruplets. . TSNE Visuals . PCA loses a lot of information in compression, TSNE might be a better plot . import sklearn.manifold tsne = sklearn.manifold.TSNE(n_components=2, random_state=0) . def runTSNE(features, word_size): featuresTSNE={} for ws in word_size: featuresTSNE[ws] = tsne.fit_transform(features[ws]) return featuresTSNE . featuresTSNE = runTSNE(features, word_size) . /opt/conda/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from &#39;random&#39; to &#39;pca&#39; in 1.2. FutureWarning, /opt/conda/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to &#39;auto&#39; in 1.2. FutureWarning, /opt/conda/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from &#39;random&#39; to &#39;pca&#39; in 1.2. FutureWarning, /opt/conda/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to &#39;auto&#39; in 1.2. FutureWarning, /opt/conda/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from &#39;random&#39; to &#39;pca&#39; in 1.2. FutureWarning, /opt/conda/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to &#39;auto&#39; in 1.2. FutureWarning, /opt/conda/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from &#39;random&#39; to &#39;pca&#39; in 1.2. FutureWarning, /opt/conda/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to &#39;auto&#39; in 1.2. FutureWarning, . from matplotlib.pyplot import figure fig, axes = plt.subplots(2,2, figsize=(15, 15)) axes[0,0].scatter(featuresTSNE[1].T[0], featuresTSNE[1].T[1], s=5) axes[0,1].scatter(featuresTSNE[2].T[0], featuresTSNE[2].T[1], s=5) axes[1,0].scatter(featuresTSNE[3].T[0], featuresTSNE[3].T[1], s=5) axes[1,1].scatter(featuresTSNE[4].T[0], featuresTSNE[4].T[1], s=5) . &lt;matplotlib.collections.PathCollection at 0x7fbad8978fd0&gt; . kmeans_tsne = KMeans(n_clusters=3, random_state=0).fit(featuresTSNE[3]) . plt.figure(figsize=(10,10)) plt.scatter(featuresTSNE[3].T[0], featuresTSNE[3].T[1], s=2, c=kmeans_tsne.labels_) . &lt;matplotlib.collections.PathCollection at 0x7fbad8978f10&gt; . TSNE produced a similar plot as PCA . End of Notebook .",
            "url": "https://prashantmdgl9.github.io/ml_experiments/2020/04/05/covid-19-gnome-analysis-using-k-means-pca-tsne.html",
            "relUrl": "/2020/04/05/covid-19-gnome-analysis-using-k-means-pca-tsne.html",
            "date": " • Apr 5, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Setup Vim",
            "content": "Vim Setup . Setting up vim for the first time . Run the commands on the terminal . brew update brew install vim . git clone https://github.com/gmarik/Vundle.vim.git ~/.vim/bundle/Vundle.vim . touch ~/.vimrc . Open vimrc file using vim .vimrc and in the command mode install the plugins using :PluginInstall . All the plugins will be installed. . There can be a problem with the YCM server, solve it using below . cd ~/.vim/bundle/YouCompleteMe/ sudo pip install cmake python install.py . Link to the .vimrc file . Link to .vimrc . Vim commands . There are modes in vim, insert mode, command mode . i for the insert mode Escape for the command mode h - left l - right j - up k - down . :w for write :wq for write and quit :q for quit :q! for quitting without saving In command mode use dd to delete a line use x to delete char by char in command mode . move to next word = w move to end of the line = $ move to the end of the file = G move to the beginning of the line = ^ u for undo . dw deletes the next word d$ deletes to the end of the line dG to the end of the file . A few interesting keybindings are: . Move to the end of the line $ | Move to the start of the line 0 | Move to end of doc is G and move to the start of the doc is gg | Copy a line yy, paste a line p | x for deleting the word and dd for deleting the line | :q! for quitting without saving | :x for saving with a filename | :w for saving the changes | u for undo | . Vim resources . https://realpython.com/vim-and-python-a-match-made-in-heaven/ | https://linuxhint.com/vim-python-development/ | https://www.fullstackpython.com/vim.html | https://www.vimfromscratch.com/articles/vim-for-python/ |",
            "url": "https://prashantmdgl9.github.io/ml_experiments/2019/04/17/Setup-Vim.html",
            "relUrl": "/2019/04/17/Setup-Vim.html",
            "date": " • Apr 17, 2019"
        }
        
    
  
    
        ,"post16": {
            "title": "Boosting Basics",
            "content": "Boosting . Boosting in another ensembling method of creating a strong learner from various weak learners. . Basic idea . It is a sequential method in which the learners are created in a sequential manner and the errors/misclassifications from the previous model/learner are given a higher weight to be included in sample that will be selected for training the next learner/model. This goes on till a condition is met such as batch size or the time reached. . . In the above diagram, Box 1 is the first learner, the task is to separate the positives and the negatives. The vertical line D1 is called the decision stump and is drawn to separate the data in two parts. The left side has correct classification but the right side has 3 positive points which are the errors/residuals. | In the next learner, Box 2, the previously misclassified points are given higher weightage and keeping that in mind, a new decision stump is created. | The Box 3 takes care of the errors in previous misclassifcations. | In the last step, all 3 learners combine to give Box 4 which is the final classification of the data points. | . Algorithm . Create a small model(a weak learner e.g. a tree that’s not large enough) and get the predictions. | Calculate the residuals, fancy name for error. Target - Prediction for each point in dataset. | Train another model but instead of using original target use the residual calculated in step 2 as the target for training the model. | Continue till you reach a stopping criteria. | Why Boosting works? . Each new model will be attempting to fit the error of all of the previous models combined. As we are continuoously creating new residuals, by subtracting the predictions of each new model from the residuals from the previous model, the residuals will get smaller and smaller and thus prediction will get stronger. To make predictions with an ensemble of boosted trees, we calculate the predictions from each tree, and then add them all together. Also, Boosting unlike Bagging can overfit a lot. Bagging doesn’t overfit because trees aren’t correlated while in Boosting, one model is dependent on all the previous ones. .",
            "url": "https://prashantmdgl9.github.io/ml_experiments/2018/04/04/Boosting-Basics.html",
            "relUrl": "/2018/04/04/Boosting-Basics.html",
            "date": " • Apr 4, 2018"
        }
        
    
  
    
        ,"post17": {
            "title": "Bagging In English",
            "content": "What is Bagging? . This is a simple explanatory note to myself on how boosting works without use of much Mathematics. There are times when the model make a not so decent prediction and have errors in the results. To mend this, Leo Brieman came up with the idea of Bagging. . Basic idea . . Let’s say, we have a model that’s not very good and has many errors in the predictions. It’s not a systematically biased error, it’s not always predicting too high or too low, the models generally predict the average, a few high values, a few low values. One can build a different model, with different parameters and that model will have its own errors; and one can keep building many such different models. All these models are unbiased i.e. don’t really predict one value in favour of the other or classify in one category over the others. . What will happen if we average their predictions? . Assuming that the models are not correlated with each other, then we will have the errors on either side of the correct prediction, some are bit high and some are bit low. There will a distribution of errors. The average of those errors will be 0, this is an important insight. We are taking all the distributions and stacking them on top of each other, the average of the errors will be 0. If we add all the predictions together and average them out, they will be centered around the mean value, which in this case will be the correct value. In nutshell, this process helps in reaching us close to the real value and close the gap between the real and the predicted. . The models need to be unbiased and uncorrelated, and we can average the results, because the average of the error will be 0. . Bagging is a short form for bootstrap aggregation. .",
            "url": "https://prashantmdgl9.github.io/ml_experiments/2018/03/03/Bagging-in-English.html",
            "relUrl": "/2018/03/03/Bagging-in-English.html",
            "date": " • Mar 3, 2018"
        }
        
    
  
    
        ,"post18": {
            "title": "Object Tracking in 2D using Kalman Filters",
            "content": "I think Kalman filter is the gradient descent of the control system world! . Problem Statement : Track the state(position) of the object moving at a constant velocity. . Scenario :You drive with your car in a tunnel and the GPS signal is lost. Now the car has to determine, where it is in the tunnel. The only information it has, is the noisy measurement of its velocity in driving direction. The x and y component of the velocity (x&#729; and y&#729;) can be calculated from the absolute velocity (revolutions of the wheels) and the heading of the vehicle (yaw rate sensor). . Methodology : Use Kalman filters. Kalman filter is an algorithm for estimating the state of the system for example speed and position of the car using past and possibly noisy observations and current possibly noisy measurements of the system . Credits :The notebook is based on the following video . Imports . import numpy as np import matplotlib as mpl from matplotlib import pyplot as plt . Data and Parameters . Synthetically generate noisy measurements of constant veclocity in the two directions. . Number of measurements . m = 100 . velocity in x and y directions . vx = 20 vy = 10 . Add random noise to each constant velocity measurement . mx = np.array(vx + np.random.randn(m)) my = np.array(vy + np.random.randn(m)) measurements = np.vstack((mx,my)) . measurements . array([[19.18199113, 19.60261424, 19.32473278, 20.12852329, 19.52369446, 19.82546316, 19.06530783, 19.71584217, 18.98359359, 19.3511552 , 20.59768421, 18.6762028 , 20.10370297, 19.34737904, 18.85925546, 20.04053824, 19.666659 , 19.8294875 , 20.60267812, 19.26751258, 21.00369316, 21.174452 , 20.10650566, 18.81306794, 19.77758407, 18.99368314, 18.72884021, 20.94424653, 19.81926115, 19.38409701, 18.62335228, 19.22830253, 21.49077775, 21.20593409, 18.89539579, 21.39367392, 20.86408409, 19.37585653, 20.98341741, 20.62395896, 19.50720261, 20.275245 , 19.76221664, 18.82291059, 19.33142931, 19.73368244, 21.68614649, 19.65439569, 19.62773894, 19.52972908, 20.08991476, 18.45097066, 20.73256885, 20.16034379, 19.23471851, 20.6325844 , 20.39147966, 19.55833639, 21.35074911, 20.83077009, 19.20000411, 19.86332946, 19.49645957, 21.17023862, 21.13986568, 20.67177091, 21.10954083, 19.8804764 , 20.85039946, 20.82658322, 21.58162223, 20.4770023 , 18.80738633, 20.95726542, 20.05465047, 20.08725368, 18.47539485, 18.24079446, 20.252773 , 19.55652348, 19.46998851, 19.83214639, 22.13787482, 18.82270259, 22.14045505, 20.52087147, 20.77137805, 20.00188037, 19.75945835, 20.90742125, 20.6434509 , 20.79799423, 19.73688161, 19.46605566, 19.15541015, 20.28386053, 23.33689359, 19.70271146, 19.04375873, 20.45089215], [10.92048931, 9.8863825 , 8.86822177, 10.59282149, 10.11877421, 11.3798237 , 8.06622975, 10.06728227, 9.97189956, 11.39169522, 8.42294765, 8.81097704, 10.42235895, 9.63462592, 9.72772483, 11.11578141, 10.45465692, 10.32790723, 9.91878333, 12.59486868, 10.89286265, 10.81624795, 9.72672587, 11.81031635, 8.42605262, 10.03565605, 9.72044743, 10.1369021 , 9.65486931, 10.47236949, 8.13788994, 9.64408753, 8.6390424 , 10.3771825 , 9.16922205, 10.16954616, 9.11870002, 11.15704648, 8.95889076, 9.13370211, 11.798778 , 10.8115414 , 10.14513063, 10.5667429 , 9.02247277, 11.82303437, 8.95662452, 10.82982073, 12.03799151, 8.72640959, 9.87481995, 10.12017395, 10.50463769, 10.96067734, 9.24929337, 10.53206731, 10.47035753, 11.10796151, 10.82243543, 10.35446594, 10.35956969, 10.59167183, 9.84439971, 10.52009804, 10.30207581, 10.65396118, 9.1372178 , 9.93587069, 12.03891181, 10.73426222, 8.93921963, 9.75706653, 9.74389448, 10.45093319, 11.04315493, 10.28968005, 8.9661058 , 9.6043691 , 9.25281423, 9.29747955, 11.4208568 , 8.48332818, 9.97810659, 9.62025616, 9.32552378, 9.31716574, 10.62222332, 9.13736011, 9.65248014, 9.7888286 , 12.41077194, 8.33794753, 10.46855703, 8.77296165, 8.22390735, 10.56812338, 9.66273854, 11.07476392, 8.56549026, 9.53474445]]) . Visualise the data . plt.figure(figsize=(10,7)) plt.plot(range(m),mx, label=&#39;$v_1 (measurements)$&#39;) plt.plot(range(m),my, label=&#39;$v_2 (measurements)$&#39;) plt.ylabel(&#39;Velocity Measurements&#39;) plt.title(&#39;Noisy Measurements&#39;) plt.legend(loc=&#39;best&#39;,prop={&#39;size&#39;:15}) plt.show() . Initializing Variables . . These are dynamic matrices . dt = 0.1 # Identity matrix I = np.eye(4) # state matrix x = np.matrix([[0.0, 0.0, 0.0, 0.0]]).T # P matrix P = np.diag([1000.0, 1000.0, 1000.0, 1000.0]) # A matrix A = np.matrix([[1.0, 0.0, dt, 0.0], [0.0, 1.0, 0.0, dt], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]) # H matrix H = np.matrix([[0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]) # R matrix r = 100.0 R = np.matrix([[r, 0.0], [0.0, r]]) # Q, G matrices s = 8.8 G = np.matrix([[0.5*dt**2], [0.5*dt**2], [dt], [dt]]) Q = G*G.T*s**2 . Kalman Filtering Algorithm . The Following variables will store the results, at each iteration . xt = [] yt = [] dxt= [] dyt= [] Zx = [] Zy = [] Px = [] Py = [] Pdx= [] Pdy= [] Rdx= [] Rdy= [] Kx = [] Ky = [] Kdx= [] Kdy= [] . Algorithm in process . for n in range(len(measurements[0])): # Prediction # state prediction x = A*x # error covariance prediction P = A*P*A.T + Q # Update Steps # Kalman Gain S = H*P*H.T + R K = (P*H.T) * np.linalg.pinv(S) # Update the estimate via z Z = measurements[:,n].reshape(2,1) y = Z - (H*x) x = x + (K*y) # error covariance P = (I - (K*H))*P # storing the results xt.append(float(x[0])) yt.append(float(x[1])) dxt.append(float(x[2])) dyt.append(float(x[3])) Zx.append(float(Z[0])) Zy.append(float(Z[1])) Px.append(float(P[0,0])) Py.append(float(P[1,1])) Pdx.append(float(P[2,2])) Pdy.append(float(P[3,3])) Rdx.append(float(R[0,0])) Rdy.append(float(R[1,1])) Kx.append(float(K[0,0])) Ky.append(float(K[1,0])) Kdx.append(float(K[2,0])) Kdy.append(float(K[3,0])) . Kalman Gains . def plot_K(): fig = plt.figure(figsize=(16,9)) plt.plot(range(len(measurements[0])),Kx, label=&#39;Kalman Gain for $x$&#39;) plt.plot(range(len(measurements[0])),Ky, label=&#39;Kalman Gain for $y$&#39;) plt.plot(range(len(measurements[0])),Kdx, label=&#39;Kalman Gain for $ dot x$&#39;) plt.plot(range(len(measurements[0])),Kdy, label=&#39;Kalman Gain for $ dot y$&#39;) plt.xlabel(&#39;Filter Step&#39;) plt.ylabel(&#39;&#39;) plt.title(&#39;Kalman Gain (the lower, the more the measurement fullfill the prediction)&#39;) plt.legend(loc=&#39;best&#39;,prop={&#39;size&#39;:22}) . plot_K() . Visualising Results . # Our estimates are in Red plt.figure(figsize=(10,10)) plt.plot(range(len(measurements[0])),dxt, label=&#39;$v_1estimate$&#39;, c=&#39;r&#39;) plt.plot(range(len(measurements[0])),dyt, label=&#39;$v_2estimate$&#39;, c=&#39;r&#39;) # The noisy velocity measurements in both directions are in green and blue. plt.plot(range(len(measurements[0])),mx, label=&#39;$z_1 (noisy Measurement)$&#39;, c=&#39;g&#39;) plt.plot(range(len(measurements[0])),my, label=&#39;$z_2 (noisy Measurement)$&#39;, c=&#39;b&#39;) # The actual constant velocity for both directions are in black plt.axhline(vx, color=&#39;#999999&#39;, label=&#39;$v_1(real)$&#39;) plt.axhline(vy, color=&#39;#999999&#39;, label=&#39;$v_2(real)$&#39;) plt.title(&#39;Estimates of Velocity&#39;) plt.legend(loc=&#39;best&#39;) plt.ylim([0, 30]) plt.show() # Position Tracking # Scatter plot of x and y location estimates in black # these should ideally form a straight line plt.figure(figsize=(10,10)) plt.scatter(xt,yt, s=30, label=&#39;State&#39;, c=&#39;black&#39;) # starting point in green and end point in red plt.scatter(xt[0],yt[0], s=100, label=&#39;Start&#39;, c=&#39;g&#39;) plt.scatter(xt[-1],yt[-1], s=100, label=&#39;Goal&#39;, c=&#39;r&#39;) plt.xlabel(&#39;$x_1$&#39;) plt.ylabel(&#39;$x_2$&#39;) plt.title(&#39;Estimates of Position (Tracking)&#39;) plt.legend(loc=&#39;best&#39;) plt.show() . End of Notebook .",
            "url": "https://prashantmdgl9.github.io/ml_experiments/2017/11/10/object-tracking-in-2d-using-kalman-filters.html",
            "relUrl": "/2017/11/10/object-tracking-in-2d-using-kalman-filters.html",
            "date": " • Nov 10, 2017"
        }
        
    
  
    
        ,"post19": {
            "title": "Decision Trees",
            "content": "How Decision Trees Work? . Decision Trees work on the principle of the binary split of the data; The splits are decided on the basis of asking questions in a manner that will give us pure leaf nodes. Pure leaf nodes don’t have/or have minimum impurity in them and what remains after the end of the iterations is a well split tree based on the questions asked. . How do we make the decision of the split? . Goal is to get the pure leaf nodes. The model will choose the split that maximises the information gain, our goal is to reach the state of low entropy i.e. as little mixing of labels/data as possible in the leaf node. Entropy should be as low as possible for the leaf nodes. A binary split is a split that splits the rows into two groups. . We want a score of how good a binary split is and we want it for every variable in the dataset. A decent scoring system can be - A good split is one in which all of the values of the dependent variables on one side are pretty much the same and the values on the other side are the same. So, how to measure how similar things are in a group - standard deviation. For the split, add the standard deviation of the two sides and that will be your score. And the lower score is better. . We will find the score for all the variables, not only all the variables but also where to split i.e. there would be unique values for each of the variable, we’ll try a split for each of the unique value of that variable and will calculate the score. We’ll do this for all the variables and for all the unique values and eventually split the one with the lowest “score”. . There is another score called gini, it should keep getting lower as we go to the leaf nodes from the root. Gini calculates the probability that if you picked 2 rows from a group, then they’re from the same category. If they are from the same group, the probability will be 1, and otherwise 0. . Once you have fond the way to split the root node, you have two children nodes. Treat those 2 children nodes as separate datasets and repeat the steps as above to find the best split fot each of them. Continue this process recursively, until you have reached some stopping criteria. . Note : The splitting criteria is calculated differently for different variables. - For continuous variables - reduction in variance(similar to the “score” method above) - For categorical variables - information gain, entropy, gini impurity, chi-sq test. . Random forests are just a lot of trees based on the concept of Bagging. .",
            "url": "https://prashantmdgl9.github.io/ml_experiments/2017/07/27/Decision-Trees.html",
            "relUrl": "/2017/07/27/Decision-Trees.html",
            "date": " • Jul 27, 2017"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "LinkedIn Wordpress Substack Medium . . Interested in science, maths, startup, and films. Management consultant and data scientist .",
          "url": "https://prashantmdgl9.github.io/ml_experiments/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Applications",
          "content": "These are the apps that have been developed and deployed on a cloud server. These apps have been developed to solve business particular problem statements and are client facing. . Product Recommender in B2B Space in Telco Domain Soil Classifier and Plant Health Analyser for Agriculture Sector Oil Rig Sound Classifier for Safety Enhancement . .",
          "url": "https://prashantmdgl9.github.io/ml_experiments/Applications/",
          "relUrl": "/Applications/",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "CV",
          "content": "Link to the PDF version .",
          "url": "https://prashantmdgl9.github.io/ml_experiments/CV/",
          "relUrl": "/CV/",
          "date": ""
      }
      
  

  

  
  

  
  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://prashantmdgl9.github.io/ml_experiments/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}